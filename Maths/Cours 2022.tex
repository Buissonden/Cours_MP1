\documentclass[a4paper,12pt]{book}
\usepackage{ae}
\usepackage{aeguill}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[left=1cm, right= 1cm, top=2cm, bottom = 2cm]{geometry}
\usepackage{array,multirow}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{tcolorbox}
\usepackage{lmodern}



\newcommand{\Def}[2]{\begin{tcolorbox}[sharp corners, colback=white,colframe=blue!90!black!75, title=Définition : #1]#2\end{tcolorbox}}
\newcommand{\Thr}[2]{\begin{tcolorbox}[sharp corners, colback=white,colframe=red!90!black!75, title=Théorème : #1]#2\end{tcolorbox}}
\newcommand{\Prop}[2]{\begin{tcolorbox}[sharp corners, colback=white,colframe=red!90!black!75, title=Proposition : #1]#2\end{tcolorbox}}
\newcommand{\Pre}[1]{\begin{tcolorbox}[sharp corners, colback=white,colframe=green!60!green!30!black!75, title=Preuve]#1\end{tcolorbox}}

\newcommand{\Meth}[2]{\begin{tcolorbox}[colback=white,colframe=green!60!green!30!black!75, title=Méthode :  #1]#2\end{tcolorbox}}

\newtheorem{Exe}{Exemple}[section]
\newtheorem{Exes}{Exemples}[section]
\newtheorem{Rem}{Remarque}[section]
\newtheorem{Rems}{Remarques}[section]

\def\R{\mathbb{R}}
\def\D{\mathbb{D}}
\def\C{\mathbb{C}}
\def\Q{\mathbb{Q}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\K{\mathbb{K}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 


\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\Roman{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\Roman{section}.\arabic{subsection}.\Alph{subsubsection}}

\title{Cours}
\author{Chakroun}

\begin{document}
\tableofcontents
\chapter{Rappels séries numériques}
\section{Généralités}
\Def{Convergence série}{Avec $u_n\in\K^\N$ on dit que la série $(\sum u_n)$ converge si la suite $(\sum\limits_{k=1}^n u_k)_{n\in\N}$ converge. En cas de convergence, on définit la somme de la série $\sum_{n = 0}^{\infty} u_n = \lim\limits_{n\to+\infty} \sum u_n$ seulement si la série est convergente. $(S_n) = \left(\sum\limits_{k=0}^nu_n\right)$ est la suite des sommes partielles de la série. En cas de convergence, $(R_n) =\left(\sum\limits_{k = n+1}^{\infty} u_n\right)$ est la suite des restes (d'ordre $n$) de la série. $R_n$ tend vers 0 par définition.}
\Thr{de divergence grossière}{$(u_n)\in\K^\N$, si $(\sum u_n)$ converge, alors $(u_n)\to0$.}
\Pre{$\forall n\in\N, u_n = S_n-S_{n-1}$ donc quand $(u_n)$ tend vers une limite $l$, alors $u_n$ tend vers 0.}
\Thr{Séries géométriques}{On prend $a\in\C$, la série $(\sum a^n)$ converge si, et seulement si, $\vert a\vert<1$ et alors : $\sum_{n = 0}^{\infty} a^n = \dfrac{1}{1-a}$}
\Pre{Si $\vert a\vert\geq 1$ alors la série diverge grossièrement. Si $\vert a\vert<1$ en particulier $a\neq1$, donc $\forall n\in\N,\sum\limits_{k=0}^n a^n =\dfrac{1-a^{n+1}}{1-a}$}
\Thr{Dominos ou série téléscopique}{$(a_n)\in\K^\N$, la série $(\sum a_n - a_{n+1})$ converge si, et seulement si, la suite $(a_n)$ converge.}
\Pre{Soit $n\in\N, \sum\limits_{k=0}^n a_k - a_{k+1} = a_0 -a_1 + a_1 -a_2+a_2... = a_0+a_{n+1}$. Alors la suite des sommes partielles converge si, et seulement si, $a_n$ converge. En cas de convergence, on a $\sum_{n = 0}^{\infty} a_n - a_{n+1} = a_0 - \lim\limits_{n\to+\infty}a_n$}
\Prop{Opérations sur les séries convergentes}{$(u_n),(v_n)\in\K^\N$ termes généraux de séries convergentes, alors $(\sum u_n + v_n)$ converge. Pour tout $\lambda\in\K$, alors $(\sum \lambda u_n)$ converge aussi.}
\begin{Exe}
Avec $\sum u_n$ convergente et $\sum v_n$ divergente, quel est le comportement de $\sum u_n + v_n$ ? Elle diverge. Prouvons-le par l'absurde. Supposons que $\sum (u_n +v_n)$ converge. On a que $\forall n\in\N, v_n = (u_n + v_n) - u_n$. Donc $\sum v_n$ converge. D'où la contradiction.
\end{Exe}
\section{Cas des séries à terme général réel positif}
\Def{Série à terme général réel positif}{Soit $(u_n)\in\R_+^\N$, alors $(\sum\limits_{k=0}^n u_k)$ est croissante. Elle converge si, et seulement si, $(\sum\limits_{k=0}^n u_k)$ est majorée, et $(\sum u_n)$ diverge si, et seulement si, $(\sum\limits_{k=0}^n u_k)\to +\infty$.}
\Thr{Critères de convergence d'une série}{Si $(u_n), (v_n)\in\R_+^\N$ : \begin{itemize}
\item Si $\forall n\in\N, v_n\leq u_n$, alors si $\sum u_n$ converge alors $\sum u_n$ converge. De même, si $\sum u_n$ diverge, alors $\sum v_n$ aussi (critère de majoration positif)
\item Si $u_n = o(v_n)$, alors si $\sum v_n$ converge, alors $\sum u_n$ converge (critère de domination positif)
\item Si $u_n\sim v_n$, alors $\sum v_n$ et $\sum u_n$ sont de même nature (critère d'équivalent positif)\end{itemize}}
\Thr{Séries de Riemann}{Pour $\alpha\in\R$, alors $(\sum \dfrac{1}{n^\alpha})$ converge si, et seulement si, $\alpha>1$.}
\Pre{Avec $\alpha\in\R,\alpha\neq1$, on a que $(n+1)^{\alpha+1}=n^{\alpha +1}(1+\dfrac{1}{n})^{\alpha+1} = n^{\alpha+1}(1+\dfrac{\alpha +1}{n} + o(\dfrac{1}{n})) = n^{\alpha+1} + (\alpha+1)n^\alpha + o(n^\alpha)$.
\par $(n+1)^{\alpha+1} - n^{\alpha+1}\sim  (\alpha+1)n^\alpha$, $(n^\alpha)>0$ donc $\sum n^\alpha$ et $\sum (n+1)^{\alpha+1} - n^{\alpha+1}$ sont de même nature.
\par Alors la suite des $\sum n^\alpha$ converge si, et seulement si, la somme des $n^{\alpha+1}$ converge, donc quand $\alpha+1<0$, donc $\alpha<-1$.
\par Dans le cas de $\alpha =-1$...}
\begin{Exes}\begin{enumerate}
\item Avec $u_n =\dfrac{(n^2+n+3)^{2/3}}{n(n+\sqrt{n})^{3/2}}$. C'est un terme général positif. $(u_n) \sim \dfrac{n^{4/3}}{n\times n^{3/2}}$ (ce qu'on prouve en mettant en facteur le prépondérant sur le dénominateur et le numérateur).
\par $u_n\sim \dfrac{n^{4/3}}{n\times n^{3/2}}\sim n^{-7/6}\sim\dfrac{1}{n^{7/6}}$.
\par $\sum \dfrac{1}{n^{7/6}}$ est une série de Riemann convergente, donc par critère d'équivalent positif, $\sum u_n$ converge.
\item $u_n = th(n) + \dfrac{1}{n}$, d'un côté $\sum th(n)$ diverge grossièrement et accessoirement $\sum\dfrac{1}{n}$ diverge, donc par critère de majoration positive, $\sum th(n) +\dfrac{1}{n}$ diverge.
\item $u_n = th(\dfrac{1}{n}) + ln(1-\dfrac{1}{n})$ or $th(x) = \dfrac{1}{2}(1+x+o(x) - 1 - x + o(x)) = x + o(x)$ et $ln(1+x) = x -\dfrac{x^2}{2} + \dfrac{x^3}{6} + o(x^3)$ en 0. Donc en l'infini $u_n = \dfrac{1}{n} + o(\dfrac{1}{n^2}) - \dfrac{1}{n} + -\dfrac{1}{2n^2} + o(\dfrac{1}{n^2}) = -\dfrac{1}{2n^2} + o(\dfrac{1}{n^2})$ Donc par critère d'équivalent positif avec une série convergente, $\sum u_n$ converge.
\item $u_n = e - \left(1+\dfrac{1}{n}\right)^n$, et $\left(1+\dfrac{1}{n}\right)^n = \exp(n\ln(1+\dfrac{1}{n})) = \exp(n (\dfrac{1}{n} - \dfrac{1}{2n^2} +)$
\end{enumerate}\end{Exes}

\Thr{Critère de d'Alembert}{Soit $(u_n)\in\R_+^{*\N}$. Si $\left(\dfrac{u_{n+1}}{u_n}\right)\to l\in\R$, alors :\begin{itemize}
\item Si $l<1$ alors $\sum u_n$ converge.
\item Si $l>1$ alors $\sum u_n$ diverge grossièrement.
\item Si $l=1$ alors on ne peut rien dire.\end{itemize}}
\Pre{C'est le rapport entre un terme et son successeur, donc c'est la raison locale en comparaison à une série géométrique. Si $l<1$, prenons $\varepsilon = \dfrac{1-l}{2}$. On peut fixer $n_0$ tel que $\forall n\geq n_0, \dfrac{u_{n+1}}{u_n}< l+\varepsilon \Rightarrow \dfrac{u_{n+1}}{u_n}<\dfrac{1+l}{2}$ donc pour $n\geq n_0 : u_n \leq \left(\dfrac{1+l}{2}u_{n_0}\right)$. $\dfrac{1+l}{2}<1$ donc $\sum u_n $ converge par critère de majoration positif.
\par Si $l>1$, on prend $\varepsilon = \dfrac{l-1}{2}, \exists n_0\in\N,\forall n\in\N, n\geq n_0 \Rightarrow u_n \geq (\dfrac{1+L}{2})^{n-n_0}u_{n_0}$}
\Prop{Comparaisons séries-intégrales}{Si $f:\R_+\to\R$ fonction réelle positive décroissante, continue par morceaux alors : $\forall n\in\N^*, \int_n^{n+1}f(t)dt\leq f(n)\leq \int_{n-1}^nf(t)dt$.}
\Thr{Comparaisons séries-intégrales}{Si $f:\R_+\to\R$ fonction positive décroissante continue par morceaux, alors $\left(\sum f(n)\right)$ est de même nature que la suite $\left(\int_0^nf(t)dt\right)$}
\Pre{Montrer que la suite des sommes partielles converge c'est que la suite est majorée, et on doit utiliser la croissance de l'intégrale.}
\begin{Rem}
L'encadrement de $f(n)$ par les intégrales $\int_{n-1}^nf(t)dt, \int_n^{n+1}f(t)dt$ peut être exploité dans d'autres cadres, par exemple dans le cas où la série $\sum f(n)$ converge, on peut écrire pour $n,p\in\N^*, n< p, \int_n^{p+1}f(t)dt\leq\sum\limits_{k=n}^p f(n)\leq \int_{n-1}^pf(t)dt$, et par passage à la limite quand $p\to+\infty, \lim\limits_{p\to+\infty}\int_n^pf(t)dt\leq\sum\limits_{k=n}^\infty f(k)\leq\lim\limits_{p\to+\infty}\int_{n-1}^pf(t)dt$.
\end{Rem}
\begin{Exe}
On peut retrouver par comparaisons séries-intérales que $\sum \dfrac{1}{n^\alpha}$ converge si, et seulement si, $\alpha<1$ (ce qui n'est valable que pour $\alpha>0$). Soit $\alpha\in\R, \alpha>1,R_n = \sum_{k=n+1}^\infty \dfrac{1}{k^\alpha}$. On cherche un équivalent de $R_n$.
\par Alors : $t\mapsto \dfrac{1}{t^\alpha}$ est positive décroissante. Donc : $\forall n,p\in\N, 2\leq n<p,\int_n^{p+1}\dfrac{dt}{t^\alpha}\leq\sum\limits_{k=n}^pf(k)\leq\int_{n-1}^p\dfrac{dt}{t^\alpha}$. On a que $\int_a^b\dfrac{dt}{t^\alpha} = \left[-\dfrac{1}{-\alpha+1}t^{-\alpha+1}\right]_a^b$.
\par Par passage à la limite quand $p$ tend vers l'infini, on a pour $n\in\N$ : $\dfrac{1}{(\alpha-1)(n+1)^{\alpha-1}}\leq\sum\limits_{k=n+1}^{+\infty}\dfrac{1}{k^\alpha}\leq \dfrac{1}{(\alpha-1)n^{\alpha-1}}$
\par Et, les suites encadrantes étant positives et équivalentes à $\dfrac{1}{n^{\alpha-1}}$ donc par théorème d'endcadrement des équivalents, $R_n \sim\dfrac{1}{(\alpha-1)n^{\alpha-1}}$
\par On cherche un équivalent de $\left(\sum\limits_{k=1}^n\dfrac{1}{n}\right)$. Par séries-intégrales, on obtient que $\left(\sum\limits_{k=1}^n\dfrac{1}{n}\right)\sim (log(n))$
\end{Exe}
\Thr{Sommation des ordres de grandeur}{Avec $(a_n),(b_n)$ deux suites réelles positives :\begin{itemize}
    \item Si $b_n = O(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k = O(\sum\limits_{k=n+1}^{+\infty} a_k)$ ; si $\sum a_n$ diverge, alors $\sum\limits_{k=0}^n b_k = O(\sum\limits_{k=0}^n a_k)$.
    \item Si $b_n = o(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k = o(\sum\limits_{k=n+1}^{+\infty} a_k)$ ; si $\sum a_n$ diverge, alors $\sum\limits_{k=0}^n b_k = o(\sum\limits_{k=0}^n a_k)$.
    \item Si $b_n\sim(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k \sim \sum\limits_{k=n+1}^{+\infty} a_k$ ; si $\sum a_n$ diverge, alors $\sum b_n$ diverge et $\sum\limits_{k=0}^n b_k \sim \sum\limits_{k=0}^n a_k$.
    \end{itemize}
}

\Pre{Prenons $(u_n), (v_n)\in\R_+^\N$, avec $(u_n)=o(v_n)$. On suppose que $\sum v_n$ converge.
\par Alors $\sum u_n$ converge par critère de domination positif. Donc $(u_n) = (v_n\varepsilon_n)$ avec $(\varepsilon_n)\to 0$. Soit $\varepsilon>0$, on peut fixer $n_0$ tel que $\forall n\geq n_0, \varepsilon_n<\varepsilon$
\par Donc pour $n_0\leq n\leq p$ : $\sum\limits_{k=p}^nu_k\leq \left(\sum\limits_{k=n}^p v_k\right)\varepsilon$.
\par Donc par passage à la limite quand $p\to+\infty, \sum\limits_{k=n}^\infty u_k\leq \varepsilon\sum\limits_{k=n}^{\infty}v_k$
\par Si $(v_n)$ n'est pas la suite nulle, $\sum v_k>0$. On a montré que : $\forall\varepsilon\in\R_+^*, \exists n_0\in\N, \forall n\geq n_0, \dfrac{\sum\limits_{k=n}^\infty u_k}{\sum\limits_{k=n}^\infty v_k}<\varepsilon$
\par $(u_n), (v_n) \geq 0$, si $\sum v_n$ diverge alors : on pose aussi $(u_n)=(\varepsilon v_n)$ avec $\varepsilon_n\to 0$
\par Soit $n\in_N, \sum\limits_{k=0}^nu_k = \sum v_k\varepsilon_k$
\par Soit $\varepsilon\in\R_+^*$, on fixe $n_0$ tel que $\forall n\geq n_0, \varepsilon_n\leq \varepsilon$.
\par Pour $n\geq n_0, \sum u_k = \sum\limits_{k=0}^{n_0} \varepsilon_k v_k + \sum\limits_{k=n_0}^n \varepsilon_k v_k$
\par On peut supposer $\sum v_k>0$, alors : $\dfrac{\sum\limits_{k=0}^n u_k}{\sum\limits_{k=0}^n v_k} \leq \dfrac{\sum\limits_{k=0}^{n_0} \varepsilon_k v_k}{\sum\limits_{k=0}^n v_k} + \varepsilon$
\par $\left( \dfrac{\sum\limits_{k=0}^{n_0}\varepsilon_k v_k}{\sum\limits_{k=0}^n v_k}\right)\to 0$ donc on peut fixer $n_1\in\N, \forall n\geq n_1, \dfrac{\sum\limits_{k=0}^{n_0}\varepsilon_k v_k}{\sum\limits_{k=0}^n v_k} < \varepsilon$
\par Alors $\forall\varepsilon\in\R_+^*, \exists N=\max(n_0, n_1), \forall n\geq N, \dfrac{\sum\limits_{k=0}^n u_k}{\sum\limits_{k=0}^n v_k} < 2\varepsilon$
\par Pour les équivalents : $u_n\sim v_n \Rightarrow (u_n) =(v_n) + o(v_n) = (v_n) + (w_n)$ avec $\dfrac{w_n}{v_n}\to 0$.
\par Dans le cas convergent : $\left(\sum\limits_{k=n}^\infty u_k\right) = \left(\sum\limits_{k=n}^\infty v_k\right) + \left(\sum\limits_{k=n}^\infty w_k\right)$ mais comme $w_n = o(v_n)$, on a bien l'équivalence.
\par Pour les O : $\exists M\in\R,\forall n\in\N, u_n\leq Mv_n$. Il suffit de majorer les sommes.
}

\section{Séries de terme général quelconque}
Ici, $(u_n)\in\K^\N$
\Def{Absolue convergence}{Soit $(u_n)\in\C^\N$, on dit que $\sum u_n$ converge absolument si $\sum \vert u_n\vert$ converge.}
\Thr{Conséquences de l'absolue convergence}{Toute série absolument convergente est convergente}
\Pre{Dans le cas réel, en utilisant que $u_n^+\leq\vert u_n\vert$ et $u_n^-\leq\vert u_n\vert$ donc les deux convergent par critère de majoration, comme $\sum u_n = \sum u_n^+ - u_n^-$.
\par Dans le cas complexes, les séries des parties réelles et imaginaires sont majorées par les modules.}
\begin{Rem}
Pour une série absolument convergente, $\left|\sum\limits_{n=0}^{+\infty}\right| \leq \sum\limits_{n=0}^{+\infty}\vert u_n\vert$. Comme on a $\forall p\in\N, \left|\sum\limits_{n=0}^pu_n\right|\leq \sum\limits_{n=0}^p\vert u_n\vert$, on passe à la limite et on a la propriété.
\end{Rem}
\begin{Exe}
Quelques exemples de séries absolument convergentes sont $\sum \dfrac{(-1)^n}{n^2}, \sum \dfrac{sin(n)}{n^2}$, mais il existe cependant des séries qui soient convergentes sans l'être absolument, telles que $\sum \dfrac{(-1)^n}{n}$ ou $\sum \dfrac{sin(n)}{n}$.
\end{Exe}
\Thr{Théorème spécial des séries alternées}{Soit $(a_n)$ une suite réelle positive décroissante de limite nulle. Alors $\sum(-1)^n a_n$ converge et $\forall n\in\N, \left|\sum\limits_{k=n}^{+\infty}(-1)^ka_k\right|\leq a_n$}
\Pre{Pour montrer la convergence, considérons la suite $(S_n)$ des sommes partielles : $\forall n\in\N, S_n = \sum\limits_{k=0}^n (-1)^ku_k$ ;
\par Alors pour tout $n\in\N$ on a :\begin{itemize}
\item $S_{2n+2}-S_{2n} = u_{2n+2}-u_{2n+1} \leq0$ donc la suite $(S_{2n})$ est décroissante ;
\item $S_{2n+3}-S_{2n+1} = -u_{2n+3} + u_{2n+2}\geq 0$ donc la suite $(S_{2n+1})$ est croissante ;
\item $S_{2n} - S_{2n+1} = u_{2n+1}\to0$ en l'infini\end{itemize}
Donc les suites $(S_{2n}), (S_{2n+1})$ sont adjacentes : elles convergent vers une même limite l qui est donc la limite de $(S_n)$. Donc la série $\sum (-1)^nu_n$ est convergente, de somme $l$. Et de plus, $l$ vérifie :
\par $\forall n,m\in\N, S_{2m+1}\leq l\leq S_{2n}$
\par Pour la majoration de $\vert R_n\vert$ on procède par disjonction de cas :\begin{itemize}
\item Si $n$ est pair : on a par propriétés des suites adjacentes que : $S_{n+1}\leq l\leq S_n$. Et donc $R_n = l-S_n\in [S_{n+1} - S_n, 0] = [-u_{n+1}, 0]$. Donc $\vert R_n\vert = -R_n \leq u_{n+1}$.
\item Si $n$ est impair : on a de même que : $S_n\leq l\leq S_{n+1}$ et donc $R_n\in[0, u_{n+1}]$. Donc $\vert R_n\vert = R_n \leq u_{n+1}$.
\end{itemize}
}

\begin{Exe}
On a ainsi cet exemples de série semi-convergente, pour $0<\alpha\leq1$ : $\sum \dfrac{(-1)^n}{n^\alpha}$. Pour $\alpha\leq 0$, on a divergence grossière, et pour $\alpha>0$, on a $(\dfrac{1}{n^\alpha})$ positive décroissante tendant vers $0$, donc le critère spécial des séries alternées dit que ces séries sont convergentes.
\par On en tire aussi cette inégalité pour $\alpha>0$ : $\forall n\in\N, \left|\sum_{k=n}^{+\infty}\right|\leq \dfrac{1}{n^\alpha}$
\end{Exe}

\begin{Rem}
Les critères des séries positives ne s'appliquent plus pour ces séries.
\par Pour le critère des équivalents : on peut prendre la série de terme général $u_n = \dfrac{(-1)^n}{\sqrt{n}} + \dfrac{1}{n}$. On a que $u_n\sim\dfrac{(-1)^n}{\sqrt{n}}$, dont la série associée est convergente. Mais $\sum u_n$ n'est pas convergente à cause de la somme harmonique.
\par Pour le critère de domination : on peut prendre la série de terme général $\dfrac{1}{n}$, dominée par $\dfrac{(-1)^n}{\sqrt{n}}$. Si le terme général qui domine est celui d'une série convergente, on sait que $\sum \dfrac{1}{n}$ n'est pas convergente.
\par On a donc un nouveau critère de domination positive : Si $(u_n), (v_n)$ termes généraux de séries, qu'on a que $v_n>0$, que $\sum v_n$ converge et que $u_n = o(v_n)$, alors on aura bien que $u_n$ est absolument convergente.
\par Le critère de d'Alembert subit une petite modification : On considère $(u_n)$ terme général d'une série, si $\left|\dfrac{u_{n+1}}{u_n}\right|$ admet une limite $l$, on a que $\sum u_n$ est absolument convergente si $l<1$ et diverge grossièrement si $l>1$. Toujours pas de lois s'appliquant sur le cas où $l=1$ cependant.
\par Pour le théorème de sommation des ordres de grandeur, on a pour le o et le O besoin que $v_n>0$ pour que le théorème fonctionne, le signe de $u_n$ n'a pas besoin d'être controlé. "La suite de référence doit être positive pour appliquer le théorème." Pour les équivalents, on a que les deux sont déjà du même signe.
\end{Rem}

\begin{Exe}
On fait les séries de Bertrand, alternées et non.
\par Pour les séries de Bertrand : on finit par trouver une convergence si, et seulement si, $\alpha >1$ ou $\alpha = 1, \beta >1$
\par Pour les séries de Bertrand alternée : on étudie $\sum\dfrac{(-1)^n}{n^\alpha ln(n)^\beta}$. On commence par regarder les divergences grossières : $\sum u_n$ diverge si $\alpha<0$ ou si $\alpha = 0$ et $\beta\leq0$.
\par Les cas de convergence absolue sont les cas où $\alpha >1$ ou  $\alpha=1, \beta>1$ d'après ce qui venait avant.
\par On prend le cas de $\alpha =0$ et $\beta>0$, alors $u_n$ tend vers $0$, et elle est décroissante en l'infini, donc par critère des séries alternées on a la convergence.
\par Si $0<\alpha<1$, alors $\exists \alpha'>0$ tel que $\vert u_n\vert\ll\dfrac{1}{n^{\alpha'}}$ Ainsi, $w_n$ est positive et décroissante à partir d'un certain rang, et tend vers $0$, donc en utilisant le critère des séries alternées on a la convergence.
\par Si $\alpha =1$, $\sum u_n$ est semi-convergente. 
\end{Exe}

\begin{Exe}
Exemple de série avec un terme en $(-1)^n$ : on pose $u_n = \dfrac{(-1)^n}{n^\alpha + (-1)^n}$ terme général d'une série.
\par Les cas de divergence grossière sont si $\alpha< 0$ et on a des impossibilités si $\alpha =0$.
\par Si $\alpha>1$, alors la série converge absolument puisqu'on trouve un équivalent qui converge par Riemann à $\vert u_n\vert$
\par Prenons les cas $0<\alpha\leq 1$ : on va faire un "éclatement des termes" (un DL quoi) : $\dfrac{(-1)^n}{n^\alpha}\left(\dfrac{1}{1+\frac{(-1)^n}{n^\alpha}}\right) = \dfrac{(-1)^n}{n^\alpha}\left(1+\dfrac{(-1)^n}{n^\alpha} + o(\dfrac{1}{n^\alpha})\right) = \dfrac{(-1)}{n^\alpha} - \dfrac{1}{n^{2\alpha}} + o(\dfrac{1}{n^\alpha})$. On pose $v_n$ tel que $\forall n\in\N, u_n =\dfrac{(-1)^n}{n^\alpha}+ v_n$, avec $(v_n)\sim\dfrac{1}{n^{2\alpha}}$. On a que $(-v_n)$ est positive, et que $\sum v_n$ converge si, et seulement si, $\alpha >\dfrac{1}{2}$. Donc on en déduit que $\sum u_n$ converge si $\alpha >\dfrac{1}{2}$ et diverge sinon.
\par Donc $\sum u_n$ converge si, et seulement si, $\alpha >\dfrac{1}{2}$
\end{Exe}



\chapter{Familles sommables}
\section{Dénombrabilité}
\Def{Dénombrable}{On dit qu'un ensemble est dénombrable s'il est en bijection avec $\N$.}
\Def{Au plus dénombrable}{Un ensemble est au plus dénombrable s'il est fini ou dénombrable.}
\begin{Exe}
$\Z$ est dénombrable avec $\varphi:\N\to\Z, \varphi:n\mapsto (-1)^{n+1}\left\lfloor\dfrac{n+1}{2}\right\rfloor$ bijective :\begin{itemize}
\item Injectivité : supposons $\varphi(n_1)=\varphi(n_2)$, si $n_1$ pair, alors $n_1$...
\item surjectivité : Soit $n\in\Z$, trouvons un antécédent ; si $n$ est positif on a $2n-1$, si $n$ est négatif $-2n$\end{itemize} 
\end{Exe}
\Prop{Parties infinies de $\N$}{Toute partie infinie de $\N$ est dénombrable.}
\Pre{Soit $X$ une partie infinie de $\N$. On construit $(x_n)\in X^\N$ par récurrence. $x_0 =\min(X)$ qui existe comme toute partie non-vide de $\N$ possède un plus petit élément. Avec $n\in\N$, on suppose $x_0,... x_n$ bien définies, et on définit $x_{n+1} = min(X\backslash\{x_0,...,x_n\})$.
\par Il faut démontrer que $(x_n)$ est une bijection. Elle est injective parce que par construction, elle est croissante. Elle est surjective par l'absurde : s'il y avait un élément qui n'avait pas d'antécédent, considérons le plus petit élément sans antécédent, mais son prédécesseur aurait un antécédent, etc.}
\Prop{Parties infinies}{Toute partie infinie d'un ensemble dénombrable est dénombrable.}
\Pre{Soit $X$ dénombrable, $Y\subset X$ infinie, alors on peut composer la bijeection avec laquelle on obtient les éléments de $Y$ par les éléments de $X$ avec la bijection entre $X$ et $\N$, qui est donc bijective aussi.}
\Prop{Produit cartésien}{$\N^2$ est dénombrable.}
\Pre{$\varphi :\left\{\begin{array}{rcl} \N^2&\to&\N \\ (p,q) &\mapsto&2^p(2q+1) \end{array}\right.$. Prouvons sa bijectivité. 
\par Pour l'injectivité, avec $(p,q), (p',q')\in\N^2$, on suppose que $2^p(2q+1) = 2^{p'}(2q'+1)$. Par lemme de Gauss : $2^p\vert 2^{p'}(2q'+1)$, or $2^p\wedge (2q'+1)=1$ donc $2^p\vert 2^{p'}$ donc $p\leq p'$, de même on a $p'\leq p$ donc $p=p'$ et $q=q'$, d'où l'injectivité.
\par Pour la surjectivité, soit $n\in\N$. Considérons $A=\{p\in\N\vert 2^p\vert n\}$. $A$ est non-vide car $2^0=1$ divise tout, c'est une partie de $\N$ et $A$ est majoré ($2^p$ tend vers l'infini et $n$ est fini). Donc $A$ possède un plus grand élément $p_0$. Donc $\frac{n}{2^{p_0}}$ est impair, alors $(p_0, \dfrac{1}{2}(\dfrac{n}{2^{p_0}}-1))$ est antécédent de $n$, d'où la surjectivité.}
\Prop{Produit cartésien étendu}{$\N^p$ est dénombrable.}
\Pre{Par récurrence, avec l'application de la preuve précédente mais des ensembles $\N^{n-1}\times \N\to\N$ pour prouver le tout.}
\Prop{Dénombrabilité de $\Q$}{$\Q$ est dénombrable.}
\Pre{C'est une partie infinie de $\N\times\N^*$, donc elle est dénombrable.}
\Prop{Dénombrabilité de $\R$}{$\R$ est en bijection avec $[0,1[$, donc $\R$ n'est pas dénombrable.}
\Pre{Avec un nombre écrit sous la forme $0,a_1a_2a_3...$. On peut écrire $a_1=\lfloor 10x\rfloor$, $a_2 = \left\lfloor(x-\dfrac{a_1}{10})10^2\right\rfloor$,...
\par On peut construire une bijection avec $\R$. Par diagonalisation (on indexe tous les réels entre 0 et 1 par des entiers, et ensuite on construit un nouveau réel qui n'est pas du tout présent dans la suite), on a que $\N$ n'est pas en bijection avec $[0,1[$. Ainsi, $\N$ n'est vraiment pas en bijection avec $\R$.}
\begin{Rem}
L'hypothèse du continu est indécidable, et dit que tout ensemble inclus dans $\R$ est soit en bijection avec $\N$, soit avec $\R$, sans entre-deux. Il faut le rajouter à l'axiomatique pour ne pas créer de paradoxes en se posant la question.\end{Rem}
\Prop{Réunion}{Toute réunion au plus dénombrable d'ensembles au plus dénombrables est au plus dénombrable. Si $I$ est au plus dénombrable, avec $(A_i)_{i\in I}$ est une famille d'ensembles au plus dénombrable, alors $\cup_{i\in I} A_i$ est au plus dénombrables.}
\Pre{$E$ ensemble, $A,B$ deux parties de $E$. Alors $A\cup B = \{x\in E\vert (x\in A)\vee (x\in B)\}$. Alors $(A_i)_{i\in I}\in\mathcal{P}(E)^I$, et donc $\cup_{i\in I}A_i = \{x\in E\vert \exists i\in I, x\in A_i\}$. }

\section{Familles sommables de réels positifs}
\Def{Somme de familles réelles positives}{Soit I un ensemble, soit $(u_i)_{i\in I}\in \R_+^I$. On définit la somme des $(u_i)_{i\in I}$ comme : $\sum\limits_{i\in I} u_i = \sup\left(\sum\limits_{j\in J} u_j\right)_{J\subset I\text{ fini}}$ si ces sommes sont majorées et $+\infty$ sinon.}
\Def{Familles sommables de réels positifs}{Soit $(u_i)_{i\in I}\in\R_+^I$, on dit que $(u_i)_{i\in I}$ est sommable si $\sum\limits_{i\in I} u_i\in \R_+$}
\Prop{Sommabilité et dénombrabilité}{Si $I$ est non-dénombrable et $(u_i)_{i\in I}\in \R_+^I$, alors $(u_i)_{i\in I}$ n'est pas sommable.}
\Pre{Ici, $I$ est non-dénombrable. Pour $n\in\N^*$, on note $A_n=\left\{i\in I\vert u_i>\dfrac{1}{n}\right\}$. Alors $\cup_{n\in\N^*} A_n = I$.
\par On a une réunion dénombrable d'ensembles donnant un ensemble non-dénombrable, donc il existe $n_0$ tel que $A_{n_0}$ non-dénombrable. Donc $A_{n_0}$ est infini : pour tout $p\in\N^*$, on peut trouver $J\subset A_{n_0}$ tel que $\mathrm{card}(J)= p$. 
\par Donc $\sum\limits_{j\in J} u_j>\dfrac{p}{n_0}$ donc la somme n'est pas majorée. Donc $\sum\limits_{i\in I} u_i = +\infty$}
Dans la suite du cours, $I$ est dénombrable.
\Thr{}{Soit $I$ un ensemble dénombrable, soit $\begin{array}{rcl} \N&\to&I \\n&\mapsto&i_n\end{array}$ une bijection de $\N$ dans $I$. Soit $(u_i)_{i\in I}\in\R_+^I$. $(u_i)_{i\in I}$ est sommable si, et seulement si, la série $\sum u_{i_n}$ est convergente. En cas de convergence, $\sum\limits_{i\in I} u_i = \sum\limits_{n=0}^{+\infty} u_{i_n}$.}
\Pre{Dans le cadre de l'énoncé, on suppose $(u_i)_{i\in I}$ sommable.\par Fixons $n\in\N, \sum\limits_{k=0}^n u_{i_k} = \sum\limits_{k\in\llbracket0,n\rrbracket}u_{i_k}$ donc $\sum\limits_{k=0}^nu_k \leq \sum\limits_{i\in I}u_i$.\par Donc $\sum u_{i_n}$ converge et $\sum\limits_{n=0}^{+\infty}u_n\leq \sum\limits_{i\in I} u_i$.
\par Soit $J\subset \N$ fini. $\sum\limits_{j\in J}u_{i_j}\leq \sum\limits_{n=0}^{\max(J)} u_{i_n} \leq \sum\limits_{i=0}^{+\infty}u_{i_n}$ \par donc $\sum\limits_{i\in I}u_i \leq \sum\limits_{n=0}^{+\infty}u_{i_n}$
\par Pour $(u_i)_{i\in I}\in\R_+^I$, avec $I$ dénombrable. Alors $\sum\limits_{i\in I} u_i = \sum\limits_{n=0}^{+\infty} u_{\varphi(n)}$ si la sére converge et $+\infty$ sinon.}
\Thr{Sommation par paquets positif}{Soit $I$ dénombrable et $(J_j)_{j\in J}$ une partition de $I$ avec $J$ au plus dénombrable, ie $\cup_{j\in J} J_j = I, \forall j, h\in I, j\neq h \Rightarrow J_j\cap J_h = \emptyset$. Soit $(u_i)_{i\in I}\in\R_+^I$, alors : $\sum\limits_{i\in I} u_i = \sum\limits_{j\in J}\left(\sum\limits_{k\in J_j}u_k\right)$ }
\begin{Exe}
$\sum\limits_{n\in\N^*} \dfrac{1}{n^2} = \sum\limits_{n\in\N^*}\dfrac{1}{(2n)^2} + \sum\limits_{n\in\N}\dfrac{1}{(2n+1)^2}$ d'où $\dfrac{3}{4}\sum\limits_{n\in\N^*}\dfrac{1}{(2n+1)^2}$
\par Mais en revanche, on n'a pas que $\sum\limits_{n=1}^{+\infty}\dfrac{(-1)^n}{n} = \sum\limits_{n=1}^{+\infty}\dfrac{1}{2n} - \sum\limits_{n=0}^{+\infty}\dfrac{1}{2n+1}$ puisque ces deux séries ne sont pas convergentes.
\par $\left(\dfrac{1}{n^2+p^2}\right)_{(n,p)\in\N^{*2}}$ est-elle sommable ? D'abord, notons que $\left(\sum\dfrac{1}{n^2+p^2}\right)_{np}$ est positive, et donc on a le droit d'écrire :
\par $\sum\limits_{n,p\in\N}\dfrac{1}{n^2+p^2} = \sum\limits_{n\in\N^*}\sum\limits_{p\in\N^*} \dfrac{1}{n2+p^2}$ car $\N^{*2}=\cup \{(n,p)\vert n\in\N^*, p\in\N^*\}$ et la réunion est disjointe (signe $\coprod$, non homologué)
\par $= \sum\limits_{n\in\N^*}I_n$ avec $I_n=\sum{p\in\N^*}\dfrac{1}{n^2+p^2}$. Soit $n\in\N^*$, alors $\left(\dfrac{1}{n^2+p^2}\right)_p\sim \dfrac{1}{p^2}$, donc $\forall n\in\N^*, I_n<+\infty$. Soit $n\in\N^*$. On définit $f:x\mapsto \dfrac{1}{n^2+p^2}$ décroissante et positive. On fixe  $p\in\N^*a$. Et alors : $\int_p^{p+1}\dfrac{dt}{n^1+t^2}\leq\dfrac{1}{n^2+p^2}$.
\par Donc $\forall N\in\N,\sum\limits_{p=1}^N\dfrac{1}{n^2+p^2}\geq \int_1^{N+1}\dfrac{dt}{n^2+t^2} \geq \frac{1}{n}\left[\arctan(u)\right]_{\frac{1}{n}}^{\frac{N+1}{n}}\geq \dfrac{1}{n}\left(\arctan(\dfrac{N+1}{n})-\arctan(\dfrac{1}{n})\right)$. Par passage à la limite quand $N$ tend vers $+\infty$, on a que $I_n\geq \dfrac{\pi}{2n}\arctan(\dfrac{1}{n})$.
\par Donc $\dfrac{\pi}{2n}-\dfrac{1}{n}\arctan(\dfrac{1}{n})\sim\dfrac{\pi}{n}$. Or $\dfrac{\pi}{n}$ diverge, donc $\sum I_n$ diverge. Donc $\sum\limits_{n\in\N^*} =+\infty$. Donc $\left(\dfrac{1}{n^2+p^2}\right)_{n,p}$ n'est pas sommable.
\par Dans le cas plus général de $\left(\dfrac{1}{n^\alpha+p^\alpha}\right)_{(n,p)\in\N^{*2}}$
\end{Exe}

\section{Familles de complexes.}
\Def{Sommabilité complexe}{Soit $(u_i)_{i\in I}\in\C^I$. On dit que la famille est sommable si $(|u_i|)_{i\in I}$ est sommable, donc que $\sum\limits_{i\in I}|u_i|<+\infty$}
\Prop{Somme d'une famille sommable.}{Soit $I$ dénombrable et $(u_i)_{i\in I}$ sommable. Alors : pour toute bijection $\varphi:\left\{\begin{array}{rcl} \N&\to&I \\ n&\mapsto&\varphi(n)\end{array}\right.$ on a que $\sum\limits_{n}u_{\varphi(n)}$ est une série absolument convergente et $\sum\limits_{n=0}^{+\infty}u_{\varphi(n)}$ ne dépend pas de $\varphi$.
\par On appelle somme de $(u_i)_{i\in I}$ : $\sum\limits_{i\in I} =\sum\limits_{n=0}^\infty u_{\varphi(n)}$ où $\varphi$ est une bijection donnée. La somme d'une famille non-sommable n'est pas définie.}
\Pre{$(u_i)$ sommable. $\varphi$ bijection de $\N$ dans $I$. $\sum \vert u_i\vert = \sum \vert u_{\varphi(n)}\vert = \sum\limits_{n=0}^\infty\vert u_{\varphi(n)}\vert$. $\sum u_{\varphi(n)}$ est absolument convergente. De plus, $\forall N\in\N, \sum\limits_{k=0}^N u_{\varphi(k)} = \sum\limits_{k=0}^N\mathcal{R}(u_{\varphi(k)})_+ - \mathcal{R}(u_{\varphi(k)})_- + i(\mathcal{I}(u_{\varphi(k)})_+ - \mathcal{I}(u_{\varphi(k)}))$.
\par Tous ces restes sont majorés par $\vert u_{\varphi(k)}\vert$ donc sommables. Donc $\sum\limits_{k=0}u_{\varphi(k)}\to \sum\limits_{n\in\N} \mathcal{Re}(u_{\varphi(n)})_+ + \sum\limits_{n\in\N} \mathcal{Re}(u_{\varphi(n)})_-+...$}
\Thr{Sommation par paquets}{$I$ dénombrable, dont les $J_j$ sont une partition. Avec $(u_i)_{i\in I}\in\C^I$ sommable. Alors $\sum\limits_{i\in I}u_i = \sum\limits_{j\in J}\sum\limits_{k\in J_j} u_k$}

\section{Applications}
\Thr{de Fubini}{Soit $(u_{i,j})_{(i,j)\in I\times J}\in\C^{I\times J}$ sommable. Alors $\sum\limits_{i,j\in I\times J} u_{i,j} = \sum\limits_{i\in I}\sum\limits_{j\in J}u_{i,j} = \sum\limits_{j\in J}\sum\limits_{j\in J}$, avec le cas particulier où $u_{i,j} = a_ib_j$ où : $(a_ib_j)_{i,j\in I\times J}$ qui est sommable si, et seulement si, $(a_i)_{i\in I}$ et $(b_j)_{j\in J}$ sont sommables et dans ce cas, $\sum\limits_{(i,j)\in I\times J} a_ib_j = \left(\sum\limits_{i\in I}a_i\right)\left(\sum\limits_{j\in J}b_j\right)$}
\Pre{On prend $I\times J = \cup_{i\in I}\{(i,j), j\in J, i\in I\}$. Par sommation par paquets : $\sum\limits_{(i,j)\in I\times J}\vert a_ib_j\vert = \sum\limits_{i\in I} \left(\sum\limits_{j\in J} \vert a_i\vert \vert b_j\vert\right) = \sum\limits_{i\in I}\vert a_i\vert \left(\sum\limits_{j\in J}\vert b_j\vert\right) = \left(\sum\limits_{i\in I}\vert a_i\vert\right)\left(\sum\limits_{j\in J}\vert b_j\vert\right)$
\par Donc $\sum\vert a_ib_j\vert<+\infty$ si, et seulement si, $\sum\vert a_i\vert<+\infty$ et $\sum\vert b_j\vert < +\infty$. Dans le cas de sommabilité, on peut reprendre tous ces calculs en enlevant les modules.}
\Thr{Produit de Cauchy}{Soit $(a_n), (b_n)\in\C^\N$. Si $\sum a_n, \sum b_n$ sont abolument convergentes alors $\sum\left(\sum\limits_{k=0}^na_nb_{n-k}\right)$ est absolument convergente et $\left(\sum\limits_{n=0}^{+\infty}a_n\right)\left(\sum\limits_{n=0}^{+\infty}b_n\right) = \sum\limits_{n=0}^{+\infty}\left(\sum\limits_{k=0}^n a_kb_{n-k}\right)$}
\Pre{$(a_nb_p)_{(n,p)\in\N^2}$ est sommable. $\N^2 = \cup_{s\in\N} \{(n,p)\in\N^2\vert n+p=s\} = \cup_{s\in\N} S_s$ donc par théorème de sommation par paquets, on a que $\sum\limits_{(n,p)\in\N^2}a_nb_p = \sum\limits_{s\in\N} \sum\limits_{(n,p)\in S_s}a_nb_p = \sum\limits_{s\in\N}\left(\sum\limits_{k=0}^na_kb_{s-k}\right)$}
\begin{Exe}
Avec $t\in]-1,1[$, montrez que $\dfrac{1}{(1-t)^2} = \sum\limits_{n=0}^{+\infty}(n+1)t^n$. C'est convergent par $(n+1)t^n$ est absolument convergent en remarquant que $[n+1]t^n = o(\dfrac{1}{n^2})$. On peut donc faire un produit de Cauchy : $\left(\sum\limits_{n=0}^{+\infty} t^n\right) \left(\sum\limits_{n=0}^{+\infty} t^n\right) = \sum\limits_{n=0}^{+\infty}\left(\sum\limits_{k=0}^n t^k t^{n-k}\right)$
\end{Exe}

\begin{Exe}
Prenons l'exponentielle complexe, définie par : pour $z\in\C$, on définit : $\exp(z) = \sum\limits_{n=0}^{+\infty} \dfrac{z^n}{n!}$.
\par On prouve sa convergence facilement par le critère de d'Alembert : $\left\vert\dfrac{\vert z\vert}{n+1}\right\vert$ qui tend vers 0. Donc par critère de d'Alembert, on a sa convergence absolue. Comme elle est absolument convergence, on peut l'écrire comme une famille sommable : $\sum\limits_{n\in\N}\dfrac{z^n}{n!}$.
\par Prouvons que $\forall z,z'\in\C, \exp(z+z') = \exp(z)\exp(z')$ : comme la famille est sommable, avec $z,z'\in\C$, on a donc que $\exp(z)\exp(z')$ existe en tant que limite de la famille $\sum\limits_{p\in\N}\sum\limits_{n\in\N} \dfrac{z^n}{n!}\dfrac{z'^{p-n}}{(p-n)!} = \sum\limits_{p\in\N}\sum\limits_{p\in\N} \dfrac{1}{p!}\binom{p}{n}zz'^{n-p} = \sum\limits_{n\in\N} \dfrac{(z+z')^{n}}{n!}$
\par Prouvons que $\forall z\in\C, \exp(\bar{z}) = \bar{\exp(z)}$. Soit $z\in\C$, soit $n\in\N$ : $\sum\limits_{k=0}^n \dfrac{\bar{z}^k}{k!} = \bar{\sum\limits_{k=0}^n\dfrac{z^k}{k!}}$. Par passage à la limite lorsque $n$ tend vers l'infini, on a donc que $\exp(\bar{z})=\bar{\exp(z)}$.
\par On peut alors utiliser que $\forall \theta\in\R, \exp(i\theta)\bar{\exp(i\theta)} = \exp(i\theta)\exp(-i\theta) = \exp(i(\theta - \theta)) = \exp(0) = 1$
\par La restriction de l'exponentielle à $\R$ coïncide bien avec la fonction exponentielle réelle.
\end{Exe}



\chapter{Espaces vectoriels normés}
\section{Généralités}
On note $\K$ un corps, $\R$ ou $\C$. Dans tous les cours, $E$ est un $\K$-ev.
\Def{Norme}{Soit $E$ un $\K$-ev. Soit $\varphi$ une application de $E$ dans $\R_+$. On dit que $\varphi$ est une norme si elle vérifie:\begin{enumerate}
\item $\forall u\in E, \varphi(u) = 0\Rightarrow u=0$ (on dit que l'application est définie)
\item homogénéité : $\forall\lambda\in\K, \forall u\in E,\varphi(\lambda u) =\vert \lambda\vert \varphi(u)$
\item inégalité triangulaire : $\forall u, v\in E, \varphi(u+v) \leq \varphi(u)+\varphi(v)$
\end{enumerate}}
\Def{EVN}{On dit que $E$ est un espace vectoriel normé si on a choisi une norme dans $E$. On note alors $(E,\varphi), (E,N), (E,\Vert.\Vert)$.}
\Def{Boule unité}{Avec $(E,\Vert.\Vert)$, on appelle la boule unité de $E$ l'ensemble $\mathcal{B}(\vec{0}, 1) = \{x\in\K^p, \Vert x\Vert\leq 1\}$. Une norme est définie par sa boule unité.}
\begin{Exe}
Dans $\K^p$, avec $p\in\N, p\geq 1$, pour $(x_1,x_2,..., x_p)\in\K^p, \Vert x\Vert_{\infty} = \max\limits_{1\leq i\leq p} \vert x_i\vert$ est une norme : prenons un vecteur $x = (x_1,..., x_p)$, alors si leur max est nul leur vecteur est nul, pour l'homogénéité, toutes les composantes sont multipliées fonctionne
\par Pour l'inégalité triangulaire, prenons aussi $y= (y_1,..., y_p)$. Alors $\Vert x+y\Vert_\infty =\max\limits_{1\leq i\leq p} \vert x_i+y_i\vert\leq\max\limits_{1\leq i\leq p}(\vert x_i\vert +\vert y_i\vert)\leq\max\limits_{1\leq i\leq p}\vert x_i\vert + \max\limits_[1\leq i\leq p]\vert y_i\vert \leq \Vert x\Vert_\infty + \Vert y\Vert_\infty$
\par Dans $\R^2$, on note $\mathcal{B}(\vec{0}, 1)$ la boule de centre $0$ de rayon 1, telle que $\mathcal{B}(\vec{0}, 1) =\{\vec{x}\in\R^2\vert \Vert x\Vert_\infty\leq 1\}$. Dans $\R^2$ munie de la norme infinie précédente, on obtient un carré et pas un rond, comme on aurait pu l'attendre. Les différentes nrmes sont donc catégorisées par leurs boules unités, ce qui découle de l'homogénéité.
\par On appelle la norme 1 de $x = (x_1,...,x_p)\in\K^p$ l'application : $\Vert x\Vert_1 = \sum\limits_{i=1}^p \vert x_i\vert$, qui est bien une norme. L'inégalité triangulaire vient des propriétés du module. Sa boule unité est un carré dont les sommets sont sur les axes.
\par On appelle la norme 2 de $x = (x_1,...,x_p)\in\K^p$ l'application : $\Vert x\Vert_2 = \sqrt{\sum\limits_{i=1}^p \vert x_i\vert^2}$, qui est bien une norme : la définition est assurée par le fait que si une somme de réels positifs est nulle, ils sont nuls, l'homogénéité se fait par calcul direct (sur les modules, sur les facteurs de sommes, sur la racine carrée) et l'inégalité triangulaire est prouvée par : $\forall x,y\in\C^{2p},\sqrt{\sum\limits_{i=1}^p\vert x_i+y_i\vert^2} \leq \sqrt{\sum\limits_{i=1}^p\vert  x_i\vert^2}+\sqrt{\sum\limits_{i=1}^p\vert y_i\vert^2}$.
\par En considérant $\R^p$ muni du produit scalaire canonique et $\vec{xtilde} = (\vert x_1\vert,..., \vert x_p\vert), \vec{ytilde}=(\vert y_1\vert,...,\vert y_p\vert)$. Alors $\Vert \vec{xtilde}+\vec{ytilde}\Vert_{euclidienne}\leq \Vert\vec{xtilde}\Vert + \Vert\vec{y}\Vert$ donc on a l'égalité précédente.
\par Donc on a trois normes usuelles pour $\K^p$.
\end{Exe}

\begin{Exe}
Si $E$ est de dimension finie, on choisit une base $B = (e_1,..., e_p)$. On pourra parler de la norme infinie, de la norme 1 et de la norme 2 relativement à la base $B$ : si $x$ élément de $E$ avec une décomposition $x\sum\limits_{i=1}^px_ie_i$, alors on définira $\Vert x_i\Vert_\infty = \max\limits_{1\leq i\leq p}\vert x_i\vert, \Vert x\Vert_1 = \sum\limits_{i=1}^p\vert x_i\vert, \Vert x\Vert_2 = \sqrt{\sum\limits_{i=1}^p \vert x_i\vert^2}$
\par Si $E$ est en dimension infinie, par exemple $E=\K[X]$ (réunion dénombrable d'espaces de dimension finie), on peut définir pour $P=\sum\limits_{k=0}^pa_kX^k$ les mêmes normes : $\Vert P\Vert_\infty = \max\limits_{k\in\N} \vert a_k\vert$, $\Vert P\Vert_1 = \sum\limits_{k=0}^{\deg P} \vert a_k\vert$, $\Vert P\Vert_2 = \sqrt{\sum\limits_{k=0}^{\deg P}\vert a_k\vert^2}$
\par Si $E = \mathcal{C}([0,1],\K)$, pour $f\in E$, on considère $\Vert f\Vert_\infty =\sup\limits_{[0,1]}\vert f\vert$ qui est d'ailleurs même un max mais le sup est toujours plus adapté. C'est une norme : elle est définie car si le sup du module de la fonctin est nul, elle est nulle partout ; soit $\lambda\in\K$, montrons l'homogénéité avec deux inégalités :
\par On doit majorer l'ensemble : $\forall x\in[0,1], \vert\lambda f(x)\vert =\vert\lambda\vert\vert f(x)\vert \leq\vert\lambda\vert\sup\limits_{[0,1]}\vert f(x)\vert$ 
\par On doit minorer l'ensemble : si $\lambda=0$, on a $\vert\lambda\vert \sup\vert f\vert\leq \sup\vert\lambda f\vert$. \par Si $\lambda\neq 0$, $\forall x\in[0,1], \vert f(x)\vert \dfrac{1}{\vert\lambda\vert}\vert\lambda\vert \vert f(x)\vert \leq \dfrac{1}{\vert\lambda\vert}\sup\limits_{[0,1]}\vert\lambda\vert\vert f\vert$ \par donc $\sup\vert f\vert\leq\dfrac{1}{\vert\lambda\vert}\sup\limits_{[0,1]}\vert\lambda f\vert$. D'où l'homogénéité.
\par Pour l'inégalité triangulaire : $\sup\limits_{[0,1]}\vert f+g\vert \leq \sup\limits_{[0,1]} \vert f\vert +\vert g\vert\leq \sup\limits_{[0,1]}\vert f\vert +\sup\limits_{[0,1]}\vert g\vert$. Soit $x\in[0,1], \vert f(x)\vert + \vert g(x)\vert\leq \sup\limits_{[0,1]}\vert f\vert + \sup\limits_{[0,1]}\vert g\vert$, voilà l'inégalité prouvée.
\par Pour $\mathcal{C}([0,1,\K)$, on définit pour $f\in\mathcal{C}([0,1],\K), \Vert f\Vert_1 =\int_0^1\vert f(t)\vert dt, \Vert f\Vert_2=\sqrt{\int_0^1\vert f(t)\vert^2dt}$.
\par Pour la norme 1 : soit $f\in E$, supposons que $\Vert f\Vert_1 = 0$, alors on a la définition par positivité (améliorée) de l'intégrale puisqu'on a une fonction continue sur tout le segment $[0,1]$ positive d'intégrale nulle. Le reste est évident.
\par Pour la norme 2 : soit $f, g\in E$, montrons que $\sqrt{\int_0^1\vert f+g\vert^2}\leq\sqrt{\int_0^1\vert f\vert^2} +\sqrt{\int_0^1\vert g\vert^2}$. On en déduit son produit scalaire $<f,g> = \int_0^1 f(t)g(t)dt$, puis on applique Cauchy-Schwarz avec les modules des deux fonctions.
\par Si on prend $E=\K^\N$, alors on peut définir pour $(u_n)\in E$, on a besoin de restreindre aux suites bornées pour que $\Vert u\Vert_\infty = \sup\limits_{n\in\N}\vert u_n\vert$, ou pour que $\Vert u\Vert_1 = \sum\limits_{n=0}^{+\infty} u_n$, il faut se restreindre à l'ev des termes généraux de séries absolument convergentes ($L_1$), tandis que la norme $\Vert u\Vert_2 = \sqrt{\sum\limits_{n=0}^{+\infty} \vert u_n\vert^2}$ n'est bien définie que sur $L_2$, ensemble des suites dont le module au carré est sommable. Mais il faut encore savoir si c'est un ev...
\par $\vert u_n+v_n\vert^2\leq (\vert u_n\vert+\vert v_n\vert)^2\leq \vert u_n\vert^2 +\vert v_n\vert^2 + 2\vert u_n\vert\vert v_n\vert\leq 2(\vert u_n\vert^2 + \vert v_n\vert^2)$
\end{Exe}

\Prop{Produit fini d'EVN}{Avec $E,F$ deux EVN de normes $\Vert.\Vert_E,\Vert.\Vert_F$, alors $E\times F$ est un EVN qui admet comme norme l'application : $\left\{\begin{array}{rcl} E\times F & \to & \R_+ \\ (e,f) & \mapsto & \max (\Vert e\Vert_E,\Vert f\Vert_F) \end{array}\right.$.
\par On peut étendre par récurrence : si $E_1,..., E_p$ un nombre fini d'EVN, on peut définir une norme sur $\prod\limits_{i=1}^p E_i$ par : $\forall (e_1,...,e_p)\in E_1\times E_2\times...\times E_p, \Vert(e_1,...,e_p)\Vert = \max\limits_{i\in\{1,...,p\}} \Vert e_i\Vert$.}
\Def{Distance}{Soit $E$ un EVN. On appelle distance associée à la norme sur $E$ l'application :\par \begin{center}$d:\left\{\begin{array}{rcl} E & \to & \R_+ \\ (x,y) & \mapsto & \Vert x-y\Vert \end{array}\right.$\end{center} \par Une distance définit un espace métrique.}
\Prop{Propriétés de la distance}{\begin{itemize}
\item $\forall x,y\in E, d(x,y) = 0 \Rightarrow x=y$ ;
\item $\forall x,y,z\in E, d(x,z)\leq d(x,y) + d(y,z)$.\end{itemize}}
\Prop{Deuxième forme de l'inégalité triangulaire}{Soit $E$ un EVN, $\forall x,y\in E, \vert\Vert x\Vert-\Vert y\Vert\vert\leq\Vert x\pm y\Vert \leq \Vert x\Vert + \Vert y\Vert$}
\Pre{L'inégalité de droite est déjà une des propriétés des normes. Pour l'inégalité de gauche, elle est équivalente à $\begin{array}{rcl} \Vert x\Vert -\Vert y\Vert & \leq & \Vert x-y\Vert \\ -\Vert x-y\Vert &\leq & \Vert x\Vert - \Vert y\Vert \end{array}$.
\par On la prouve en utilisant $x = x-y+y$ et puis $\Vert x\Vert \leq \Vert x-y\Vert +\Vert y\Vert$}
\Def{Boule ouverte}{Avec $E$ un EVN, soit $x\in E, r\in\R_+$, on appelle la boule ouverte de centre $x$ et de rayon $r$ : $\mathcal{B}(x,r)=\{y\in E, \Vert x-y\Vert<r\}$}
\Def{Boule fermée}{Avec $E$ un EVN, soit $x\in E, r\in\R_+$, on appelle la boule fermée de centre $x$ et de rayon $r$ : $\mathcal{B}_f(x,r)=\{y\in E, \Vert x-y\Vert\leq r\}$}
\Def{Sphère}{Avec $E$ un EVN, soit $x\in E, r\in\R_+$, on appelle la sphère de centre $x$ et de rayon $r$ : $\mathcal{S}(x,r)=\{y\in E, \Vert x-y\Vert=r\}$}
\Def{Partie convexe}{Soit $E$ un $\R$-ev, soit $A\subset E$, on dit que $A$ est convexe si : $\forall x,y\in A, [x,y]\subset A$, où $[x,y] = \{tx + (1-t)y, 0\leq t\leq 1\} = \{y+t(x-y), 0\leq t\leq 1\}$}
\Prop{Convexité des boules}{Soit $E$ un EVN. Toute boule de $E$ est convexe.}
\Pre{Prenons $x\in E, r\in\R_+$ et $\mathcal{B}(x,r)$ la boule ouverte qu'ils définissent. Alors, pour $y,z\in \mathcal{B}(x,r)$, on a que $\Vert x-z\Vert< r$ et $\Vert x- y\Vert<r$. Prenons $\exists t\in[0,1]$, ce qui donne que $ty+(1-t)z\in [y,z]$.
\par On a que $\Vert ty + (1-t)z - x\Vert = \Vert ty - tx + (1-t)z - (1-t)x\leq\vert t\vert\Vert y-x\Vert + \vert 1-t\vert\Vert z-x\Vert < (t+(1-t))r = r$ ce qui est justifié par $t\neq0$ ou $(1-t)\neq0$ et par les inégalités strictes de la boule ouverte.
\par Donc $ty+(1-t)z\in\mathcal{B}(x,r)$.
\par On procède de même pour les boules fermées, sauf qu'on n'a pas besoin de justifier une inégalité stricte.}
\Prop{Séparabilité}{Soit $E$ un EVN. Soient $x,y\in E$ tels que $x\neq y$. Alors il existe deux boules disjointes centrées en $x$ et $y$.}
\Pre{Soient $x,y\in E$ différents. Alors $\Vert x-y\Vert \neq 0$, notons $\Vert x-y\Vert = d$.
\par Prenons les boules ouvertes $B_1 = \mathcal{B}(x,\frac{d}{4}), B_2 = \mathcal{B}(y,\frac{d}{4})$.
\par Soit $z\in B_1$, alors $\Vert z-x\Vert<\frac{d}{4}$.
\par On note que $\Vert z-y\Vert = \Vert z-x+x-y\Vert$, donc $\Vert z-y\Vert\geq\Vert y-x\Vert-\Vert z-x\Vert\geq d-\frac{d}{4}$
\par Donc $\Vert z-y\Vert\geq \dfrac{d}{4}$}

\Def{Normes équivalentes}{Soit $E$ un espace vectoriel, et $\Vert.\Vert_1, \Vert.\Vert_2$ deux normes sur $E$. On dit que $\Vert.\Vert_1$ et $\Vert.\Vert_2$ sont équivalentes si : $\exists \alpha,\beta\in\R_+^*,\forall x\in E, \alpha\Vert x\Vert_1\leq \Vert x\Vert_2\leq \beta\Vert x\Vert_1$, ou si $x\neq0$ : $\alpha\leq\dfrac{\Vert x\Vert_2}{\Vert x\Vert_2}\leq\beta$}
\Prop{Bornes pour normes équivalentes}{Soit $E$ un espace vectoriel avec $\Vert.\Vert_1, \Vert.\Vert_2$ deux normes équivalentes. Toute partie de $E$ bornée pour $\Vert.\Vert_1$ est bornée pour $\Vert.\Vert_2$}
\begin{Exe}
Sur $\K^p$, comparons $\Vert.\Vert_\infty, \Vert.\Vert_1,\Vert.\Vert_2$. Soit $(x_1,..., x_p)\in\K^p$ : $\Vert x\Vert_1=\sum\limits_{i=1}^p\vert x_i\vert\leq p\Vert x\Vert_\infty$ et $\Vert x\Vert_\infty \leq \Vert x\Vert_1$. Donc les deux normes sont équivalentes.
\par Comparsons $\Vert.\Vert_2$ et $\Vert.\Vert_\infty$ : $\Vert x\Vert_2 =\sqrt{\sum\limits_{i=1}^p\vert x\vert^2}\leq \sqrt{p}\Vert x\Vert_\infty$ et $\Vert x\Vert_\infty\leq \Vert x\Vert_2$.
\par On a bien prouvé les équivalences (c'est une relation d'équivalence donc $\Vert.\Vert_1$ et $\Vert.\Vert_2$ aussi), mais on n'a pas forcément les constantes optimales. On a donc besoin d'établir que $\Vert x\Vert_2\leq\Vert x\Vert_1$ (en mettant au carré) et que $\Vert x\Vert_1\leq \sqrt{p} \Vert x\Vert_2$ qui est immédiat en considérant $\Vert.\Vert_1$ comme un produit scalaire avec un vecteur de norme infinie valant 1, et ayant sur chaque coordonnée le même signe.
\par On admet provisoirement le théorème : Dans un espace de dimension finie, toutes les normes sont équivalentes.
\end{Exe}
\begin{Exe}
Prenons les normes qu'on a défini sur $E =\mathcal{C}([0,1],\R)$ (rappel : $\forall f\in E, \Vert f\Vert_\infty = \sup\limits_{[0,1]}\vert f\vert, \Vert f\Vert_1 = \int_0^1\vert f(t)\vert dt$).
\par Pour prouver que deux normes ne sont pas équivalentes, prouvons qu'un ensemble borné pour l'une ne l'est pas pour l'autre (rappel : avec $A\subset E$, $A$ bornée signifique $\exists M\in\R_+^*, \forall x\in A, \Vert x\Vert\leq M$ et $A$ non-bornée signifie $\forall M\in\R_+^*,\exists x\in A, \Vert x\Vert\geq M$). On peut donc prouver la non-bornaison avec l'aide d'une suite tendant vers l'infini.
\par Soit $f\in E$, alors $\int_0^1\vert f\vert \leq\sup\limits_{[0,1]}\vert f\vert (1-0)\leq \Vert f\Vert_\infty$. Cependant, il n'existe pas de constantes permettant une seconde inégalité, donc on peut faire des suites de fonction avec une norme 1 qui tend vers 0 et une norme infinie restant à 1. Par exemple : $\forall n\in\N, f_n:\left\{\begin{array}{rcl} [0,1] & \to & \R \\ x & \mapsto & e^{-nx} \end{array}\right.$. Alors, $\Vert f_n\Vert_1 = \dfrac{1}{n}(1-e^{-n})$ mais $\Vert f_n\Vert_\infty = 1$ donc les deux normes ne sont pas équivalentes.
\end{Exe}

\section{Suites d'un EVN}
\Def{Convergence}{Soit $E$ un EVN sur $\K$, soit $(u_n)\in E^\N$. Soit $l\in E$. On dit que $(u_n)$ converge vers $l$ si $(\Vert u_n-l\Vert)\to 0$
\par On peut aussi écrire :
\par\begin{center} $\forall\varepsilon\in\R_+^*, \exists n_0\in\N,\forall n\in\N, n\geq n_0\Rightarrow u_n\in\mathcal{B}(l, \varepsilon)$\end{center}
\par On parle de suites convergentes et de limites (notées $\lim (u_n)=l$ et $(u_n)\to l$) dans un EVN}
\Thr{Unicité de la limite}{La limite d'une suite convergente est unique.}
\Pre{Supposons qu'une suite soit convergente avec deux limites $l_1, l_2$, on prend deux boules centrées sur les deux qui soient disjointes, puis on applique le fait qu'à partir d'un certain rang, tous les termes de la suite soient dedans. On y trouve une contradiction, et donc la limite est unique.}
\Prop{Convergences équivalentes}{Si E est un ev et que $\Vert.\Vert_1, \Vert.\Vert_2$ sont deux normes de $E$ équivalentes, alors les suites convergentes de $(E, \Vert.\Vert_1)$ sont exactement les suites convergentes de $(E,\Vert.\Vert_2)$}
\Pre{Il existe $\alpha, \beta$ tels que : $\forall x\in E, \alpha \Vert x\Vert_1\leq \Vert x\Vert_2\leq \beta\Vert x\Vert1$.
\par Alors si $u_n\to l$ en norme 1, alors $\Vert u_n - l\Vert_1\to 0$.
\par Comme $\forall n\in\N, \Vert u_n-l\Vert_2 \leq \beta\Vert u_n-l\Vert_1$, alors $(\Vert u_n-l\Vert_2)\to 0$
\par Et de même pour la réciproque.}
\begin{Rem}
Si $\Vert.\Vert_1$ et $\Vert.\Vert_2$ ne sont pas équivalentes, il existe des suites qui convergent pour l'une et pas pour l'autre.
\par En effet, si $\Vert.\Vert_1$ et $\Vert.\Vert_2$ ne sont pas équivalentes, on a soit $x\mapsto\dfrac{\Vert x\Vert_1}{\Vert x\Vert_2}$ non majorée, soit $x\mapsto\dfrac{\Vert x\Vert_1}{\Vert x\Vert_2}$.
\par Dans le premier cas, on peut construire $(u_n)\in E^\N$ telle que $\frac{\Vert u_n\Vert_1}{\Vert u_n\Vert_2}\to+\infty$ (existe par caractérisation séquentielle de la non-majoration). Prenons alors $v_n = \frac{1}{\Vert u_n\Vert_1}u_n$. Alors $\Vert v_n\Vert_2 = \frac{\Vert u_n\Vert_2}{\Vert u_n\Vert_1}\to 0$ et $\Vert v_n-0\Vert_2\to 0$ et donc $v_n$ tend vers 0 au sens de la norme 2 alors qu'elle tend vers 1 au sens de la norme 1.
\par On rappelle que c'est seulement possible dans le cas de la dimension infinie, même si l'on n'a pas encore démontré qu'en dimension finie toutes les normes sont équivalentes.
\end{Rem}
\Prop{Opérations sur les suites convergentes}{Avec $E$ un EVN, $(u_n)$ qui tend vers $l_u$ et $(v_n)$ qui tend vers $l_v$, $\lambda,\mu\in\K$, alors :
\par \begin{center} $\left((\lambda u+\mu v)_n\right)\to \lambda l_u+\mu l_v$.\end{center}}
\Pre{Soit $n\in\N$, alors $\Vert \lambda u_n+\mu v_n-(\lambda l_u + \mu l_v)\Vert \leq \vert\lambda\vert\Vert u_n - l_u\Vert + \vert\mu\vert\Vert v_n-l_v\Vert$.}
\Prop{Corollaire}{L'ensemble des suites convergentes est un sev de l'ensemble des suites d'un EVN.}
\Prop{Extension}{Si $(u_n)\in E^\N$, $(u_n)\to a$ et $(\lambda_n)\in\K^\N$ avec $(\lambda_n)\to\alpha$, alors $\lambda_nu_n\to\alpha a$}
\Pre{Soit $n\in\N$ :
\par $\Vert\lambda_nu_n-\alpha a\Vert=\Vert\lambda_nu_n-\lambda_na+\lambda_na-\alpha a\Vert$
\par $\leq \Vert\lambda_n(u_n-a)\Vert + \Vert(\lambda_n-\alpha)a\Vert$
\par $\leq \vert\lambda_n\Vert u_n-a\Vert + \vert\lambda-\alpha\vert\Vert a\Vert$
\par $\leq \sup\vert\lambda_n\vert \Vert u_n-a\Vert + \vert\lambda_n-\alpha\vert\Vert a\Vert$}
\begin{Rem}
On "rappelle" rapidement ce que sont des algèbres :
\par Une algèbre $(E, +,\times, \cdot)$ est un ensemble munis des lci $+$ et $\times$ et de la lce $\cdot$ tel que que $(E,\K,\cdot)$ est un $\K$-ev et $(E,+,\times)$ est un anneau où on a pour $\lambda\in\K, a,b\in E, \lambda(ab)=a(\lambda b)$
\par $\K[X], (\mathcal{L}(E), +,\circ,\cdot), (\mathcal{M}_n(\K),+, \times,\cdot)$ sont des $\K$-algèbres. De plus, $\K[X]$ est commutatif intègre. $\mathcal{L}(E)$ et $\mathcal{M}_n(\K)$ ne sont ni commutatifs ni intègres (il y a bijection entre les deux).
\end{Rem}
\Def{Algèbre normée}{Soit $(A,+,\times,\cdot)$ une algèbre, on dit qu'elle est normée si on la munit d'une norme $\Vert.\Vert$ vérifiant de plus : $\forall x,y\in A, \Vert x\times y\Vert\leq \Vert x\Vert \Vert y\Vert$}
\Prop{Produit de suites dans une algèbre normée}{Avec $\mathcal{A}$ algèbre normée, soient $(u_n), (v_n)\in \mathcal{A}^\N$ qui tendent vers $x$ et $y$. Alors $(u_nv_n)\to xy$}
\Pre{Soit $n\in\N$ : $\Vert u_nv_n - xy\Vert = \Vert u_nv_n -u_ny+u_ny - xy$
\par $\leq \Vert u_n(v_n-y)\Vert + \Vert (u_n-x)y\Vert$
\par $\leq \Vert u_n\Vert\Vert v_n-y\Vert + \Vert u_n-x\Vert\Vert y\Vert$ (propriété de la norme d'algèbre)
\par $\leq \sup\Vert u_n\Vert\Vert v_n-y\Vert + \Vert u_n-x\Vert \Vert y\Vert$}
\Thr{Convergence dans un produit cartésien fini}{Soient $E_1,...,E_p$ p $\K$-ev normés. Avec $E=E_1\times...\times E_p$, $(u_n)\in E^\N$ s'écrivant $(u_n^1,...,u_n^p)$.
\par Alors $(u_n)$ tend vers $l=(l_1,...l_p)$ si, et seulement si, $\forall i\in\{1,...,p\}, u_n^i\to l_i$}
\Pre{$\forall i\in\{1,...,p\}, \Vert u_n^i - l_i\Vert\leq \Vert u_n-l\Vert$ Donc si $(u_n)$ tend vers $l$, alors $\forall i\in\{1,...,p\}, u_n^i\to l_i$
\par Réciproquement, supposons que $\forall i\in\{1,...,p\}, u_n^i\to l_i$.
\par Soit $\varepsilon\in\R_+^*$.
\par Pour $i\in\{1,...,p\}$, comme $u_n^i\to l_i$ on peut trouver $n_i\in\N, \forall n\geq n_i, \Vert u_n^i-l_i\Vert <\varepsilon$
\par Donc pour $n\geq\max\limits_{i\in\{1,...,p\} } n_i, \Vert u_n-l\Vert<\varepsilon$}
\begin{Rem}
Ce théorème s'applique notamment à $\K^n$ pour $n\in\N^*$ et donc à tout $\K$-ev de dimension finie. Et comme en dimension finie, toutes les normes sont équivalentes, on a la propriété pour les normes qui ne sont pas la norme infinie.
\par On a la propriété : Si $E$ est un $\K$-ev de dimension finie $p$ et $B=(e_1,...,e_p)$ base de de $E$. Alors à toute suite de $E$ $(u_n) =(\sum\limits_{i=1}^pu_n^ie_i)$ on peut associer ses suites coordonnées $(u_n^i)$ dans la base $B$.
\par Donc $(u_n)\to l = \sum\limits_{i=1}^pl_ie_i$ si, et seulement si, $\forall i\in\{1,...,p\}, u_n^i\to l_i$.
\end{Rem}
\begin{Exe}
$\left(\begin{pmatrix} \dfrac{1}{n} & \dfrac{2+n}{1+n} \\ \sin\left(\dfrac{1}{n}\right) & \exp\left(\dfrac{n}{n+1}\right) \end{pmatrix}\right)_{n\in\N} \to \begin{pmatrix} 0 & 1 \\ 0 & e \end{pmatrix}$
\par $\left(\begin{pmatrix} \dfrac{1}{n} & \dfrac{2+n}{1+n} \\ \sin\left(\dfrac{1}{n}\right) & (-1)^n \end{pmatrix}\right)_{n\in\N}$ diverge.
\end{Exe}

\subsection{Séries d'un EVN}
\Def{Série d'un EVN}{Soit $E$ un $\K$-ev normé. Avec $(u_n)\in\K^\N$, on dit que $\sum u_n$ converge si $\left(\sum\limits_{k=0}^nu_k\right)$ converge.}
On a les mêmes propriétés sur le fait que la suite des termes généraux tende vers 0.
\Prop{Absolue convergence en dimension finie}{Si $E$ est de dimension finie, alors toute série absolument convergente est convergente.}
\Pre{Si on fixe une base $B=(e_1,...,e_p)$ de $E$ de dimension finie $p$.
\par Soit $(u_n)\in E^\N$. Pour $n\in\N, u_n = \sum\limits_{i=1}^pu_n^ie_i$.
\par La série $\sum u_n$ converge si, et seulement si, $\forall i\in\{1,...,p\}, \sum u_n^i$ converge.
\par Si $\sum \Vert u_n\Vert$ converge, alors $\sum\Vert u_n\Vert_\infty$ converge (car on est en dimension finie)
\par Or $\forall i\in\{1,...,p\}, \vert u_n^i\leq\Vert u_n\Vert_\infty$
\par Donc par critère de majoration positif, $\sum u_n^i$ converge absolument, donc converge.
\par Donc $\sum u_n$ converge.}

\subsection{Application dans une algèbre normée}
\Prop{Série exponentielle}{Soit $\mathcal{A}$ une algèbre normée de dimension finie. On peut définir pour $u\in\mathcal{A}$ : $\exp(u) = \sum\limits_{n=0}^{+\infty}\frac{1}{n!}u^n$}
\Pre{Prouvons la convergence de $\sum \frac{1}{n!}u^n$ : $\frac{1}{n!}\Vert u^n\Vert\leq \frac{\Vert u\Vert^n}{n!}$ qui est le terme général d'une série convergente. La série exponentielle converge absolument, et comme on est en dimension finie, elle converge aussi dans $\mathcal{A}$.}
\begin{Exe}
Dans $\mathcal{M}_p(\K)$ qui est une algèbre de dimension $p^2$, on peut définir pour $A\in\mathcal{M}_p(\K)$ : $\exp(A)=\sum\limits_{n=0}^{+\infty}\dfrac{1}{n!}A^n$
\end{Exe}
\Prop{Série géométrique}{Avec $\mathcal{A}$ algèbre normée, on peut définir pour $u\in\mathcal{A}$ tel que $\Vert u\Vert < 1$, la somme géométrique $\sum\limits_{n=0}^{+\infty}u^n$. Et si $\Vert u\Vert<1$, alors $1-u$ est inversible et $(1-u)^{-1}=\sum\limits_{n=0}^{+\infty}u^n$}
\Pre{Pour $n\in\N, (1-u)\sum\limits_{k=0}^nu^k = 1-u^{n+1}$ et donc par passage à la limite quand $n\to+\infty$, on a $(1-u)\sum\limits_{k=0}^{+\infty}u^n=1$. Donc $(1-u)$ possède un inverse à droite.
\par De même : $\sum\limits_{n=0}^{+\infty}(1-u)=1$, d'où le résultat.}
\begin{Exe}
Exemple d'anneau où un inverse à droite n'est pas u n inverse à gauche aussi : 
\par $f:\left\{\begin{array}{rcl}\R[X]&\to&\R[X] \\ X^p & \mapsto & X^{p+1}\end{array}\right.$ et $g:\left\{\begin{array}{rcl}\R[X]&\to&\R[X] \\ X^p & \mapsto & \left\{\begin{array}{cl}X^{p-1} & \text{si $p\geq 1$} \\ 0 & \text{sinon}\end{array}\right.\end{array}\right.$
\par On peut alors composer les deux applications, mais selon l'ordre dans lequel on les compose, on n'aura pas nécessairement l'identité.
\end{Exe}

\subsection{Familles sommables d'un K-ev}
\Def{Familles sommables d'un $K$-ev de dimension finie}{Avec $E$ un $\K$-ev de dimension finie muni d'une base $B=(e_1,...,e_p)$, avec $I$ dénombrable et avec $(u_i)_{i\in I}\in E^I$, on dit que $(u_i)_{i\in I}$ est sommable si $\Vert u_i\Vert_{i\in I}$ est sommable.}
\Prop{Sommabilité des familles coordonnées}{Une famille $(u_i)_{i\in I}$ s'écrivant : $\forall i\in I, u_i=\sum\limits_{k=1}^p u_i^k$ est sommable si, et seulement si, $\forall k\in\{1,...,p\}, (u_i^k)_{i\in I}$ est sommable.}
Tous les théorèmes comme la sommations par paquets ou l'indiçage en identifiant $I$ à $\N$ par une bijection sont valides pour toutes les familles sommables de $E$.
\begin{Exe}
Si $\mathcal{A}$ est une algèbre de dimension finie, alors avec $u,v\in\mathcal{A}, uv=vu$ ($u$ et $v$ commutants), on a que $\exp(u+v)=\exp(u)+\exp(v)$ (la commutativité est utilisée lors du binôme de Newton)
\par $\sum\limits_{i\in\N}\frac{1}{i!}u^i\sum\limits_{j\in\N}\frac{1}{j!}u^j = \sum\limits_{(i,j)\in\N^2}\frac{1}{i!}u^i\frac{1}{j!}u^j=\sum\limits_{s\in\N}\frac{1}{s!}\sum\limits_{i+j=s}\frac{s!}{i!j!}u^iv^j=\sum\limits_{s\in\N}\frac{1}{s!}(u+v)^s$
\end{Exe}

\subsection{Suites extraites}
\Def{Suites extraites}{Soit $E$ un $\K$-EVN, avec $(u_n)\in E^\N$. On extrait une suite de $(u_n)$ en se donnant une fonction $\varphi:\N\to\N$ strictement croissante. $(u_{\varphi(n)})_{n\in\N}$ est alors une suite extraite de $(u_n)$.}
\Thr{Convergence des suites extraites}{Si une suite $(u_n)\in E^\N$ converge $l$ alors toute suite extraite de $(u_n)$ converge vers $l$.}
\Thr{Union d'applications d'extraction}{Si $\varphi, \psi$ sont deux applications de $\N$ dans $\N$ strictement croissantes vérifiant $\varphi(\N)\cup\varphi(\N)=\N$, si $(u_{\varphi(n)})$ et $(u_{\psi(n)})$ convergent vers $l$, alors $u_n$ est convergente de limite $l$.}
\Thr{Bolzano-Weierstrass}{De toute suite bornée dans un $\K$-ev de dimension finie on peut extraite une suite convergente.}
\Pre{Soit $E$ un $\R$-ev de dimension finie, avec $B=(e_1,...,e_p)$ une base et $(u_n)\in E^\N$ bornée.
\par Pour $n\in\N, u_n=\sum\limits_{i=1}^pu_n^ie_i$, donc $(u_n^1)\in\R^\N$ est bornée, donc on peut en extraire une suite convergente $(u_{\varphi_1(n)}^1)$ de $(u_n^1)$.
\par Pour $(u_n^2)$, on a encore une suite bornée réelle, on peut en extraire une suite convergente $(u_{\varphi_1\circ\varphi_2(n)}^2)$.
\par On itère ce résultat jusqu'à $(u_{\varphi_1\circ...\circ\varphi_{p-1}(n)}^p)$ qui est une suite bornée de $\R^\N$, donc on peut en extraire la suite $(u_{\varphi_1\circ...\circ\varphi_p(n)}^p)$ convergente.
\par On a donc que $\forall i\in\{1,...,p\}, (u_{\varphi_1\circ...\circ\varphi_p(n)}^i)$ est convergente, donc $(u_{\varphi_1\circ...\varphi_p(n)})$ est convergente.
\par Le cas dans un $\C$-ev en découle par le fait que les $\C$-ev de dimension $p$ soient isomorphes aux $\R$-ev de dimensions $2p$. }


\section{Topologie}
Dans toute cette section, $E$ sera un $\K$-EVN de dimension quelconque
\subsection{Ouverts}
\Def{Point intérieur}{Soit $A\subset E$ et $a\in A$, on dit que $a$ est intérieur à $A$ si $\exists\alpha\in\R_+^*,\mathcal{B}(a,\alpha)\subset A$}
\Def{Intérieur}{Soit $A\subset E$, on appelle intérieur de $A$ l'ensemble des points intérieurs de $A$, noté $\overset{\circ}{A}$ ; on a donc $\overset{\circ}{A}\subset A$}
\begin{Exe}
Soient $x\in E,r\in\R_+^*$, alors $\overset{\circ}{\mathcal{B}_f(x,r)} =\mathcal{B}(x,r)$.
\par Soit $y\in\mathcal{B}(x,r)$. \par Considérons $r'=r-\Vert y-x\Vert$. \par Soit $z\in E$, tel que $\Vert z-y\Vert \leq r'$, alors on a que $\Vert z-x\Vert\leq\Vert z-y\Vert + \Vert y-x\Vert\leq r$ \par Donc $\mathcal{B}(y,r)\subset \mathcal{B}_f(x,r)$ donc $y\in\mathcal{B}_f(x,r)$
\par Pour l'intre inclusion : soit $y\in E$ tel que $\Vert y-x\Vert=r$. Soit $\alpha\in\R_+^*$. \par Considérons $z=y+\frac{\alpha}{2\Vert y-x\Vert}(y-x)$ \par alors $z\in\mathcal{B}(y,\alpha)$ \par $\Vert z-x\Vert = (1+\frac{\alpha}{2\Vert y-x\Vert})\Vert y-x\Vert = r+\frac{\alpha}{2}>r$
\par Donc $z\notin\mathcal{B}_f(x,r)$ \par donc : $\forall\alpha\in\R_+^*, \mathcal{B}(y,\alpha)\cap\mathcal{CB}_f(x,r)\neq\emptyset$
\par Donc $\overset{\circ}{\mathcal{B}_f(x,r)}=\mathcal{B}(x,r)$
\end{Exe}
\Def{Ouvert}{Soit $A\subset E$, on dit que $A$ est ouvert si tous les points de $A$ sont intérieurs (ou si $A$ contient un voisinage de chacun de ses points), ce qui signifie $A\subset\overset{\circ}{A}$}
\Prop{Ouverture de la boule}{Une boule ouverte d'un EVN est un ouvert}
\Pre{Soient $x\in E,r\in\R_+^*$
\par Soit $y\in\mathcal{B}(x,r)$ ; on pose $r'=1-\Vert x-y\Vert$\par Soit $z\in\mathcal{B}(y,r')$ : $\Vert z-x\Vert\leq\Vert z-y\Vert + \Vert y-x\Vert < r -\Vert y-x\Vert + \Vert y-x\Vert < r$ \par Donc $\mathcal{B}(y,r')\subset\mathcal{B}(x,r)$\par Donc $\mathcal{B}(x,r)$ est un ouvert.}
\Prop{Plus grand ouvert}{$\overset{\circ}{A}$ est le plus grand ouvert inclus dans $A$ (au sens de l'inclusion)}
\Pre{Montrons que $\overset{\circ}{A}$ est un ouvert :
\par Soit $x\in\overset{\circ}{A}$ : par définition de $\overset{\circ}{A}$, on peut fixer $\alpha\in\R_+^*$ tel que $\mathcal{B}(x,\alpha)\subset A$ \par Soit $y\in\mathcal{B}(x,\alpha)$ \par $\exists \alpha'\in\R_+^*,\mathcal{B}(y, \alpha')\subset\mathcal{B}(x,\alpha)\subset A$ puisque que $\mathcal{B}(x,\alpha)$ est un ouvert
\par Donc $y\in\overset{\circ}{A}$ \par Donc $\mathcal{B}(x,\alpha)\subset\overset{\circ}{A}$ \par $\overset{\circ}{A}$ est donc un ouvert inclus dans $A$
\par Pour la maximalité : Soit $B$ un ouvert, $B\subset A$. \par Soit $b\in B$ \par Comme $B$ ouvert $\exists\alpha\in\R_+^*,\mathcal{B}(b\alpha)\subset A$ \par $b\in\overset{\circ}{A}$ \par Donc $B\subset\overset{\circ}{A}$}
\Prop{Réunion et intersection d'ouverts}{Avec $E$ un EVN, $(O_i)_{i\in I}$ une famille d'ouverts de $E$, on a que :\begin{itemize}
\item $\cup_{i\in I}O_i$ est un ouvert
\item $\cap_{i\in I}O_i$ est un ouvert à condition que $I$ soit fini\end{itemize}}
\subsection{Adhérence}
\Def{Point adhérent}{Soit $A\subset E$ et $x\in E$, on dit que $x$ est adhérent à $A$ si : $\forall\alpha\in\R_+^*, \mathcal{B}(x,\alpha)\cap A\neq\emptyset$}
\Def{Adhérence}{Soit $A\subset E$, on appelle adhérence de $A$ l'ensemble des points adhérents à $A$, noté $\bar{A}$. On notera que $A\subset\bar{A}$}
\begin{Exe}
On a que $\overset{\circ}{\Q}=\emptyset$ par densité des irrationnels dans les réels, et, symétriquement, $\bar{\Q}=\R$
\end{Exe}
\Thr{Caractérisation séquentielle de l'adhérence}{Soient $A\subset E,x\in E$, alors $x$ est adhérent à $A$ si, et seulement si, $\exists (a_n)\in A^\N, (a_n)\to x$}
\Pre{Procédons par double-implication.
\par Supposons que $x$ soit adhérent à $A$ : alors on peut choisir (axiome du choix) pour $n\in\N$, $a_n\in A\cap\mathcal{B}(x,\frac{1}{n+1})$ \par La suite ainsi construite converge vers $x$ et $(a_n)\in A^\N$
\par Réciproquement : on suppose l'existence de $(a_n)\in A^\N, (a_n)\to x$. \par soit $\alpha\in\R_+^*$ \par comme $(a_n)\to x, \exists n_0\in\N, \forall n\in\N, n\geq n_0\Rightarrow a_n\in\mathcal{B}(x,\alpha)$ \par Donc $\mathcal{B}(x,\alpha)\cap A\neq\emptyset$
\par D'où l'équivalence.}
\Def{Fermé}{Si $A\subset E$, $A$ est fermé si $\bar{A}=A$ (donc que $\bar{A}\subset A$), donc que $A$ contient tous les points qui lui sont adhérents.}
\Thr{Caractérisation séquentielle des fermés}{Soit $A\subset E$, $A$ est fermé si, et seulement si, pour toute suite d'éléments de $A$ convergente vers $l$, $l\in A$. ie $\forall (a_n)\in A^\N, (a_n)\to l\Rightarrow l\in A$}
\Pre{Découle de la caractérisation séquentielle de l'adhérence.}
\begin{Exe}
Soit $A=\{(x,y)\in\R^3\vert x+2y\geq 0\}$, montrons que $A$ est feré. Soit $((\alpha_n,\beta_n))\in A^\N$. \par On soppose que $((\alpha_n, \beta_n))\to (a,b)\in\R^2$ \par Or $\forall n\in\N, \alpha+2\beta\geq 0$ et on a que $(\alpha_n)\to a$ et $(\beta_n)\to b$ \par donc par passage à la limite : $a+2b\geq 0$ \par donc $(a,b)\in A$
\par $\{(x,y,z)\in\R^3\vert x^2+y^2+z^2\leq 1\text{ et } 0\leq z\leq x^2+y^2\}$ est un fermé
\par $\mathcal{S}_p(\R)=\{M\in\mathcal{M}_p(\R)\vert M^T=M\}$, montrons que $\mathcal{S}_p(\R)$ est un fermé de $\mathcal{M}_p(\R)$. \par Soit $(A_n)_{n\in\N}\in\mathcal{S}_p(\R)$. On suppose que $(A_n)\to M\in\mathcal{M}_p(\R)$ \par $(A_n^T)\to M^T$ donc que $\forall i,j\in\llbracket 1,p\rrbracket, [A_n^t]_{i,j}\to [M]_{i,j}$ \par et $\forall n\in\N, A_n^T=A_n$ \par Donc par passage à la limite : $M^T=M$ \par Donc $M\in\mathcal{S}_p(\R)$ \par Donc $\mathcal{S}_p(\R)$ est un fermé.
\par La sphère $S(x,r)=\{y\in E\vert\Vert y-x\Vert=r\}$ est un fermé.
\end{Exe}
\Prop{Fermeture de la boule}{Une boule fermée est un fermé}
\Pre{Soit $x\in E$ \par Soit $r\in\R_+^*$\par Soit $(u_n)\in\mathcal{B}_f(x,r)^\N$ qui tend vers $y\in E$. \par Or $\forall n\in \N, \Vert u_n-x\Vert \leq r$ \par et comme $(u_n)\to y$, on a $(u_n-x)\to y-x$\par On a que la limite de la norme d'une suite est la norme de sa limite par la deuxième inégalité triangulaire : $\vert\Vert v_n\Vert - \Vert m\Vert\vert\leq\Vert v_n-m\Vert$ \par Donc $\Vert u_n-x\Vert\to \Vert y-x\Vert$
\par Et donc par passage à la limite : $\Vert y-x\Vert\leq r$\par donc $y\in\mathcal{B}_f(x,r)$}
\Prop{Produit de fermés}{Tout produit cartésien fini de fermés est un fermé}
\begin{Exe}
Soit $(u_n)\in E^\N$. On dit que $x\in E$ est une valeur d'adhérence de $u$ si $x$ est limite d'une suite extraite de $E$. Par exemple, les valeurs d'adhérence de $(-1)^n$ sont $1$ et $(-1)$ et les valeurs d'adhérence de $\sin(n)$ sont l'ensemble des réels entre $-1$ et $1$. Attention à ne pas confondre les valeurs d'adhérence d'une suite et l'adhérence d'une suite, qui est l'ensemble des points de la suite.
\par On peut reformuler Bolzano-Weierstrass : Toute suite bornée admet au moins une valeur d'adhérence.
\par \underline{\textbf{Exercice :}} Montrer que l'ensemble des valeurs d'adhérence d'une suite est fermé. 
\end{Exe}
\Thr{Complémentarité d'un ouvert}{Soit $E$ un EVN et $A\subset E$, alors $A$ est fermé si, et seulement si, $\mathcal{C}_EA$ est ouvert.}
\Pre{La proposition est équivalente à : $A$ est ouvert si, et seulement si, $\mathcal{C}_EA$ est fermé. On montrera la contraposée des deux implications.
\par Soit $A\subset E$\par On suppose $A$ non-ouvert. On peut donc prendre $a\in A$ tel que $\forall\alpha\in\R_+^*, \mathcal{B}(a,\alpha)\cap \mathcal{C}_EA\neq\emptyset$ \par Donc $a$ est adhérent à $\mathcal{C}_EA$ et $a\notin\mathcal{C}_EA$ \par Donc le complémentaire de $A$ dans $E$ n'est pas fermé.
\par On suppose $A$ non-fermé. On a donc l'existence de $x\in\bar{A}\backslash A$. \par Et donc $\forall \alpha\in\R_+^*,\mathcal{B}(x, \alpha)\in A\neq\emptyset$ \par Donc $x\in\mathcal{C}_EA$ et $x\notin\overset{\circ}{\mathcal{C}_EA}$\par Donc $\mathcal{C}_EA$ n'est pas ouvert.}
"Plutôt que de me prendre l'oreille gauche avec la main droite, je prends $A$" - Sami Chakroun, tranquillement, 2022
\Prop{Réunion et intersection de fermés}{Avec $E$ un EVN, $(F_i)_{i\in I}$ une famille de fermés de $E$, on a que :\begin{itemize}
\item $\cap_{i\in I}F_i$ est un fermé
\item $\cup_{i\in I}F_i$ est un fermé à condition que $I$ soit fini\end{itemize}}
\Pre{Si $(F_i)_{i\in I}$ est une famille de fermés, alors $\mathcal{C}_E\cap_{i\in I}F_i=\cup_{i\in I}\mathcal{C}_EF_i$ qui est ouvert
\par $\mathcal{C}_E\cup_{i\in I}F_i=\cap_{i\in I}\mathcal{C}_CF_i$}
\begin{Exe}
$A=\{(x,y)\in\R^2\vert x^2+y<0\text{ et }x+3y>0\}$ est un ouvert comme son complémentaire est une réunion de fermés.
\par $\cup_{n\in\N^*}\left[\dfrac{1}{n}, 1\right] = ]0,1]$ est une réunion infinie de fermés qui est ouverte.
\end{Exe}
\Def{Densité}{Soit $A\subset E$. On dit que $A$ est dense dans $E$ si $\bar{A}=E$
\par i.e. $\forall x\in E,\exists (a_n)\in A^\N, (a_n)\to x$
\par i.e. Tout élément de $E$ est limite d'une suite d'éléments de $A$}
\begin{Exe}
$\Q$ est dense dans $\R$, $\Q^2$ est dense dans $\R^2$, $\Q\times (\R\backslash\Q)$ est dense dans $\R^2$...
\end{Exe}
\Def{Frontière}{Soit $A\subset E$, on appelle frontière de $A$ : $F_r(A) = \bar{A}\backslash\overset{\circ}{A}$}
\begin{Exe}
Avec $x\in E,r\in\R_+^*$, et donc $F_r(B(x,r))=S(x,r)$
\par On a que $F_r(\Q)=\R$
\end{Exe}
\begin{Rem}
Toutes les notions topologiques évoquées dans ce chapitre sont invariantes par changement de normes équivalentes. En particulier en dimension finie, ces notions sont indépendantes de la norme.
\end{Rem}
\subsection{Relativité vaguement générale quand même}
\Def{Ouvert relatif}{Soit $E$ un EVN, $A\subset E$. Soit $B\subset A$. On dit que $B$ est ou ouvert relatif de $A$ si $\forall x\in B,\exists\alpha\in\R_+^*,\mathcal{B}(x,\alpha)\cap A\subset B$}
\Prop{Ouvert induit}{Soit $E$ un EVN, $A\subset E$. Avec $B\subset A$, $B$ est un ouvert relatif de $A$ si, et seulement si, il existe un ouvert $O$ de $E$ tel que $B=O\cap A$}
\Pre{Supposons $O$ ouvert de $E$ et $B=O\cap A$. \par Soit $x\in B$. Comme $O$ est ouvert, on peut fixer $\alpha\in\R_+^*$ tel que \par $\mathcal{B}(x,\alpha)\subset O$\par Donc $\mathcal{B}(x,\alpha)\cap A\subset B$
\par Réciproquement, si $B$ est un ouvert relatif de $A$, pour $b\in B$, on peut choisir $\alpha_b\in\R_+^*, \mathcal{B}(b,\alpha_b)\cap A\subset B$ \par Donc $B=\cup_{b\in B}(\mathcal{B}(b,\alpha_b)\cap A)=\left(\cup_{b\in B}(\mathcal{B}(b,\alpha_b))\right)\cap A$ \par Donc il existe un ouvert (réunion d'ouverts) dont l'intersection avec $A$ donne $B$.}
\begin{Exe}
Avec $E=\R$ et $A=]0,1]$. $B=\left]\frac{1}{2},1\right]$ n'est pas un ouvert de $E$ car $1\in B$ et $1$ n'est pas intérieur à $B$. Mais c'est un ouvert relatif de $A$ car $B=\left]\frac{1}{2},2\right[\cap A$.
\end{Exe}
\Def{Fermé relatif}{Soit $E$ un EVN, $A\subset E$. Soit $B\subset A$. On dit que $B$ est un fermé relatif de $A$ s'il vérifie l'une des propriétés équivalentes suivantes :\begin{enumerate}
\item $\mathcal{C}_AB$ est un ouvert relatif de $A$ ($\mathcal{C}_AB=A\backslash B=A\cap\mathcal{C}_EB$) ;
\item $\exists F$ fermé de $E$ tel que $B=F\cap A$ ;
\item Caractérisation séquentielle : pour toute suite $(b_n)\in B^\N$ convergente de limite $l\in A$, alors $l\in B$. \end{enumerate}}

\subsection{Compacts}
\Def{Compact}{Soit $E$ un EVN, soit $A\subset E$. On dit que $A$ est compact si de toute suite de $A$, on peut extraire une suite convergente dans $A$.}
\begin{Exe}
Les segments sont des compacts de $\R$, les réunions de segments sont des compacts, les intervalles semi-ouverts ne sont pas des compacts.
\par Pour $a,b\in\R, a<b$, $[a,b]$ est un compact de $\R$ mais $[a,b[$ n'est pas compact. $[a,b]$ est bornée, donc Bolzano-Weierstrass pour l'existence d'une suite convergente, qui a bien une limite dans le segment $[a,b]$ par passage à la limite et inégalités larges. Cependant, cette inégalité large mène à ce que $b$ soit une limite possible, alors que $b$ n'est pas dans l'ensemble.
\end{Exe}
\Prop{Fermeture du compact}{Tout compact est fermé et borné.}
\Pre{Soit $K$ un compact. \par Soit $(k_n)\in K^\N$ une suite convergent vers $l\in E$. \par Comme $K$ est compact, on peut extraire de de $(k_n)$ une suite $(k_{\varphi(n)})$ de limite dans $K$. \par Or comme $(k_n)$ est convergente de limite $l$, alors $(k_{\varphi(n)})\to l$ donc $l\in K$. \par Donc $K$ est fermé.
\par Supposons $K$ non-borné. \par On peut construire une suite $(k_n)$ de $K$ telle que $\Vert k_n\Vert\to+\infty$ (comme $K$ est non-borné, pour tout $n\in\N$, on peut choisir $k_n\in K$ tel que $\Vert k_n\Vert\geq n$) \par Pour toute suite extraite de $(k_n)$ $(k_{\varphi(n)})$, la suite $\Vert k_{\varphi(n)}\Vert\to+\infty$ donc $(k_{\varphi(n)})$ diverge. \par Donc $K$ est non-compact.}
\Prop{Produit cartésien de compacts}{Avec $E_1,..., E_p$ des EVN, $E=E_1\times E_2\times...\times E_p$ et $K_1,..., K_p$ des compacts, alors $K_1\times K_2\times...\times K_p$ est un compact.}
\Pre{Soit $(x_n)\in (K_1\times K_2\times...\times K_p)^\N$ (ie $(x_n)= (x^1_n, x^2_n,..., x^p_n)$), et on refait la démonstration de BW. La seule différence c'est qu'au lieu d'avoir des suites scalaires, on a des suites de vecteurs.
\par $K_1$ est compact, donc on peut extraire de $(x^1_n)$ une suite $(x_{\varphi_1(n)^1})$ convergente dans $K_1$. Ensuite $(x_{\varphi_2(n)}^2)\in K_2^\N$, qui est compact, donc on peut extraire de $(x_{\varphi_1(n)}^2)$ une suite $(x_{\varphi_1\circ\varphi_2(n)}^2)$ convergente dans $K^2$. \par Au final on aura une suite extraite $(x_{\varphi_1\circ...\circ\varphi_p})$ qui converge.}
\Prop{Fermé d'un compact}{Un fermé dans un compact est compact (ou un fermé relatif d'un compact, même si tout fermé relatif d'un compact serait un fermé)}
\Pre{Soit $K$ un compact et $B\subset K$ fermé. \par Soit $(x_n)\in B^\N$, alors $(x_n)\in K^\N$. \par Alors on peut extraire $(x_{\varphi(n)})$ de $(x_n)$ convergente vers $l\in K$. \par Comme $B$ est fermé, $l\in B$. \par Donc $B$ est compact.}
\Thr{Compacts}{Dans un espace de dimension finie $E$, les compacts sont les fermés bornés.}
\Pre{On a déjà vu que tout compact est fermé borné. Réciproquement : \par Soit $A$ un fermé borné. Soit $(x_n)\in A^\N$ \par $A$ est bornée, donc par Bolzano-Weierstrass en dimension finie, on peut trouver une suite extraite $(x_{\varphi(n)})$ qui converge vers $l\in E$. \par Comme A est fermé, $l\in A$. \par Donc $A$ est compact.}
\begin{Exe}
Une boule fermée en dimension finie est compacte. En dimension infinie, il est possible de démontrer qu'une boule fermée n'est jamais compacte. \par Une sphère est compacte en dimension finie.
\par On prend $E$ un $\K$-ev et $A\subset E, A\neq\emptyset$. Pour $x\in E$, on définit $d(x,A)=\inf\limits_{a\in A}\Vert x-a\Vert$. \par 1) Montrer que si $A$ est compact, $d(x, A)$ est atteinte. \par 2) Montrer que si $E$ est de dimension finie et $A$ est fermée, alors $d(x, A)$ est atteinte. \par 3) Pour $A,B\subset E$ on définit $d(A,B)=inf\{\Vert a-b\Vert, a\in A, b\in B\}$. \par a) Montrer que si $A$ et $B$ sont compactes, $d(A,B)$ est atteinte. \par Si $E$ de dim finie, $A$ compact et $B$ fermé, $d(A,B)$ atteinte. \par c) Si $A$ et $B$ sont fermés, $d(A, B)$ est-elle atteinte ?
\begin{enumerate}
\item Dans $\R$, $\sup A\in\bar A$ et $\inf A\in\bar{A}$. Ici, $d(x,A)=\inf\{\Vert x-a\Vert \vert a\in A\}$ donc on peut trouver $(a_n)\in A^\N$ telle que $d(x,A)=\lim\limits_{n\to+\infty}\Vert x-a_n\Vert$. \par $A$ est un compact, et comme $(a_n)$ est à valeurs dans un compact, on peut extraire de $(a_n)$ une suite convergente $(a_{\varphi(n)})$ de limite $\alpha\in A$. \par $(\Vert x-a_{\varphi(n)}\Vert)\to d(x,A)$ \par Et $\Vert x-a_{\varphi(n)}\Vert \to \Vert x-\alpha\Vert$ \par Donc la distance est atteinte. 
\item Avec $E$ de dimension finie et $A$ fermé, $d(x,A)=\lim\limits_{n\to+\infty}\Vert x_n-a_n\Vert$ avec $(a_n)\in A^\N$ qu'on prend. \par $\forall n\in\N,\Vert a_n\Vert\leq \Vert x\Vert + \Vert x-a_n\Vert$ \par $(\Vert x-a_n\Vert)$ est convergente donc bornée \par Donc $(a_n)$ est bornée. \par Donc par Bolzano-Weierstrass, on peut extraire de $(a_n)$ une suite $(a_{\varphi(n)})$ convergente vers $l\in E$. Comme $A$ est fermée et que $(a_n)$ est une suite convergente de $A$, $l\in A$. \par Donc $d(x,A)=\Vert x-l\Vert$
\item \begin{itemize}
\item On a $A$ et $B$ deux compacts, donc on prend $(a_n)\in A^\N, (b_n)\in B^\N$ telle que $d(A,B)=\lim\limits_{n\to +\infty} \Vert a_n-b_n\Vert$ \par Comme $A$ est borné, on peut prendre $(a_{\varphi(n)})$ convergente vers $l\in A$. \par De $(b_{\varphi(n)})$ bornée, on peut extraire la suite convergente $(b_{\varphi\circ\psi(n)})$ qui tend vers $l'\in B$ \par Donc $\left(\Vert a_{\varphi\circ\psi(n)}-b_{\varphi\circ\psi(n)}\right)\to \Vert l-l'\Vert  = d(A,B)$ \par Donc la distance est atteinte.
\item Avec $E$ de dimension finie, $A$ compact et $B$ fermé. On a que $d(A,B)=\lim\limits_{n\to+\infty} \Vert a_n-b_n\Vert$ en ayant pris $(a_n)\in A^\N$ et $b_n\in B^\N$. \par $A$ est compact, donc on extrait de $(a_n)$ la suite $(a_{\varphi(n)})$ convergente vers $l\in A$. \par Donc $d(A,B)=\lim\limits_{n\to+\infty}\Vert a_{\varphi(n)}-b_{\varphi(n)}$ \par Et comme $\forall n\in\N, \Vert b_{\varphi(n)}\Vert\leq\Vert a_{\varphi(n)}\Vert +\Vert b_{\varphi(n)}-a_{\varphi(n)}\Vert$, alors $b_{\varphi(n)}$ est bornée. \par Donc par BW (car la dimension est finie), on peut extraire de $(b_{\varphi(n)})$ une suite extraite $(b_{\varphi\circ\psi(n)})$ convergent vers $l'$. \par Comme $B$ est fermé, $l'\in B$. Donc $d(A,B)=\Vert l-l'\Vert$
\item Prenons un contre-exemple en dimension infinie, dans l'espace vectoriel des courbes de fonctions. Avec $A = \{(x,e^x)\vert x\in\R\}$ et $B=\{(x,-e^x)\vert x\in\R\}$ on a une distance de $0$, bien que les deux courbes ne se rencontrent jamais. $O_n(\R)$ (groupe orthogonal) n'est plus du tout au programme mais permettrait d'avoir un exemple d'un compact.
\end{itemize}
\end{enumerate}
\end{Exe}
\Prop{Convergence d'une suite compacte}{Une suite d'un compact est convergente si, et seulement si, elle admet une unique valeur d'adhérence}
\Pre{Soit $A$ un compact. \par Montrons la première implication : soit $(u_n)\in A^\N$ convergente. \par Alors elle admet une unique valeur d'adhérence.
\par Réciproquement, montrons la contraposée : toute suite divergente dans un compact admet au moins deux valeurs d'adhérence. \par Soit $(u_n)\in A^\N$ divergente. \par $A$ est compact, donc $(u_n)$ possède une valeur d'adhérence $\lambda$. \par Mais $(u_n)$ ne tend pas vers $\lambda$, on a donc : $\exists\varepsilon\in\R_+^*,\forall n_0\in\N,\exists n\in\N,n\geq n_0\Rightarrow\Vert u_n-\lambda\Vert>\varepsilon$ \par Donc $\{n\in\N\vert \Vert u_n-\lambda\Vert>\varepsilon\}$ est infini. \par Donc on peut extraite de $(u_n)$ une suite $(u_{\varphi(n)})$ telle que $\forall n\in\N, \Vert u_{\varphi(n)}-\lambda\Vert>\varepsilon$ \par Mais $(u_{\varphi(n)})$ est une suite de $A$, donc elle possède une valeur d'adhérence $\lambda'\neq\lambda$, avec $\lambda'\in A$.}

\section{Limites de fonctions}
\subsection{Définitions}
\Def{Limite}{Soient $E,F$ deux EVN, et $A\subset E$. Soit $f\in\mathcal{F}(A,F)$, $x_0\in\bar{A}$. Soit $l\in F$. On dit que $f$ converge vers $l$ en $x_0$ si :
\par \begin{center} $\forall\varepsilon\in\R_+^*,\exists\alpha\in\R_+^*,\forall x\in\mathcal{B}(x_0,\alpha)\cap A, f(x)\in\mathcal{B}(l,\varepsilon)$ \end{center}
\par On notera $f\to_{x_0} l$ ou $f(x)\to_{x\to x_0} l$ (notation plus abusive) ou $\lim f = l$ (notation plus adaptée à une conclusion) et $\lim\limits_{x\to x_0}f(x)=l$ (le combo que Chakroun aime pas).}
\Def{Vocabulaire du voisinage}{Avec $E$ un EVN et $b\in E$, on appelle voisinage de $b$ un ensemble contenant une boule ouverte centrée en $b$.
\par Avec ce vocabulaire, $f\to_{x_0} l$ si, pour tout voisinage de $l$ $\mathcal{V}_l$, il existe un voisinage relatif de $x_0$ $\mathcal{V}_{x_0}$ tel que $f(\mathcal{V}_{x_0})\subset\mathcal{V}_l$
\par On étend la définition de voisinage à $+\infty$ :
\par Dans $E$ un EVN quelconque, $\mathcal{V}$ est un voisinage de $+\infty$ est si $\exists A\in\R_+^*,\{x\in E, \Vert x\Vert>A\}\subset\mathcal{V}$.
\par Dans $\R$ on définit des voisinages de $\pm\infty$
\par $\mathcal{V}$ est un voisinage de $+\infty$ si $\exists A\in\R_+^*, ]A,+\infty[\subset\mathcal{V}$
\par $\mathcal{V}$ est un voisinage de $-\infty$ si $\exists B\in\R_-^*, ]-\infty, B[\subset\mathcal{V}$
\par On peut aussi étendre la notion de convergence en $+\infty$ (ou $-\infty$)}
\begin{Exe}
$f\to_{+\infty} l$ donne que $\forall\varepsilon\in\R_+^*,\exists M\in\R_+^*,\forall x\in A, \Vert x\Vert\geq M\Rightarrow \Vert f(x)-l\Vert<\varepsilon$
\end{Exe}
\Prop{Caractérisation séquentielle de la limite}{Avec $f\in\mathcal{F}(A,F)$ avec $A\subset E$ et $E,F$ deux EVN. Avec $x_0\in\bar{A}$ et $l\in F$, alors :
\par \begin{center}$f\rightarrow_{x_0}l\Leftrightarrow \forall (u_n)\in A^\N, (u_n)\to x_0, (f(u_n))\to l$ \end{center}}
\Pre{On suppose que $f$ tend vers $l$ en $x_0$ : \par Soit $(u_n)\in A^\N$ tel que $(u_n)\to x_0$ \par Soit $\varepsilon\in\R_+^*$ \par $f\to_{x_0} l$, on peut trouver $\alpha\in\R_+^*, \forall x\in A\cap\mathcal{B}(x_0,\alpha), \Vert f(x)-\alpha\Vert<\varepsilon$ \par Or $(u_n)\to x_0$ donc on peut trouver $n_0\in\N,\forall n\in\N, n\geq n_0\Rightarrow \Vert u_n-x_0\Vert<\alpha$ \par Donc pour $n\geq n_0$ : $\Vert f(u_n)-l\Vert<\varepsilon$
On suppose que $\forall (u_n)\in A^\N, (u_n)\to x_0, (f(x_0))\to l$ : on prouve la contraposée : \par $f$ ne tend pas vers $l$ en $x_0$ donc on peut construire une suite $(u_n)$ tendant vers $x_0$ telle que $(f(u_n))$ ne tend pas vers $l$. \par On peut donc prendre un $\varepsilon\in\R_+^*$ tel que $\forall\alpha\in\R_+^*,\exists x\in A\cap\mathcal{B}(x_0,\alpha)\text{ et }\Vert f(x)-l\Vert>\varepsilon$ \par Pour $n\in\N^*$, on peut donc choisir $u_n\in A\cap\mathcal{B}(x_0,\frac{1}{n})$ et $\Vert f(u_n)-l\Vert>\varepsilon$ \par On a donc construit $(u_n)\in A^\N$ tel que $\forall n\in\N, \Vert u_n-x_0\Vert<\varepsilon\frac{1}{n}$ et $\Vert f(u_n)-l\Vert>\varepsilon$}
\begin{Exe}
$f:\left\{\begin{array}{rcl} \R^2\backslash\{(0,0)\} & \to & \R \\ (x,y) & \mapsto & \frac{xy}{\vert x\vert+\vert y\vert} \end{array}\right.$
\par $f$ a-t-elle une limite en $(0,0)$ ? \par Soient $(x,y)\in\R^2\backslash\{(0,0)\}$, alors $\vert f(x,y)\vert\leq \frac{(\vert x\vert + \vert y\vert)^2}{\vert x\vert + \vert y\vert}\leq \vert x\vert+\vert y\vert$ \par Donc $f\to_{(0,0)}0$
\par $f:\left\{\begin{array}{rcl} \R^2\backslash\{(0,0)\} & \to & \R \\ (x,y) & \mapsto & \frac{x^\alpha y^\beta}{\Vert(x,y)\Vert^\gamma} \end{array}\right.$
\par $f$ a-t-elle une limite en $(0,0)$ ? \par On a l'équivalence des normes en dimension finie, donc $\exists K_1,K_2>0, \frac{K_1\vert x^\alpha y^\beta\vert}{\Vert(x,y)\Vert_\infty^\gamma}\leq\vert f(x,y)\leq\frac{K_2 \vert x^\alpha y^\beta\vert}{\Vert(x,y)\Vert_\infty^\gamma}$ \par Dans un premier cas, où $\alpha+\beta>\gamma$ : $\frac{\vert x^\alpha\vert\vert y^\beta\vert}{\Vert(x,y)\Vert_\infty^\gamma}\leq\Vert(x,y)\Vert_\infty^{\alpha+\beta-\gamma}$ \par et donc $f\to_{(0,0)}0$
\par Dans un second cas, $\alpha+\beta=\gamma$ : Soit $\lambda\in\R$, \par considérons la suite $(u_n)=((\frac{1}{n},\frac{\lambda}{n}))_{n\in\N}$ \par Alors $f(u_n)=\dfrac{(\frac{1}{n})^{\alpha+\beta}\lambda^\beta}{\vert\frac{1}{n}\vert^\gamma\Vert(1,\lambda)\Vert^\gamma} = \dfrac{\lambda^\beta}{\Vert(1,\lambda)\Vert^\gamma}$, une constante qui dépend de $\lambda$ en général. \par Donc on n'a pas de limite en $(0,0)$ 
\par Dans un troisième cas, $\alpha+\beta<\gamma$ : \par Prenons la suite, pour $n\in\N : f\left(\left(\frac{1}{n},\frac{1}{n}\right)\right)=\dfrac{(\frac{1}{n})^{\alpha+\beta-\gamma}}{\Vert(1, 1)\Vert^\gamma}\to+\infty$ \par Donc $f$ n'a pas de limite en $(0,0)$. \par Attention, on n'a pas nécessairement $f\to_{(0,0)}+\infty$ :
\par prenons le cas particulier de $f:\left\{\begin{array}{rcl} \R^2\backslash\{(0,0)\} & \to & \R \\ (x,y) & \mapsto & \frac{x^2 y}{(\vert x\vert+\vert y\vert)^4} \end{array}\right.$ \par On a alors, pour $n\in\N^*$ : $f\left(\left(\frac{1}{n},\frac{1}{n^2}\right)\right)=\dfrac{\frac{1}{n^4}}{2^4\frac{1}{n^4}}\to\frac{1}{2^4}$ qui tend bien vers $0$.
\par Avec $f:\left\{\begin{array}{rcl} \R^2\backslash D & \to & \R \\ (x,y) & \mapsto & \frac{x^{12}y^{15}}{x+y} \end{array}\right.$ et avec $D=\{(x,y)\in\R^2\vert x+y=0\}$. Est-ce que $f$ admet une limite en $(0,0)$ ? \par On prend la suite $\vert f\left(\frac{-1}{n}, \frac{1}{n}+\frac{1}{n^{100}}\right)\vert\sim\dfrac{\frac{1}{n^{27}}}{\frac{1}{n^{100}}}\to+\infty$
\end{Exe}

\subsection{Opérations sur les limites}
\Prop{Limite d'une fonction à valeurs dans un produit cartésien}{Soit $f:A\to F_1\times F_2\times...\times F_p$ avec $(F_i)$ famille d'EVN et $A\subset E$ une partie d'un EVN. On a alors $f:x\mapsto (f_1(x),f_2(x),... f_p(x))$ et on peut caractérister $f$ par ses fonctions composantes $f_i:A\to F_i$
\par Soit $x_0\in\bar{A}$ et $l=(l_1,...,l_p)\in F_1\times...\times F_p$, alors $f\to_{x_0} l$ si, et seulement si, $\forall i\in\{1,...,p\}, f_i\to_{x_0} l_i$.
\par En particulier, si $F$ est un EVN de dimension finie ramené à une base $B=(u_1, u_2,...,u_p)$, $F$ est isomorphe à $\K^p$ par la bijection $\sum\limits_{i=1}^px_iu_i\mapsto (x_1,...,x_p)$. Si $f\in\mathcal{F}(A,F) : x\mapsto f(x)=\sum\limits_{i=1}^pf_i(x)u_i$, on appelle les $f_i$ les applications coordonnées et $f$ a une limite si, et seulement si, les applications coordonnées ont des limites.}
\Prop{Opérations sur les fonctions convergentes}{\begin{itemize}
\item Avec $f,g\in\mathcal{F}(A,E)$, on a que $f$ converge et $g$ converge $\Rightarrow f+g$ converge et $\lim(f+g) = \lim f+\lim g$
\item Avec $\lambda\in\mathcal{F}(A,\K), f\in\mathcal{F}(A,F)$, $\lambda$ converge et $f$ converge $\Rightarrow \lambda f$ converge et $\lim(\lambda f) =(\lim\lambda)(\lim f)$
\item Si $F$ est une algèbre munie d'une norme d'algèbre : Si $f,g\in\mathcal{F}(A,F)$, $g$ converge et $f$ converge $\Rightarrow fg$ converge et $\lim (f\times g)=(lim f)\times(\lim g)$
\item Si $F=\K$, alors avec $f\in\mathcal{F}(A,F)$, si $f$ est convergente de limite $l\neq0$, alors $\frac{1}{f}$ converge et $lim\frac{1}{f}=\frac{1}{l}$
\item Avec $E,F,G$ des EVN, avec $A\subset E$ et $B\subset F$, $f\in\mathcal{A,F}, g\in\mathcal{F}(B,G)$, $x_0\in\bar{A}, y_0\in\bar{G}, f\to_{x_0} y_0, g\to_{y_0}l$, alors on peut affirmer que $g\circ f\to_{x_0} l$
\end{itemize}}
\Pre{Pour la composition : soit $\mathcal{V}_l$ un voisinage de $l$, comme $g\to_{y_0} l$, on peut trouver un voisinage relatif à B de $y_0$ $\mathcal{V}_{y_0}$ tel que $g(\mathcal{V}_{y_0})\subset\mathcal{V}_l$. \par Comme $f\to_{x_0}y_0$, on peut trouver un voisinage relatif à A de $x_0$ $\mathcal{V}_{x_0}$ tel que $f(\mathcal{V}_{x_0})\subset\mathcal{V}_{y_0}$ \par Donc $(g\circ f)(\mathcal{V}_{x_0})\subset\mathcal{V}_l$}
\begin{Rem}
Cette démonstration s'étend au cas $\Vert x_0\Vert=+\infty, \Vert y_0\Vert=+\infty$ 
\end{Rem}

\subsection{Continuité de fonctions}
\Def{Continuité}{Soit $f\in\mathcal{F}(A,F)$ avec $A\subset E$ et $E,F$ deux EVN. Soit $a\in A$, on dit que $f$ est continue en $a$ si $f\to_a f(a)$.}
\begin{Rem}
C'est une notion locale ; deux fonctions coïncidant sur un voisinage ont les mêmes propriétés de continuité.
\end{Rem}
\Prop{Opérations}{Localement, la somme de deux fonctions continues est continue, le produit par un scalaire d'une fonction continue est continue, le produit d'une fonction continue et d'une fonction scalaire continue est continu, dans un algèbre le produit de deux fonctions continues est continu, la composition de deux fonctions continues est continue.}
\begin{Rem}
Toutes les notions vues sur les limites de fonctions s'étendent sur les fonctions continues en $a$, notamment sur les fonctions coordonnées.
\end{Rem}
\Prop{}{Si $E$ est de dimension finie associé à une vase $B=(e_1,...,e_p)$, alors :
\par \begin{center} $P_j:\left\{\begin{array}{rcl} E & \to & \K \\ x=\sum\limits_{i=1}^px_ie_i & \mapsto & x_j \end{array}\right.$\end{center}
\par Alors : $\forall x,y\in E, x=\sum x_ie_i, y=\sum y_ie_i, \vert p_j(x) - p_j(y)\vert\leq\Vert x-y\Vert_\infty$ et l'application projection est une application continue en tous points. (1-lipschitzienne donc uniformément continue)}
\begin{Exe}
$f:\left\{\begin{array}{rcl} \R^2 & \to & \R^3 \\ (x,y) & \mapsto & \left(\frac{\arctan(xy^2sin(x+y))}{\sqrt{x^2+y^2+1}}, \frac{\log(1+x^2y^4)}{\cosh(x+3y)},x+y\right) \end{array}\right.$
\par Cette fonction est continue car toutes ses coordonnées sont continues, en tant que produits, sommes et compositions de fonctions continues. En effet, $x$ et $y$, en tant que projecteurs, sont nécessairement continus.
\par $f:\left\{\begin{array}{rcl} \mathcal{M}_n(\K) & \to & \R \\ A & \mapsto & \det(A) \end{array}\right.$ \par est une application continue en tant que polynôme avec comme coefficients des coordonnées.
\par $f:\left\{\begin{array}{rcl} E & \to & \R \\ x & \mapsto & \Vert x\Vert \end{array}\right.$ \par est continue car par seconde inégalité triangulaire ($\forall x,y\in E, \vert\Vert x\Vert-\Vert y\Vert\vert\leq\Vert x-y\Vert$), elle est lipschitzienne.
\par Avec $E$ un espace préhilbertien réel :
\par $f:\left\{\begin{array}{rcl} E\times E & \to & \R \\ (u,v) & \mapsto & \langle u,v\rangle \end{array}\right.$ \par est continue, montrons-le, même quand $E$ n'est pas de dimension finie :
\par En $(0,0)$ : Montrons que $\langle h,k\rangle\to_{(0,0)}0$ : $\vert\langle h,k\rangle\vert\leq\Vert h\Vert \Vert k\Vert$ par Cauchy-Schwarz donc $f$ est continue en $(0,0)$ \par En $(u_0,v_0)$ quelconques : $\vert\langle u_0+h,v_0+k\rangle - \langle u_0,v_0\rangle\vert = \vert\langle h,v_0\rangle+\langle k,u_0\rangle+\langle h,k\rangle\vert\leq\Vert u_0\Vert\Vert h\Vert+\Vert v_0\Vert\Vert k\Vert+\Vert h\Vert\Vert k\Vert$ \par Donc $f$ est continue en $(u_0,v_0)$ et est donc continue sur $E\times E$.
\end{Exe}


\subsection{Propriétés globales des fonctions continues}
\Prop{Images réciproques}{Soit $f\in\mathcal{F}(A,F)$ continue, alors l'image réciproque d'un ouvert de $F$ par $f$ est un ouvert relatif de $A$
\par L'image réciproque d'un fermé de $F$ par $f$ est un fermé relatif de $A$.}
\Pre{Pour les fermés : \par Soit $B$ un fermé de $F$, on veut démontrer que $f^{-1}(B)$ est un fermé relatif de $A$. \par Soit $(x_n)\in (f^{-1}(B))^\N$, on suppose que $(x_n)\to l\in A$. \par Alors pour $n\in\N, f(x_n)\in B$ et $f$ est continue en $l$ donc $f(x_n)\to f(l)$\par Donc $(f(x_n))$ est une suite convergente du fermé $B$, donc sa limite est dans $B$ \par Donc $f(l)\in B$, donc $l\in f^{-1}(B)$ \par Donc $f^{-1}(B)$ est bien un fermé relatif de $A$.
\par Pour les ouverts : \par Soit $O$ un ouvert de $F$, $\mathcal{C}_FO$ est un fermé de $F$, donc $f^{-1}(\mathcal{C}_FO)$ est un fermé relatif de $A$.\par Donc $\mathcal{C}_Af^{-1}(O)$ est un fermé relatif de $A$ \par Donc $f^{-1}(O)$ est un ouvert relatif de $A$.}
\begin{Exe}
$A=\{(x,y)\in\R^2\vert\sin(x)+2y<0\text{ et }x+3y>0\text{ et }\arctan(\frac{x}{x^2+y^2+1})>0\}$ est l'image réciproque par $f:(x,y)\mapsto\left(\sin(x)+2y, x+3y, \frac{x}{x^2+y^2+1}\right)$ de $(\R_-^*, \R_+^*,\R_+^*)$, qui est un ouvert. Or $f$ est continue, donc $A$ est ouvert.
\end{Exe}
\Prop{Continuité coïncidante}{Deux applications continues qui coïncident sur une partie dense sont égales}
\Pre{Soient $f,g\in\mathcal{C}(A,F)$ avec $A\subset E$, $E,F$ des EVN, $B$ une partie dense dans $A$, et $\forall x\in B, f(x)=g(x)$ \par Soit $x\in A$, alors on prend $(x_n)\in B^\N,(x_n)\to x$ qui existe par densité de $B$ \par $\forall n\in\N, f(u_n)=g(u_n)$ et par passage à la limite (comme $f$ et $g$ sont continues) : $f(u_n)\to f(x)$ et $g(u_n)\to g(x)$ \par Et donc par unicité de la limite, $f(x)=g(x)$}
\begin{Exe}
On recherche la partie de $\mathcal{C}(\R,\R)$ telles que : $\forall x,y\in\R, f(x+y)=f(x)+f(y)$. \par On prouve d'abord que $f(0)=0$ et que $\forall x\in\R,\forall n\in\N, f(x)=nf(x)$, puis on l'étend à $\Z$ et enfin à $\Q$. Et ensuite on se sert du théorème pour conclure
\end{Exe}
\Def{Uniforme continuité}{Soit $f\in\mathcal{F}(A,F)$ où $A\subset E$ avec $E,F$ deux EVN. On dit que $f$ est uniformément continue si :
\par\begin{center} $\forall\varepsilon\in\R_+^*,\exists\eta\in\R_+^*,\forall x,y\in A,\Vert x-y\Vert<\eta\Rightarrow\Vert f(x)-f(y)<\varepsilon\Vert$\end{center}}
\begin{Rem}
La continuité uniforme implique la continuité. \par On peut trouver un contre-exemple à la réciproque, comme $f:\left\{\begin{array}{rcl} \R & \to & \R \\ x & \mapsto & e^x\end{array}\right.$ \par La négation de la continuité uniforme, c'est que : $\exists\varepsilon\in\R_+^*,\forall\eta\in\R_+^*,\exists x,y\in A, (\Vert x-y\Vert<\eta)\text{ et }(\Vert f(x)-f(y)\Vert\geq\varepsilon)$
\par C'est peu exploitable, donc on fait une caractérisation séquentielle de la non-continuité uniforme : $\exists\varepsilon\in\R_+^*,\exists(x_n),(y_n)\in A^\N, (\Vert x_n-y_n\Vert)\to 0\text{ et }\Vert f(x_n)-f(y_n)\geq\varepsilon$ \par On note P1 la négation et P2 la caractérisation séquentielle. Prouvons leur équivalence. \par Supposons P1, et prenons $\varepsilon\in\R_+^*$ vérifiant la propriété. 
\par Soit $(\eta_n)=(\frac{1}{n+1})$, prenons $(x_n), (y_n)\in A^\N$ telles que :\par $\Vert x_n-y_n\Vert<\eta_n\text{ et }\Vert f(x_n)-f(y_n)\Vert\geq\varepsilon$ \par On a ainsi défini $(x_n),(y_n)$ avec $(\Vert x_n-y_n)\to 0\text{ et }\Vert f(x_n)-f(y_n)\Vert\geq\varepsilon$
\par L'autre implication est immédiate. \par On reprend $f$ telle que définie plus haut, on note $(x_n)=(n), (y_n)=(n+\frac{1}{n})$. \par On a $(x_n-y_n)\to 0$ 
\par Alors $e^{x_n}-e^{y_n}=e^{n}-e^{n+\frac{1}{n}}=e^{n}(1-e^{\frac{1}{n}})=e^{n}(\frac{1}{n}+o(\frac{1}{n}))=\frac{e^{n}}{n}+o(\frac{e^{n}}{n})$ \par Or ça tend vers $+\infty$, donc on a bien une fonction non-uniformément continue.
\par "L'exponentielle ça amplifie à mort, donc on s'en fout." - Chakroun, 2022
\end{Rem}

\subsection{Fonctions lipschitziennes}
\Def{Fonctions lipschitziennes}{Soit $f\in\mathcal{F}(A,F)$, où $A\subset E$ avec $E,F$ EVN. Soit $k\in\R_+^*$. On dit que $f$ est $k-lipschitzienne$ si :
\par \begin{center} $\forall x,y\in A,\Vert f(x)-f(y)\Vert\leq k\Vert x-y\Vert$ \end{center}}
\Prop{Uniforme continuité des fonctions lipschitziennes}{Toute fonction lipschitzienne est uniformément continue}
\Pre{Le $\eta$ qu'on prend est le $k$.}
\begin{Rem}
Dans $\R$, si $f$ est dérivable sur un intervalle $I$, $f$ est lipschitzienne si, et seulement si, sa dérivée est bornée. \par Si $f'$ est bornée, on conclut par inégalité des accroissements finis. \par Si $f$ est déribale, on passe à la limite de l'expression de la dérivée.
\end{Rem}
\begin{Exe}
Donnons un exemple de fonction uniformément continue non-lipschitzienne. \par Prenons $f:\left\{\begin{array}{rcl} [0,1] & \to & \R \\ x & \mapsto & \sqrt{r} \end{array}\right.$ \par Alors $f$ est uniformément continue mais non lipschitzienne (elle est dérivable de dérivée non-bornée)
\end{Exe}
\Prop{Distance lipschitzienne}{Avec $E$ un EVN, $A\subset E$, l'application $\begin{array}{rcl} E & \to & \R \\ x & \mapsto & d(x,A)\end{array}$ est $1-lipschitzienne$.}
\Pre{Soient $x,y\in E$. \par Soit $a\in A$ \par $\Vert y-a\Vert\geq\Vert x-a\Vert-\Vert y-x\Vert\geq d(x,A)-\Vert x-y\Vert$ qui est constante \par donc : $d(y,A)\geq d(x,A)-\Vert x-y\Vert$ \par La fonction est donc lipschitzienne}
\Thr{Théorème de Heine}{Toute fonction continue sur un compact est uniformément continue.}
\Pre{Avec $E,F$ deux EVN, $A$ un compact de $E$, $f\in\mathcal{f}(A,F)$ non-uniformément continue. \par On peut donc fixer $\varepsilon\in\R_+^*$ et prendre deux suites $(x_n), (y_n)\in A^\N$, avec $(\Vert x_n-y_n\Vert)\to 0\text{ et }\forall n\in\N, \Vert f(x_n)-f(y_n)\Vert>\varepsilon$ \par Comme $A$ est compact, on peut extraire de $(x_n)$ une suite $(x_{\varphi(n)})$ convergente vers $a\in A$. Comme $(\Vert x_n-y_n\Vert)\to 0$, alors $(y_{\varphi(n)})\to a$ et $(f(x_{\varphi(n)})-f(y_{\varphi(n)})$ ne tend pas vers 0. \par Donc $f$ n'est pas continue en $a$.}
\Thr{Bornes atteintes le retour}{L'image d'un compact par une application continue est un compact.}
\Pre{Soit $f\in\mathcal{F}(A,F)$ avec $A\subset E$ un compact. Montrons que $f(A)$ est compact. \par Soit $(y_n)\in f(A)^\N$ \par Alors pour $n\in\N$, on peut prendre $x_n\in A, f(x_n)=y_n$. \par Donc $(x_n)\in A^\N$ \par $A$ est compact donc on peut extraire de $(x_n)$ une suite $(x_{\varphi(n)})$ convergente vers $a\in A$. \par Comme $f$ est continue, la suite $\left(f(x_{\varphi(n)})\right)\to f(a)$ \par Donc $(y_{\varphi(n)})\to f(a)$ et par compacité de $A$, $f(a)\in f(A)$. \par Donc $f(A)$ est compact.}
\Prop{Corollaire}{Le théorème des bornes atteintes : si $f\in\mathcal{C}(A,\R)$ avec $A$ compact, alors $f$ est borné et "atteint ses bornes". ie $f$ est bornée ($f(A)$ est compact donc bornée). \par $\exists a\in A, \sup f = f(a)$ \par $\exists b\in A,\sup f=f(b)$}
\begin{Exe}
Soit $f:\left\{\begin{array}{rcl}[0,1]^2 & \to & \R \\ (x,y) & \to & e^{x^2\sin(3y)\arctan(x+y)}\end{array}\right.$
\par Montrer que $\exists d\in\R_+^*, \forall (x,y)\in [0,1]^2,f(x,y)\geq d$. \par Par compacité comme $[0,1]^2$ compact, la borne inférieure de f est atteinte et est supérieure à $0$,, d'om l'existence de $]0,f(x,y)]$
\end{Exe}
\Prop{Corollaire}{Si $f\in\mathcal{C}(A,F)$, avec $A$ compact de $F$, l'application $\Vert f\Vert:\left\{\begin{array}{rcl} A&\to&\R \\ x&\mapsto&\Vert f(x)\Vert\end{array}\right.$ atteint ses bornes.}
\begin{Exe}
Soit $f:\left\{\begin{array}{rcl} \R^2 & \to & \R \\ (x,y) & \mapsto & (2x^4+3y^2+1)e^{-(x^2+3y^2)} \end{array}\right.$ \par $\forall x,y\in\R^2, f(x,y)\geq 0$, donc $f$ est minorée et admet une borne inférieure. \par Avec $x\in\R$, $f(x,0) = (2x^4+1)e^{-x^2}$ et alors $\left(f(x,0)\right)\to 0$ donc $\inf f=0$ 
\end{Exe}

\subsection{Continuité des applications linéaires}
\Thr{Critère de continuité des applications linéaires}{Soit $E, F$ des $\K$-EVN et $f\in\mathcal{L}(E,F)$. $f$ est continue si, et seulement si, elle vérifie l'une des propriétés équivalentes suivantes :\begin{enumerate}
\item $f$ est continue en $0$ ;
\item $\exists k\in\R_+^*,\forall x\in E, \Vert f(x)\Vert_F\leq k\Vert x\Vert_E$ ;
\item $f$ est lipschitzienne.\end{enumerate}}
\Pre{$f$ continue implique $1$ de manière immédiate. \par Pour $1\Rightarrow 2$ : supposons $f$ continue en $0$. \par Donc on peut prendre $\alpha\in\R_+^*$, $\forall x\in E, \Vert x\Vert<\alpha\Rightarrow\Vert f(x)\Vert < 37$ \par Soit $x\in E, x\neq 0$, prenons $u=\frac{\alpha}{2\Vert x\Vert}x$ \par On a bien $\Vert u\Vert<\alpha$ \par Donc $\Vert f(u)\Vert<37$ \par Donc par linéarité de $f$ et homogénéité de $\Vert.\Vert$ : $\Vert f(x)\Vert\leq\frac{37}{2}<\Vert x\Vert$
\par $2\Rightarrow 3$ : On suppose $\exists k\in\R_+^*, \forall x\in E,\Vert f(x)\Vert \leq k\Vert x\Vert$ \par Donc $\forall x,y\in E, \Vert f(x)-f(y)\Vert\leq \Vert f(x-y)\Vert\leq k\Vert x-y\Vert$}
\begin{Exe}
Soit $E=\mathcal{C}([0,1],\R)$, qu'on munit de $\Vert.\Vert_\infty$ et $\Vert.\Vert_1$. \par On cherche à connaître la continuité de l'application linéaire : \par $\varphi:\left\{\begin{array}{rcl} E & \to & \R \\ f & \mapsto & f(0) \end{array}\right.$ \par On peut avoir la continuité si le rapport $\dfrac{\vert\varphi(f)\vert}{\Vert f\Vert}$ pour $f\neq 0$ est borné. \par Soit $f\in E$\par Alors $\vert f(0)\vert \leq \Vert f\Vert_\infty$ donc $\varphi$ est continue pour la norme infinie.
\par Pour la norme 1, considérons $(f_n)_{n\in\N^*}$ telle que : $f_n : t\mapsto e^{-nt}$ \par $\forall n\in\N^*,\varphi(f_n)=1 \text{ et }\Vert f_n\Vert = \frac{1}{n}(1-e^{-n})$ \par Et donc $\frac{\vert \varphi(f_n)\vert}{\Vert f_n\Vert}\to +\infty$ et donc si $E$ est muni de $\Vert.\Vert_1$, alors $\varphi$ n'est pas continue.\end{Exe}
\begin{Rem}
En dimension finie, toute application linéaire est continue, par continuité des projecteurs (les applications linéaires sont des polynômes de degré au plus 1 sur les coordonnées).
\par Si $\varphi$ est linéaire et injective en dimension finie, on a que $\Vert\varphi\Vert$ est une norme, qu'on appelle la norme $\varphi$.
\end{Rem}
\Prop{Fermeture des sev}{Tout sous-espace de dimension finie d'un $\K$-ev est fermé.}
\Pre{Soit $F$ un sev de $E$, $F$ de dimension finie. \par Si $E$ est de dimension finie, on peut considérer $G$ un supplémentaire de $F$. Alors la projection $p_G$ sur $G$ parallèlement à $F$ est continue puisqu'on est en dimension finie. \par Et comme $F=\ker p_G = p_g^{-1}(\{0_E\})$ donc $F$ est fermé.
\par Le résultat reste vrai si $E$ n'est pas de dimension finie : \par Soit $f_n$ une suite de $F$ convergente vers $l\in E$. \par $(f_n)$ est convergente donc bornée, donc il existe une boule fermée $B$ contenant l'ensemble des $f_n$ pour $n\in\N$ \par $\forall n\in\N, f_n\in B\cap F$ \par $B\cap F$ est une boule fermée de $F$ pour la restriction de $\Vert.\Vert$ à $F$, et comme $F$ est de dimension finie, $B\cap F$ est un compact. \par Donc on peut extraire de $(f_n)$ une suite convergent vers $l'\in B\cap F$ \par Et par unicité de la limite, $l=l'$ donc $l\in F$.}
\begin{Rem}
Si $F,G$ deux sev de $E$, tels que $F\oplus G=E$, notons : $p_F$ la projection sur $F$ parallèlement à $G$ et $p_G$ la projection sur $G$ parallèlement à $F$. \par Si $p_F$ et $p_G$ sont continues, alors $F$ et $G$ sont fermés.
\end{Rem}
\begin{Exe}
Prenons $E=\mathcal{C}([0,1],\Vert 1\Vert)$ et $\varphi : f\mapsto f(0)$ \par Alors $F =\{f\in E,\varphi(f)=0\}$ est un hyperplan, mais $F$ n'est pas fermé.
\end{Exe}
\Prop{Norme triple}{Soit $E,F$ deux EVN, on appelle $\mathcal{L}_C(E,F$) l'ensemble des applications linéaires continues de $E$ dans $F$. Alors $\mathcal{L}_C(E,F)$ est un espace vectoriel normé pour la norme $\varphi\mapsto \vert\Vert\varphi\Vert\vert=\sup\limits_{x\in E, x\neq 0} \frac{\Vert \varphi(x)\Vert_F}{\Vert x\Vert_E} = \sup\limits_{x\in E, \Vert x\Vert=1}\Vert \varphi(x)\Vert$}
\Pre{soit $\varphi\in\mathcal{L}_C(E,F)$\par On suppose $\vert\Vert\varphi\Vert\vert=0$ alors $\forall x\in E, x\neq 0, \Vert \varphi(x)\Vert\leq 0$ donc $\varphi(x)=0$ \par Et comme $\varphi$ est linéaire, $\varphi(0)=0$ donc $\varphi=0$ \par L'inégalité triangulaire et l'homogénéité sont immédiates.}
\begin{Rem}
Dans le cas où $E$ et $F$ sont de dimensions finies : $\mathcal{L}_C(E,F)=\mathcal{L}(E,F)$ \par et $\mathcal{L}_C(E,F)$ est isomorphe à $\mathcal{M}_{n,p}(\K)$ \par Si on fixe une norme dans $\mathcal{M}_{n,1}(\K)$ et une norme dans $\mathcal{M}_{p,1}(\K)$, il est naturel de parler de la norme $\vert\Vert.\Vert\vert$ associée pour $\mathcal{M}_{n,p}(\K)$ \par $A\in\mathcal{M}_{n,p}(\K), \vert\Vert A\Vert\vert =\sup\limits_{\Vert X\Vert=1, X\in\mathcal{M}_{p,1}(\K)} \Vert AX\Vert_n = \sup\limits_{X\in\mathcal{M}_{n,1}(\K), X\neq 0} \frac{\Vert AX\Vert_n}{\Vert X\Vert_p}$
\end{Rem}
\begin{Exe}
$\mathcal{M}_{n,1}(\K)$ muni de la norme $\Vert.\Vert_\infty$, déterminons la norme triple sur $\mathcal{M}_n(\K)$ \par $\vert\Vert A\Vert\vert=\sup\limits_{X\in\mathcal{M}_{n,1}(\K), x\neq 0} \frac{\Vert AX\Vert_\infty}{\Vert X\Vert_\infty}$ \par $\Vert AX\Vert_\infty=\max\limits_{1\leq i\leq n}\vert\sum\limits_{j=1}^n A_{i,j}X_j\vert$ \par $\vert\sum\limits_{j=1}^n A_{i,j}X_j\vert\leq \sum\limits_{j=1}^n \vert A_{i,j}X_j\vert\leq\left(\sum\limits_{j=1}^n \vert A_{i,j}\vert\right)\Vert X\Vert_\infty\leq \max\limits_{1\leq i\leq n}\left(\sum\limits_{j=1}^n\vert A_{i,j}\vert\right) \Vert X\Vert_\infty$
\par Pour prouver qu'on a bien la meilleure majoration possible, on doit trouver un exemple de vecteur $X$ pour que l'inégalité soit une égalité. \par Prenons $i_0$ tel que $\sum\limits_{j=1}^n\vert A_{i_0}, j\vert=\max\limits_{i}\sum\limits_{j=1}^n\vert A_{i,j}\vert$ \par Prenons $X=(e^{i\theta_1},..., e^{i\theta_n})$ avec $\theta_1,...,\theta_n\in\R$ tel que $\theta_j=-\arg(A_{i_0,j})$ \par Alors pour tout $j\in\llbracket 1,n\rrbracket$ : $A_{i_0,j}e^{i\theta_j} = \vert A_{i_0,j}\vert$ \par Donc $\Vert AX\Vert_\infty=\max\limits_{1\leq i\leq n}\left(\sum\limits_{j=1}^n\vert A_{i,j}X_j\vert\right)\Vert X\Vert_\infty$
\par Donc $\vert\Vert A\Vert\vert = \max\limits_{1\leq i\leq n} \sum\limits_{j=1}^n\vert A_{i,j}\vert$
\par Faisons de même pour $\Vert.\Vert_1$ : soit $X\in\mathcal{M}_{n,1}(\K)$ \par $\Vert AX\Vert_1 = \sum\limits_{i=1}^n\vert\sum\limits_{j=1}^nA_{i,j}X_j\vert\leq \sum\limits_{i=1}^n\sum\limits_{j=1}^n\vert A_{i,j}X_j\vert\leq \sum\limits_{j=1}^n\vert X_j\vert \sum\limits_{i=1}^n\sum \vert A_{i,j}\vert \leq \left(\max\limits_{1\leq i\leq n} \sum\limits_{i=1}^n\vert A_{i,j}\vert\right)\sum\limits_{k=1}^n X_k$ \par Donc $\vert\Vert A\Vert\vert\leq\left(\max\limits_{1\leq i\leq n} \sum\limits_{i=1}^n\vert A_{i,j}\vert\right)\sum\limits_{k=1}^n X_k$
\par Pour la majoration atteinte, on prend le $j_0\in\llbracket 1,n\rrbracket$ tel que $\sum\limits_{i=1}^n\vert A_{i,j_0}\vert =\max\limits_{j}\sum\limits_{i=1}^n\vert A_{i,j}\vert$ \par Soit $X=(0_1,...,0, 1, 0,...,0)$ avec le $1$ en position $j_0$ \par $\Vert AX\Vert_1 = \sum\limits_{i=1}^n\vert A_{i,j_0}\vert$ donc $\vert\Vert A\Vert\vert =\max\limits_{1\leq j\leq n}\sum\limits_{i=1}^n\vert A_{i,j}\vert$ 
\end{Exe}
\Thr{Sous-multiplicativité}{Soit $E,F,G$ des EVN, soit $f\in\mathcal{L}_C(E,F)$ et $g\in\mathcal{L}_C(F,G)$, alors $\vert\Vert g\circ f\Vert\vert\leq \vert\Vert g\Vert\vert\vert \Vert f\Vert\vert$}
\Pre{Soit $x\in E$, tel que $\Vert x\Vert=1$ \par Alors $\Vert g\circ f(x)\Vert = \Vert g(f(x))\Vert \leq \Vert g\Vert \Vert f(x)\Vert$ par définition de $\Vert.\Vert$ dans $\mathcal{L}_C(E,F)$ \par Et $\Vert g\Vert \Vert f(x)\Vert\leq\Vert g\Vert \Vert f\Vert \Vert x\Vert\leq \Vert g\Vert \Vert f\Vert$ \par Donc $\sup\limits_{\Vert x\Vert =1}\Vert g\circ f(x)\Vert\leq \Vert g\Vert \Vert f\Vert$}
\Prop{Corollaire}{$\mathcal{L}_C(E)$ est munie naturellement d'une norme d'algèbre.}

\subsection{Continuité des applications p-linéaires}
\Def{Application p-linéaire}{Soit $f\in\mathcal{F}(E_1\times...\times E_p, F)$ est dite p-linéaire si : \par $\forall x_1,...,x_p\in E_1\times...\times E_p, \forall j\in \llbracket 1,p\rrbracket, x_j\mapsto f(x_1,..., x_j,..., x_p)$ est linéaire.}
\begin{Exe}
Le déterminant est une application $n$-linéaire de $(\K^n)^n$ \par Le produit scalaire dans une espace préhilbertien est bilinéaire.
\end{Exe}
\Prop{Critère de continuité des applications multilinéaires}{Soit $E_1,..., E_p, F$ des EVN, soit $f$ p-linaire de $E_1\times ...\times E_p$ dans $F$. \par $f$ est continue si, et seulement si, $\exists k\in\R_+^*,\forall x_1,...,x_p\in E_1\times...\times E_p, \Vert f(x_1,..., x_p)\Vert\leq k\Vert x_1\Vert ...\Vert x_p\Vert\leq k\Vert x\Vert^p$}
\Pre{Supposons $f$ continue, donc $f$ est continue en $0$. \par Donc on peut prendre $\alpha\in\R_+^*$ tel que $\forall x\in E_1\times...\times E_p, \Vert x\Vert \leq\alpha\Rightarrow \Vert f(x)\Vert \leq 1$ \par Soit $x=(x_1,..., x_p)\in E_1\times...\times E_p$, on suppose pour tout $i\in\llbracket 1,p\rrbracket x_i\neq 0$ \par Posons $u=(\frac{\alpha}{\Vert x_1\Vert}x_1,\frac{\alpha}{\Vert x_2\Vert}x_2,..., \frac{\alpha}{\Vert x_p\Vert})$, alors $\Vert u\Vert\leq \alpha$
\par Donc $\Vert f(u)\Vert\leq 1$ \par Par p-linéarité de $f$, on a $\Vert f(x)\Vert \leq \frac{1}{\alpha^p}\Vert x_1\Vert...\Vert x_p\Vert\leq \frac{1}{\alpha^p}\Vert x\Vert^p$ \par L'inégalité reste vraie si $x_1=0$ ou $x_2=0$ ou... ou $x_p=0$.
\par Réciproquement, on suppose que : $\exists k\in\R_+^*, \forall x_1,..., x_p\in E_1\times...\times E_p, \Vert f(x_1,..., x_p)\leq k\Vert x_1\Vert...\Vert x_p\Vert$ \par Soit $x=(x_1,...,x_p)\in E_1\times...\times E_p$ \par Soit $h=(h_1,...,h_p)\in E_1\times...\times E_p$ tel que $\Vert h\Vert\leq\Vert x\Vert$ \par $f(x+h)-f(x) =f(x_1+h_1,..., x_p+h_p)-f(x_1,...,x_p)$ \par $=\sum\limits_{y_1\in [x_1,h_1], y_p\in [x_p, h_p]} f(y_1,...,y_p)-f(x_1,...,x_p)$ \par On obtient une somme de $2^{p-1}$ termes, chaque terme de la forme $f(y_1,..., y_p)$ où au moins un des $y_i$ est égale à $h_i$
\par Pour un tel terme : $\Vert f(y_1,..., y_p)\Vert\leq k\Vert y_1\Vert...\Vert y_p\Vert\leq k\Vert h\Vert \Vert x\Vert^{p-1}$ \par Donc $\Vert f(x+h)-f(x)\Vert\leq K(2^p-1)\vert x\Vert^{p-1}\Vert h\Vert$ \par Donc $f(x+h)-f(x)\to_{h\to 0} 0$ \par Donc $f$ est continu en $x$.} 
Attention, ça ne veut pas dire que toute fonction multilinéaire continue est lipschitzienne, elle l'est juste sur des voisinages.
\Prop{Application}{Si $E$ est un espace préhilbertien réel, alors $\begin{array}{rcl} E\times E & \to & \R \\ (x,y) & \mapsto & \langle x,y \rangle\end{array}$ est continue ($\forall x,y\in E, \langle x,y\rangle \leq \Vert x\Vert\Vert y\Vert$)}

\section{Connexité par arcs}
\Def{Chemin}{Soit $E$ un EVN, $x,y\in E$, on appelle chemin de $x$ à $y$ une application continue $\varphi$ de $[0,1]$ dans $E$ telle que $\varphi(0)=x$ et $\varphi(1)=y$}
\begin{Rem}
Dans la définition, $0$ et $1$ n'ont pas d'importance. Si on a $\varphi:[a,b]\to E$ continue telle que $\varphi(a)=x$ et $\varphi(b)=y$ \par On a $\psi :\left\{\begin{array}{rcl} [0,1] & \to & [a,b] \\ t & \mapsto & a+t(b-a)\end{array}\right.$ \par Alors $\varphi\circ\psi$ est continue, $\varphi\circ\psi(0)=x$ et $\varphi\circ\psi(1)=y$
\end{Rem}
\Def{Chemin dans une partie}{Si $A\subset E$, si $x,y\in A$ un chemin $\varphi$ est un chemin dans $A$ si $\forall t\in [0,1],\varphi(t)\in A$}
\Prop{Relation d'équivalence}{Avec $E$ un EVN, $A\subset E$, on définit sur $A$ la relation $x\mathcal{R}_Ay \Leftrightarrow$ il existe un chemin dans $A$ joignant $x$ à $y$. \par $\mathcal{R}_A$ est une relation d'équivalence.}
\Def{Connexes}{On appelle composantes connexes les classes d'équivalences par la relation précédente \par Un ensemble est connexe par arc si $\mathcal{R}_A$ a une seule classe d'équivalence.}
\begin{Exe}
Les convexes sont tous connexes par arcs. La réciproque est en générale fausse, sauf sur $\R$. \par Les parties étoilées ($A$ est étoilée s'il existe $c\in A$ tel que $\forall x\in A, [x,c]\subset A$) sont connexes par arcs. \par Les connexes par arcs de $\R$ sont les intervalles. $\R\backslash \{a\}$ n'est pas connexe. $\R^2\backslash D$ avec $D$ une droite n'est pas connexe. $\R^2\backslash \{a\}$ est connexe par arcs. 
\end{Exe}
\Thr{Valeurs intermédiaires}{L'image d'un connexe par arcs par une application continue est connexe par arcs.}
\Pre{Soient $E,F$ EVN, $A\subset E$ et $f\in\mathcal{C}(A,F)$, et $B\subset A$ connexe par arcs. \par Soit $y_1, y_2\in f(B)$ on prend donc $x_1, x_2\in B$ tels que $f(x_1)=y_1$ et $f(x_2)=y_2$ \par Comme $B$ est connexe par arcs on a $\varphi : \left\{\begin{array}{rcl} [0,1] & \to & B \\ t & \mapsto & \varphi(t) \end{array}\right.$ continue avec $\varphi(0)=x_1$ et $\varphi(1)=x_2$ \par Alors $f\circ\varphi$ est un chemin dans $f(B)$ joignant $y_1$ à $y_2$ \par Donc $f(B)$ est connexe par arc.}
\begin{Exe}
$\mathcal{GL}_n(\R)$ n'est pas connexe par arcs : $\det(\mathcal{GL}_n(\R))=\R^*$ \par Il n'existe aucune application injective continue de $\R^2$ dans $\R$ : \par Supposons par l'absurde $f:\R^2\to \R$ continue et injective. \par L'image de $\R^2$ par $f$ est alors connexe par arcs, donc un intervalle $I$. \par Notons $(x,y)\in \R^2$ tel que $f(x,y)\in\overset{\circ}{I}$. Alors $f(\R^2 \backslash \{(x,y)\})$ est privé de $f(x,y)$ par injectivité et n'est donc pas l'intervalle $I$, d'où la contradiction comme $\R^2\backslash\{(x,y)\}$ est connexe par arc.
\par Montrons que $\mathcal{GL}_n(\C)$ est connexe par arc. \par Soit $M\in\mathcal{GL}_n(\C)$, on prend $A\in\mathcal{M}_n(\C)$ telle que $\exp(A)=M$ \par Notons : $P :\left\{\begin{array}{rcl} [0,1] & \to & \mathcal{GL}_n(\C) \\ t & \mapsto & \exp(tA) \end{array}\right.$ \par Alors $P(0)=I$ et $P(1)=M$, donc $\mathcal{GL}_n(\C)$ est connexe par arc. \par Pour $\mathcal{GL}_n(\R)$, il y a au moins deux composantes connexes : celles avec un déterminant positif et celles avec un déterminant négatif.
\end{Exe}

\section{Théorème d'équivalence des normes}
\Thr{L'équivalence des normes}{Toutes les normes sont équivalentes en dimension finie}
\Pre{Soit $E$ un $\K$-ev de dimension finie, avec $B=(e_1,...,e_p)$ une base de $E$. \par Soit $\Vert.\Vert_\infty\left\{\begin{array}{rcl} E&\to&\K \\ x=\sum\limits_{i=1}^px_ie_i & \mapsto & \max\limits_{1\leq i\leq p}\vert x_i\vert \end{array}\right.$
\par Soit $N$ une norme sur $E$ \par Pour $x\in E$, en notant $x=\sum\limits_{i=1}^px_ie_i$ : \par $N(x)\leq\sum\limits_{i=1}^p\vert x_i\vert N(e_i)\leq\Vert x\Vert_\infty\sum\limits_{i=1}^p N(e_i)$ \par Or, par la seconde inégalité triangulaire : \par $\forall x,y\in E, \vert N(x)-N(y)\vert\leq N(x-y)\leq K\Vert x-y\Vert_\infty$, où $K$ est une constante. \par Si on considère $(E,\Vert.\Vert_\infty)$, on a donc que $N$ est lipschitzienne, et donc continue.
\par On a que $\mathcal{S}_\infty(0,1)=\{x\in E\vert \Vert x\Vert_\infty=1\}$ est compact. \par Donc $N$ étant continue, elle atteint ses bornes supérieures et inférieures sur $\mathcal{S}_\infty(0,1)$. \par Donc $m=\inf\limits_{x\in\mathcal{S}(0,1)}N(x)>0$ \par Et donc $\forall x\in E,x\neq 0\Rightarrow N\left(\frac{1}{\Vert x\Vert_\infty}x\right)\leq m\Rightarrow N(x)\geq m\Vert x\Vert_\infty$
\par D'où l'équivalence entre $N$ quelconque et $\Vert.\Vert_\infty$. Par transitivité de la relation d'équivalence des normes, on a donc que toutes les normes en dimension finie sont équivalentes.}



\chapter{Réduction des endomorphismes}
\section{Généralités}
\Def{Valeur propre}{Soit $E$ un $\K$-ev et $f\in\mathcal{L}(E)$. Soit $\lambda \in\K$. \par On dit que $\lambda$ est une valeur propre de $f$ si : $f-\lambda id$ n'est pas injective. \par Ce qui correspond à $\ker (f-\lambda id)\neq\{0\}$ \par Ce qui correspond à $\exists x\in E, x\neq 0, (f-\lambda id)(x)=0$ \par Ce qui correspond à $\exists x\in E,x\neq 0, f(x)-\lambda x=0$}
\begin{Rem}
On étend la notion de valeurs propres à l'ensemble des matrices $\mathcal{M}_n(\K)$ : pour $A\in\mathcal{M}_n(\K)$, $\lambda$ est une valeur propre de $A$ si $\lambda$ est une valeur propre de l'endomorphisme canoniquement associé. \par Ce qui correspond à $\ker (A-\lambda I_n) \neq \{0\}$ \par Ce qui correspond à $\exists X\in\mathcal{M}_{n,1}(\K), X\neq 0, (A-\lambda I_n)(X) = 0$ \par Ce qui correspond à $\exists X\in\mathcal{M}_{n,1}(\K), X\neq 0, AX=\lambda X$
\end{Rem}
\Def{Spectre d'un endomorphisme}{Soit $E$ un $\K$-ev et $f\in \mathcal{L}(E)$. On appelle spectre de $f$ l'ensemble des valeurs propres de $f$, noté $S_p(f)$. \par On peut étendre la notion aux matrices.}
\Prop{}{Deux matrices semblables ont même spectre.}
\begin{Exe}
Si $A$ est triangulaire supérieure avec une diagonale $\lambda_1,..., \lambda_n$, alors $S_p(A)=\{\lambda_1,...,\lambda_n\}$. Attention, un spectre n'est pas forcément de cardinal $n$.
\par Soit $E=\mathcal{C}^\infty(\R,\C)$ et $\varphi:\left\{\begin{array}{rcl} E & \to & E \\ f & \mapsto & f' \end{array}\right.$ \par Déterminons les valeurs propres de $\varphi$ \par Supposons $\lambda\in S_p(\varphi) \Leftrightarrow \exists f\in E, f\neq 0, \varphi(f)=\lambda f$ \par $\Leftrightarrow$ l'équation $\varphi(f)=\lambda f$ admet au moins une solution non-nulle. \par Soit $\lambda\in\C$, soit $f\in E$. Alors : \par $\varphi(f)=\lambda f \Leftrightarrow f'=\lambda f \Leftrightarrow f\in Vect(t\mapsto e^{\lambda t})$ \par Donc $\lambda \in S_p(\varphi)$ \par Donc $S_p(\varphi)=\C$
\par Soit $E=\R[X]$, et $\varphi:\left\{\begin{array}{rcl} E & \to & E \\ P & \mapsto & P' \end{array}\right.$ \par Déterminons les valeurs propres de $\varphi$. Soit $\lambda\in\K$, soit $P\in E$ : \par Si $\lambda=0$ : \par $\varphi(P)=\lambda P \Leftrightarrow P'=\lambda P \Leftrightarrow P\in Vect(1)$ \par Donc $0$ est valeur propre. \par Si $\lambda\neq 0$ : \par $P'=\lambda P \Rightarrow \deg P'=\deg P \Rightarrow P=0$ \par Donc si $\lambda =0$, alors $\lambda\notin S_p(\varphi)$ \par Donc $S_p(\varphi)=\{0\}$
\end{Exe}
\Def{Espace propre}{Soit $E$ un $\K$-ev, $f\in\mathcal{L}(E)$ et $\lambda\in S_p(f)$. On appelle espace propre associé à $\lambda$ l'ensemble $\ker (f-\lambda id)$, qu'on note $E_\lambda(f)$ \par C'est un sev de $E$ non-réduit à $0$. \par On étend la notion aux matrices de $\mathcal{M}_n(\K)$. Pour $A\in\mathcal{M}_n(\K)$ et $\lambda\in S_p(A)$, alors $E_\lambda(A) = \ker (A-\lambda I_n)$ est un sev de $\mathcal{M}_{n,1}(\K)$.}
\begin{Rem}
Attention, deux matrices semblables n'ont pas les mêmes espaces propres. Ils sont simmplement isomorphes : $E_\lambda(A')=P(E_\lambda(A))$ avec $P$ la matrice de passage entre $A$ et $A'$.
\end{Rem}
\Def{Vecteur propre}{Soit $E$ un $\K$-ev, $f\in\mathcal{L}(E)$ et $\lambda\in S_p(f)$. Soit $x\in E$, on dit que $x$ est un vecteur propre de $f$ relativement à $\lambda$ si $x$ est un vecteur non-nul de $E_\lambda(f)$.}
\Thr{}{Une somme finie de sous-espaces propres d'un endomorphisme associés à des valeurs propres distinctes est directe.\par Ce qui correspond à : si $\lambda_1,...,\lambda_p$ sont des valeurs propres distinctes de $f\in\mathcal{L}(E)$, alors $E_1+...+E_p$ est une somme directe.}
\Pre{Par récurrence sur le nombre d'espaces propres : Soit, pour $p\in \N^*$, $H_p$ : Pour tout $\K$-ev $E$ et toute application $f\in\mathcal{L}(E)$, la somme de $p$ espaces propres de $f$ associés à des valeurs propres distinctes est directe.
\par $H_1$ est vérifiée. \par Soit $p\in\N^*$, on suppose $H_p$. Soit $E$ un $\K$-ev, soit $f\in\mathcal{L}(E)$ et $\lambda_1,...,\lambda_{p+1}$ $p+1$ valeurs propres de $f$. \par Soit $(u_1,..., u_{p+1})\in E_{\lambda_1}\times...\times E_{\lambda_{p+1}}$, et on suppose $u_1+...+u_{p+1}=0$ (1). \par On a donc $f(u_1)+...+f(u_{p+1})=0$ ie $\lambda_1 u_1 +...+\lambda_{p+1}u_{p+1}=0$ (2) \par On fait $\lambda_1(1)-(2)$ : \par $(\lambda_1-\lambda_1)u_2 +...+(\lambda_{p+1}-\lambda_1)u_{p+1}=0$
\par Or par $H_p$, on a que $E_{\lambda_2}+...+E_{\lambda_{p+1}}$ est directe, donc $\forall i\in\llbracket2,p+1\rrbracket, (\lambda_i-\lambda_1)u_i=0$ donc $u_i=0$ \par Donc d'après (1), on a aussi $u_1=0$ \par Donc $E_{\lambda_1}(f)+...+E_{\lambda_{p+1}}(f)$ est directe. \par D'où la récurrence.}
\Prop{Corollaire}{Si $\dim E$ est finie : $f\in\mathcal{L}(E), n=\dim E$ :\begin{itemize}
\item $S_p(f)$ est fini et $card S_p(f)\leq n$ ($\lambda \in S_p(f)\Rightarrow \dim E_\lambda(f)\geq 1$) ;
\item $f$ est diagonalisable (ie il existe une base de $E$ dans laquelle $Mat_B(f)$ est diagonale) si, et seulement si, $\sum\limits_{\lambda\in S_p(f)}\dim E_\lambda(f)=\dim E$
\item Si $f$ admet $n$ valeurs propres distinctes, alors $f$ est diagonalisable.
\end{itemize}}
\Pre{Si $\sum\limits_{\lambda\in S_p(f)}\dim E_\lambda(f)=\dim E$ : \par $S_p(f)=\{\lambda_1,...,\lambda_p\}$, on prend $B_1,...,B_p$ bases respectivement de $E_{\lambda_1}(f),...,E_{\lambda_p}(f)$ \par On considère $B=(B_1,...,B_p)$ \par $B$ est une famille libre (la somme des espaces propres est directe) et $\dim E_{\lambda_1}(f)+...\dim E_{\lambda_p}(f) =\dim E$ vecteurs de $E$ \par Donc $B$ est une base. Et alors la matrice de $f$ dans cette base est diagonale de coefficients les éléments du spectre selon leurs dimensions.
\par On suppose qu'il existe $B=(e_1,...,e_n)$ telle qu'il existe $(\lambda_1,...,\lambda_n)\in\K^n$ telle que la matrice de $f$ dans $B$ soit diagonale de coefficients les $\lambda_i$. \par On a donc $\forall i\in\llbracket 1,n\rrbracket, e_i\in E_{\lambda_i}(f)$ \par Donc $E\subset E_{\lambda_1}(f)+...+E_{\lambda_n}(f)\subset\bigoplus\limits_{\lambda\in S_p(f)}E_\lambda(f)$ \par Donc $\dim E\leq \sum\limits_{\lambda\in S_p(f)}\dim E_{\lambda}(f)$, l'égalité étant assurée par le fait que les espaces propres sont des sev de $E$.}
\begin{Exe}
Soit $E=\mathcal{C}^\infty(\R,\C)$ et $(e_\lambda)_{\lambda\in\C}$ telle que : $e_\lambda:\left\{\begin{array}{rcl} \R & \to & \C \\ t & \mapsto & e^{\lambda t} \end{array}\right.$ \par Pour montrer que $(e_\lambda)$ est libre, on considère l'application $\varphi$ qui à tout élément de $E$ associe sa dérivée. \par Alors $S_p(\varphi)=\C$\par Prenons $\lambda_1,...,\lambda_p\in\C$ deux-à-deux distincts : alors $(e_{\lambda_1},...,e_{\lambda_p})$ est une famille de $p$ vecteurs non-nuls des espaces propres de $\varphi$ $E_{\lambda_1}(\varphi),...,E_{\lambda_p}(f)$ \par Donc $(e_{\lambda_1},...,e_{\lambda_p})$ est libre.
\end{Exe}
\Prop{}{Soit $E$ un $\K$-ev. Soient $u,v\in\mathcal{L}(E)$ tels que $u\circ v=v\circ u$. Alors :\begin{itemize}
\item $Im v$ et $\ker v$ sont stables par $u$
\item $\forall\lambda\in S_p(v), E_\lambda(v)$ est stable par $u$
\end{itemize}}
\Pre{Soit $y\in Im v$ \par Donc on peut prendre $x\in E, y=v(x)$
\par donc $u(y)=u(v(x))=v(u(x))$ donc $u(x)\in Im v$ \par Soit $\lambda \in S_p(f)$ \par Soit $x\in E_\lambda(v)$ \par alors $v(u(x))=u(v(x))=u(\lambda x)=\lambda u(x)$ \par Et donc $u(x)\in Ker(v-\lambda id)$ \par Donc $E_\lambda(v)$ est stable.}

\section{Polynôme caractéristique}
Dans la suite du chapitre, $E$ est de dimension finie
Avec $f\in\mathcal{L}(E), \lambda\in\K$ on a :\par $\lambda\in S_p(f)\Leftrightarrow (f-\lambda id)\text{ est non injective}$ \par $\Leftrightarrow (f-\lambda id)\text{ est non inversible dans $\mathcal{L}(E)$ en dim finie}$\par $\Leftrightarrow \det (f-\lambda id)=0$
\Prop{Polynôme caractéristique}{Avec $f\in\mathcal{L}(E)$, on a que $\begin{array}{rcl} \K & \to & \K \\ \lambda & \mapsto & \det (f-\lambda id)\end{array}$ est polynomiale. Le polynôme associé est de degré $n$ unitaire.
\par On appelle ce polynôme le polynôme caractéristique de $f$, noté $\chi_f$}
\Pre{Soit $A$ la matrice de $f$ dans une base quelconque fixée. \par $\det (\lambda id-f)=\det (\lambda I - A)=\sum\limits_{\sigma\in\mathfrak{S}_n}\varepsilon(\sigma)\prod\limits_{k=1}^n(\lambda I-A)_{\sigma(k),k}$\par Cette fonction est polynomiale, associée au polynôme : \par $\det(XI-A)=\sum\limits_{\sigma\in\mathfrak{S}(n)}\varepsilon(\sigma)\prod\limits_{K=1}^n[XI-A]_{\sigma(k),k}$ \par Or $[XI-A]_{\sigma(k),k}$ est un polynôme de degré inférieur ou égal à $1$ (qui vaut $1$ si, et seulement si, $\sigma(k)=k$)
\par Donc $\det(XI-A)$ est une somme de polynômes de degré $\leq n$, donc un polynôme de degré $\leq n$ \par On remarque que $\prod\limits_{k=1}^n[XI-A_{\sigma(k),k}$ est de degré $n$ si, et seulement si, $\sigma =id$ \par donc $\det(XI-A)=\prod\limits_{i=1}^n(X-A_{i,i})+Q$ avec $Q\in\K_{n-1}[X]$ \par Donc $\deg(\det(XI-A))=n$ est le coefficient le degré $n$ de ce polynôme et celui de $\sum\limits_{i=1}^n(X-A_{i,i}$ c'est-à-dire 1}
\Prop{Valeurs propres}{Soit $E$ un $\K$-ev de dimension finie. \par $\lambda$ est une valeur propre de $f$ si, et seulement si, $\lambda$ est une racine de $\chi_f$}
\begin{Rem}
On retrouve le fait qu'un endomorphisme en dimension finie $n$ possède au plus $n$ valeurs propres réelles.
\end{Rem}
\subsection{Coefficients du polynôme caractéristique}
Avec $f\in\mathcal{L}(E)$, $A=Mat_B(f)$ dans une base $B$ fixée. \par $\chi_f=\sum\limits_{\sigma\in\mathfrak{S}_n}\varepsilon(\sigma)\prod\limits_{k=1}^n[XI-A]_{\sigma(k),k}$ \par Pour $j\in\{1,n\}$, calculsons le coefficient devant $X^{n-j}$ de $\chi_f$ : $\alpha_{n-j}$ \par Pour le coefficient de degré $n-1$ : soit $\sigma\in\mathfrak{S}_n$, si $\sigma\neq id$, alors $\sigma$ a au plus $n-2$ points fixes \par Donc $\forall\sigma\in\mathfrak{S}_n,\sigma\neq id, \deg(\prod\limits_{k=1}^n[XI-A]_{\sigma(k),k})\leq n-2$ \par Donc $\chi_f = \prod\limits_{i=1}^n[X-A_{i,i}] + Q$ avec $\deg Q\leq n-2$ \par Donc le coefficient de degré $n-1$ de $\chi_f$ est : $-\sum\limits_{i=1}^nA_{i,i}=-tr(A)$
\par Le coefficient constant de $\chi_f$ est la valeur en $0$ du polynôme caractéristique, donc c'est $\chi_f(0)=\det(-A)=(-1)^n\det(A)$ \par Donc $\chi_f = X^n-tr(f)X^{n-1} +...+(-1)^n\det(A)$
\begin{Exe}
Application : si $\chi_f$ est scindé, alors $\chi_f=\prod\limits_{k=1}^n(X-\lambda_i)$ où les $\lambda_i$ sont les racines de $\chi_f$, non nécessairement distinctes. \par Alors $\left\{\begin{array}{ccl}\sum\limits_{i=1}^n\lambda_i & = & tr(f) \\ \prod\limits_{i=1}^n\lambda_i & = & \det (f) \end{array}\right.$
\end{Exe}
\Def{Ordre d'une valeur propre}{Soit $E$ un $\K$-ev de dimension finie. Soit $f\in\mathcal{L}(E)$. Soit $\lambda \in\K$. \par On dit que $\lambda$ est une valeur propre d'ordre $\alpha$ si $\lambda$ est une racine de $\chi_f$ d'ordre $\alpha$
\par Ce qui correspond à $(X-\lambda)^\alpha|\chi_f$ et $(X-\lambda)^{\alpha+1}$ ne divise pas $\chi_f$ \par Ce qui correspond à $\chi_f = (X-\lambda)^\alpha Q$ pour $Q\in\K[X], Q(\lambda)\neq 0$}
\Thr{}{Soit $E$ un $\K$-ev de dimension finie. Soit $f\in\mathcal{L}(E)$. Soit $\lambda$ une valeur propre de $f$ d'ordre $\alpha$ \par Alors $\dim E_\lambda(f)\leq\alpha$}
\Pre{Dans le cadre de l'énoncé, on note $p=\dim E_\lambda(f)$. \par On peut prendre $(e_1,..., e_p)$ une base de $E_\lambda(f)$ \par On complète cette famille libre de $e$ par $e_{p+1}, ..., e_n$ en une base $B$ de $E$. \par Alors $Mat_B(A) = \begin{pmatrix} \lambda I_p & C\\ 0 & D\end{pmatrix}$ avec $C\in\mathcal{M}_{p,n-p}(\K)$ et $D\in\mathcal{M}_{n-p, n-p}(\K)$ \par Donc $\chi_f  = \det\begin{pmatrix}(X-\lambda)I_p & -C \\ 0 & XI-D \end{pmatrix} = (X-\lambda)^p\det(XI-D)$ \par Donc $(X-\lambda)^p$ divise $\chi_f$ \par Donc $p\leq\alpha$}
\Prop{Corollaire}{Si $E$ de dimension finie, $f\in\mathcal{L}(E)$, alors $f$ est diagonalisable si, et seulement si :
\par $\left\{\begin{array}{l} \chi_f\text{ est scindé} \\ \forall\lambda\in S_p(f), \mathrm{dim}E_\lambda(f)=\alpha_\lambda\end{array}\right.$}
\Pre{On note $\alpha_\lambda$ l'ordre de $\lambda$ pour $\lambda\in S_p(f)$ \par Par contraposée : si $\chi_f$ est non-scindé. \par On a alors $\sum\limits_{\lambda\in S_p(f)}\alpha_\lambda$ \par On sait que $\dim\bigoplus_{\lambda\in S_p(f)}E_\lambda(f)=\sum\limits_{\lambda\in S_p(f)}\dim E_\lambda(f)$ \par Donc $\dim\bigoplus_{\lambda\in S_p(f)}E_\lambda(f)\leq \sum\limits_{\lambda\in S_p(f)}\alpha_\lambda<n$ \par Donc $\bigoplus E_\lambda(f)\neq E$ \par $f$ n'est pas diagonalisable.
\par On suppose maintenant $\chi_f$ scindé. \par Pour $\lambda\in S_p(f)$, on note $n_\lambda$ la dimension de $E_\lambda(f)$ \par $f$ est diagonalisable si, et suelement si $\bigoplus\limits_{\lambda\in S_p(f)}E_\lambda(f)=E$ \par $\Leftrightarrow \sum\limits_{\lambda\in S_p(f)}n_i=n$ \par $\Leftrightarrow \sum\limits_{\lambda\in S_p(f)}(\alpha_\lambda-n\lambda)=0$ \par $\forall\lambda\in S_p(f),\alpha_\lambda=n_\lambda$}
\begin{Exe}
Avec $A=\begin{pmatrix} 4&1&1\\1&4&1\\1&1&4\end{pmatrix}$ \par On a immédiatement $rg(A-3I_3)=1$ et donc $\dim\ker(A-3I_3)=2$ \par Donc $3$ est une valeur propre de $A$ d'ordre au moins $2$. \par Donc les valeurs propres de $A$ comptées avec ordre de multiplicité sont $3,3,X$ telles que $3+3+X=tr(A)$ \par Donc $X=6$ et donc $\chi_A=(X-A)^2(X-6)$
\end{Exe}

\section{Trigonalisation}
\Def{Trigonalisation}{Soit $E$ un $\K$-ev de dimension finie, on dit que $f\in\mathcal{L}(E)$ esat trigonalisable si il existe une base $B$ telle que $Mat_B(f)$ est triangulaire.}
\begin{Rem}
Si $Mat_B(f)\in T_n^+(\K)$ : $f(e_1)\in Vect(e_1), f(e_2)\in Vect(e_1,e_2),..., f(e_n)\in Vect(e_1,...,e_n)$ et donc $\forall i\in\llbracket 1,n\rrbracket, f(F_i)\subset F_i$.
\par Posons $B'(e_n,...,e_1)$. Alors : $f(e_n)\in Vect(e_n,...,e_1), f(e_{n-1})\in Vect(e_{n-1},..., e_1),..., f(e_1)\in Vect(e_1)$
\par Et alors $Mat_{B'}(f)\in T_n^+(\K)$
\par Donc quand une matrice est trigonalisable, elle l'est de manière inférieure et supérieure.
\par Une matrice est dite trigonalisable si son endomorphisme canoniquement associé est trigonalisable. ie elle est semblable à une matrice triangulaire.
\end{Rem}
\Thr{}{Un endomorphisme d'un $\K$-ev $E$ de dimension finie est trigonalisable si, et seulement si, son polynôme caractéristique est scindé.}
\Pre{On suppose $f$ trigonalisable. On a donc $B$ une base de $E$ telle que : $Mat_B(f)= T$ est triangulaire, de coefficients diagonaux $\alpha_1,...,\alpha_n$. \par Et donc $\chi_f=\chi_T=(X-\alpha_1)...(X-\alpha_n)$ \par Donc $\chi_f$ est scindé.
\par Réciproquement, raisonnons par récurrence sur la dimension de l'espace. \par Montrons que $\forall n\in\N^*, H_n :$ pour tout $\K$-ev $E$ de dimension $n$, pour tout endomorphisme $f$ de $E$ de polynôme caractéristique scindé, $f$ est trigonalisable.
\par $H_1 :$ En dimension 1, les endomorphismes sont uniquement $x\mapsto \lambda x$, on a donc une matrice à un seul coefficient $\lambda$.
\par Soit $n\in\N$, on suppose $H_n$. Soit $E$ un $\K$-ev de dimension $(n+1)$. \par Soit $f\in\mathcal{L}(E)$ tel que $\chi_f$ est scindé. \par Donc $\chi_f$ a au moins une racine dans $\K$, notée $\lambda$. Donc $\lambda\in S_p(f)$. \par On considère $e_1\in E$ un vecteur propre (non nul) associé à $\lambda$ et on complète $e_1$ par $(e_2,..., e_{n+1})$ en une base $B$ de $E$. \par Alors $Mat_B(f) =\begin{pmatrix} \lambda & L \\ 0 & A \end{pmatrix}$ avec $A\in\mathcal{M}_n(\K), L\in\mathcal{M}_{1,n}(\K)$
\par On note $F=Vect(e_2,...,e_{n+1})$. On note $p$ la projection sur $F$ parallèlement à $Vect(e_1)$.
\par On appelle $g$ l'application $g:\left\{\begin{array}{rcl} F & \to & F \\ x & \mapsto & p\circ f(x) \end{array}\right.$.
\par Et alors $A=Mat_{(e_2,...,e_{n+1})}(g)$ \par On a que $\dim F =n$ et donc $g\in\mathcal{L}(F)$
\par On a que $\chi_f = (X-\lambda)\chi_A = (X-\lambda)\chi_g$ or $\chi_f$ est scindé, donc $\chi_g$ est scindé.
\par Donc par $H_n$, on peut trouver une base $B'_f$ de $F$ dans laquelle $Mat_{B'_f}(g)=T\in T_n^+(\K)$
\par Donc $B'=(e_1,e'_2,..., e'_n)$ est une base de $E$
\par Donc $Mat_{B'}(f)=\begin{pmatrix} \lambda & L' \\ 0 & T\end{pmatrix}$ qui est triangulaire supérieure.
\par Ce qui conclut la récurrence.}
\begin{Exe}
Soit $E$ un $\C$-ev de dimension $n$ et $f\in\mathcal{L}(E)$ \par Notons $\chi_f = \prod\limits_{i=1}^n (X-\lambda_i)$ \par Déterminer $\chi_{f^p}$ pour $p\in\mathcal{N}$ et $\chi_{f^p}$ pour $p\in\Z$ si $f\in\mathcal{GL}_n(\C)$ \par Comme $\chi_f\in\C[X]$, il est scindé : on fixe $B$ une base de $E$ dans laquelle $Mat_B(f)$ est triangulaire supérieure de coefficients diagonaux $\alpha_1,...,\alpha_2$ \par Comme un produit de matrices triangulaires supérieures est triangulaire supérieur avec les coefficients diagonaux qui sont les produits des éléments des deux diagonales, et donc $Mat_B(f^2)$ est une matrice triangulaire supérieure avec comme coefficients diagonaux les $\lambda_1^2,...,\lambda_n^2$
\par Donc $\chi_{f^2} = \prod\limits_{i=1}^n(X-\lambda_i^2)$ \par Par récurrence immédiate, on a $\chi_{f^p} = \prod\limits_{i=1}^n (X-\lambda_i^p)$ \par Si $f\in\mathcal{GL}_n(\K)$, alors tous ses coefficients diagonaux dans $B$ sont non-nuls et son inverse a comme coefficients diagonaux les $\frac{1}{\lambda_1},...,\frac{1}{\lambda_n}$. On peut alors étendre $\chi_{f^p}(X-\lambda_i^p)$ pour $n\in\Z$
\end{Exe}
\begin{Exe}
Est-ce qu'il y a un lien entre $f$ diagonalisable et $f^2$ diagonalisable ? \par Si $f$ est diagonalisable, alors $f^2$ l'est pour les mêmes matrices de passage. On peut trouver $B=(e_1,...e_n)$ une base de vecteurs propres de $f$ tels que $f(e_i)=\lambda_i e_i$ \par On a alors $f^2(e_i)=f(\lambda_i e_i) = \lambda_i^2e_i$ et par récurrence immédiate $f^p(e_i)=\lambda_i^pe_i$ \par On a cependant des exemples de réciproque fausse, avec la valeur propre 0. Par exemple, $\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}= \begin{pmatrix} 0 & 0 \\ 0 & 0\end{pmatrix}$ n'est pas diagonalisable mais son carré est la matrice nulle, qui est diagonale.
\end{Exe}
\begin{Rem}
Avec la trigonalisation, on retrouve le fait que : \par $\forall A\in\mathcal{GL}_n(\K), tr(A)$ est la somme des vp de $A$ comptées avec leur ordre de multiplicité. \par $\det (A)$ est le produit des vp de $A$ comptées avec leur ordre de multiplicité.
\end{Rem}

\section{Endomorphismes nilpotents}
\Def{Nilpotence}{Soit $E$ un $\K$-ev de dimension finie et $f$ un endomorphisme de $E$. $f$ est nilpotent si il existe $k\in\N^*$ tel que $f^k=0$. On appelle ordre de nilpotence de $f$ le plus petit entier $p$ tel que $f^p=0$}
\Thr{}{Soit $E$ un $\K$-ev de dimension finie $n$, $f\in\mathcal{L}(E)$. Alors $f$ est nilpotente si, et seulement si, $\chi_f = X^n$ et $\chi_f=X^n$ si, et seulement si, il existe une base $B$ de $E$ dans laquelle $Mat_B(f)$ est triangulaire avec des 0 sur la diagonale.}
\Pre{Supposons $\chi_f=X^n$. Alors $\chi_f$ est scindé, et $f$ est trigonalisable. \par Mais comme ses valeurs propres sont $0,...,0$, il existe donc une base $B$ de $E$ telle que $Mat_b(f)$ est triangulaire avec des zéros sur la diagonale.
\par On suppose qu'il existe $B$ base de $E$ telle que $Mat_B(f)$ est triangulaire avec une diagonale nulle. \par On note pour $i\in\llbracket 1,n\rrbracket : F_i = Vect(e_1,...,e_i)$. \par Pour $i\in\llbracket 1,n\rrbracket, f(F_i)\subset F_{i-1}$ avec $F_0=\{0\}$ \par Et donc par récurrence : $\forall p\in\N^*, p\leq i, f^p(F_i)\subset F_{i-p}$ \par Et en particulier, $f^n(F_n)\subset F_0$ \par Et donc $f$ est nilpotente.
\par Supposons $f$ nilpotente. On considère une base $B$ de $E$ et $A = Mat_f(B)$ \par Donc $A\in\mathcal{M}_n(\K)$. \par Donc on sait que $\chi_A = \chi_f$ \par Montrons que la seule valeur propre de $A$ dans $\C$ est $0$, donc on a besoin d'une complexification de l'espace. \par Donc on prend $\lambda \in S_{p,\C}(A)$ \par On peut donc prendre $X\in\mathcal{M}_{n,1}(\C), X\neq 0$ tel que $AX=\lambda X$ \par Par récurrence, $\forall k\in\N, A^kX = \lambda^kX$ \par $f$ est nilpotente d'ordre $\leq n$, donc $A$ aussi. \par Donc $\lambda^nX = 0$ \par Mais $X\neq 0$ donc $\lambda^n=0$ donc $\lambda=0$ \par Et donc $0$ est valeur propre d'ordre $n$ \par Donc $\chi_f=\chi_A=X^n$}

\section{Polynômes d'endomorphismes}
Un projecteur, défini polynomialement, est un endomorphisme vérifiant $p^2-p=0$. Une symétrie est un endomorphisme qui vérifie $s^2-id=0$. \par On parle dans les deux cas de polynômes annulateurs. Dans les deux cas, les endomorphismes sont diagonalisables.
\par Dans toute cette partie, $E$ est un $\K$-ev de dimension finie.
\Def{Polynôme d'endomorphismes}{Soit $f\in\mathcal{L}(E)$. On note $\K[f]$ la sous-algèbre de $\mathcal{L}(E)$ engendrée par $f$.\par Donc $\K[f] = Vect((f^n)_{n\in\N})=\{\sum\limits_{k=0}^na_kf^k|n\in\N, (a_0,...,a_n)\in\K^{n+1}\}$
\par On peut donc considérer l'application $\left\{\begin{array}{rcl} \K[X] & \to & \K[f] \\ P = \sum\limits_{k=0}^na_kX^k & \mapsto & \sum\limits_{k=0}^na_kf^k = P(f)\end{array}\right.$}
\Prop{Opérations sur les polynômes d'endomorphismes}{Pour $P,Q\in\K[X]$, $\lambda\in\K$ et $f\in\mathcal{L}(E)$, on a :\begin{itemize}
\item $(P+Q)(f) = P(f) + Q(f)$
\item $(\lambda\cdot P)(f) = \lambda P(f)$
\item $(P\times Q)(f) = P(f)\circ Q(f)=Q(f)\circ P(f)$
\item $(P\circ Q)(f) = P(Q(f))$
\end{itemize}}
\Pre{Les deux premières sont immédiates. \par Pour le produit, si $P=\sum\limits_{k=0}a_kX^k, Q=\sum\limits_{k=0}^nb_kX^k$ alors : \par $PQ=\sum\limits_{0\leq i\leq n, 0\leq j\leq n} a_ib_jx^{i+j}$ \par Et donc $(PQ)(f) = \sum\limits_{0\leq i\leq n,0\leq j\leq n} a_ib_jf^{i+j} = \left(\sum\limits_{i=0}^na_if^i\right)\circ\left(\sum\limits_{j=0}^nb_jf^j\right)$
\par Pour la composée, on montre par récurrence que $(X^n\circ Q)(f) = Q^n(f) = Q(f)^n$ \par Et ensuite on a que pour $P=\sum\limits_{k=0}^na_kX^k$ : $P\circ Q(f) = \sum\limits_{k=0}^na_kQ^k(f) = \sum\limits_{k=0}^na_kQ(f)^k = P(Q(f))$}
\begin{Rem}
Si $E$ est de dimension finie, alors $\K[f]$ est un sev de $\mathcal{L}(E)$, donc $\K[f]$ est de dimension finie. Donc l'application $\varphi:\left\{\begin{array}{rcl} \K[X] & \to & \K[f] \\ P & \mapsto & P(f)\end{array}\right.$ n'est pas injective. \par $\ker\varphi$ n'est pas réduit à $\{0\}$
\end{Rem}
\Def{Polynôme annulateur}{Soit $P\in\K[X]$ et $f\in\mathcal{L}(E)$. On dit que $P$ est un polynôme annulateur de $f$ si $P(f)=0$}
\Def{Polynôme minimal}{Soit $f\in\mathcal{L}(E)$. On appelle polynôme minimal de $f$ un polynôme non-nul unitaire annulateur de $f$ de degré minimal, qu'on note $\pi_f$ ou $\mu_f$ (cette notation existe mais elle est très rare).}
\Prop{}{Avec $E$ un $\K$-ev de dimension finie et $f\in\mathcal{L}(E)$ : \begin{enumerate}
\item $\K[f]$ est de dimension $\deg (\pi_f)$ et $(id,f,..., f^{\deg(\pi_f)-1})$ est une base de $\K[f]$
\item $P$ est annulateur de $f$ si, et seulement si, $\pi_f\vert P$
\item Si $P$ est annulateur de $f$, alors toute valeur propre de $f$ est une racine de $P$.
\item Les valeurs propres de $f$ sont les racines de $\pi_f$
\end{enumerate}}
\Pre{\underline{Montrons le 1 :} On note $p=\deg\pi_f$. Soit $(a_0,..., a_{p-1})\in\K^p$. \par On suppose $a_0id + a_1f + ... + a_{p-1}f^{p-1}=0$ \par Par l'absurde, si $(a_0,...,a_{p-1})\neq (0,...,0)$, alors le polynôme $Q=\sum\limits_{k=0}^{p-1}a_kX^k$ est non-nul, annulateur de $f$ et de degré strictement inférieur à $\deg\pi_f$. D'où la contradiction. \par Donc la famille $(id,f,...,f^{p-1})$ est libre.
\par Soit $g\in\K[X]$. on a donc $P\in\K[X], g=P(f)$. \par Par division euclidienne, $P=\pi_fQ + R$ avec $\deg R< \deg\pi_f$. \par ALors $P(f)=(\pi_fQ+R)(f) = \pi_f(f)\circ Q(f) + R(f) = R(f)$ \par Donc $g\in Vect(id,f,..., f^{p-1})$ \par D'où le résultat.
\par \underline{Montrons le 2 :} Tout multiple de $\pi_f$ est annulatgeur (immédiat) \par Réciproquement, soit $P$ un polynôme annulateur de $f$. Par division euclidienne, soit $P = \pi_fQ+R$ avec $\deg R<\deg\pi_f$ \par Alors $P(f) = 0 = R(f)$ \par Donc $R$ est annulateur de degré strictement inférieur à $\deg\pi_f$, donc $R=0$. \par De là, on a peut assurer l'unicité du polynôme minimal : s'il y en avait deux, ils se diviseraient l'un l'autre et seraient associés. Mais puisqu'ils sont unitaires, ils sont égaux.
\par \underline{Montrons le 3 :} Soit $\lambda\in S_p(f)$ \par On peut donc prendre $x\in E$ non-nul tel que $f(x)=\lambda(x)$ \par Par récurrence, $\forall i\in\N, f^i(x) = \lambda^ix$ \par Et donc, $\forall P\in\K[X], P(f)(x) = \P(\lambda)x$ \par Et si $P$ est annulateur de $f$, $P(\lambda)x=0$, et comme $x\neq 0$, $P(\lambda)=0$ \par Donc $\lambda$ est une racine de $P$.
\par \underline{Montrons le 4 :} $\pi_f$ est annulateur de $f$, donc toute valeur propre de $f$ est racine de $\pi_f$. \par Réciproquement, soit $\lambda$ une racine de $\pi_f$ \par On a donc $Q\in\K[X]$ tel que $\pi_f=(X-\lambda)Q$ \par $\pi_f(f)=0$, ce qui s'écrit $(f-\lambda id)\circ Q(f) = 0$ \par Par l'absurde, si $\lambda\notin S_p(f)$, alors $(f-\lambda id)$ est bijective. \par Donc $(f-\lambda id)^{-1}\circ(f-\lambda)\circ Q(f) = Q(f) = 0$ \par Donc $Q$ est annulateur de $f$, mais $Q\neq 0$ alors que $\deg Q<\deg\pi_f$, d'où la contradiction. \par Donc $\lambda\in S_p(f)$}
\begin{Rem}
On étend toutes ces notions aux racines de $\mathcal{M}_n(\K)$ par passage à l'endomorphisme canoniquement assotions. On note avec les notations $\K[A]$ et $\pi_f$ pour $A\in\mathcal{M}_n(\K)$ et au passage si $A$ et $B$ sont semblables, alors $\pi_A=\pi_B$.
\end{Rem}
\begin{Exe}
À refaire pendant la toussaint.
\end{Exe}
\Thr{}{Avec $E$ un $\K$-ev de dimension finie, $f\in\mathcal{L}(E)$. $f$ est diagonalisable si, et seulement si, $\pi_f$ est scindé à racines simples \par Ce qui correspond à ce qu'il existe un polynôme annulateur non-nul scindé à racines simples de $f$.}
\Pre{Supposons $f$ diagonalisable. \par $S_p(f) = \{\lambda_1,...,\lambda_p\}$ et donc $\chi_f = (X-\lambda_1)^{\alpha_1}...(X-\lambda_p)^{\alpha_p}$ \par Donc il existe une base dans laquelle la matrice de l'endomorphisme est une diagonale avec $\alpha_1$ fois $\lambda_1$, $\alpha_2$ fois $\lambda_2$... \par Et alors $\pi_f = \pi_A = (X-\lambda_1)...(X-\lambda_p)$
\par Réciproquement, on suppose qu'il existe $\lambda_1,...,\lambda_p$ distincts tels que  $pi_f=(X-\lambda_1)...(X-\lambda_p)$. On considère $L_i$ les polynômes de Lagrange associés à $\lambda_1,...,\lambda_p$ (on rappelle que $Li = \prod\limits_{j=1,j\neq i}^p\dfrac{X-\lambda_j}{\lambda_i-\lambda_j}$ et que $\forall i,j\in\llbracket 1,p\rrbracket, L_i(\lambda_j) = \delta_{i,j}$) \par Donc $(L_1,...,L_p)$ est une base de $\K_{p-1}[X]$ où on peut écrire $P = \sum\limits_{k=1}^p P(\lambda_k)L_k$. En particulier, le polynôme constant de valeur 1 est $1|=\sum\limits_{k=1}^pL_k$.
\par Alors $1|(f) =\sum\limits_{i=1}^pL_i(f)$ ce qui correspond à $id = \sum\limits_{i=1}^pL_i(f)$ \par Donc $\forall x\in E, x=\sum\limits_{i=1}^pL_i(f)(x)$ avec $L_i(f)(x)\in\ker (f-\lambda_i id)$ \par Donc $E$ est la somme des espaces propres \par Donc $f$ est diagonalisable. \par En effet, $(f-\lambda_i id)(L_i(f)(x)) = ((f-\lambda_i id)\circ Li(f))(x) = ((X-\lambda_i)L_i)(f)(x) = (\mu\pi_f)(f)(x)$ avec $\mu = \prod\limits_{j=1, j\neq i}^p\frac{1}{\lambda_i-\lambda_j}$ \par Donc $E\subset \bigoplus\limits_{i=1}^p E_{\lambda_i}(f)$}
\Thr{Cayley-Hamilton}{Avec $E$ un $\K$-ev de dimension finie, $f\in\mathcal{L}(E)$, alors $\chi_f$ est un polynôme annulateur de $f$.}
\Pre{On se place d'abord dans le cas où $\K=\C$. \par On sait que les valeurs propres sont les racines de $\pi_f$ et de $\chi_f$. \par $\pi_f = \prod\limits_{\lambda\in S_p(f)}(X-\lambda)^{\alpha_\lambda}$ et $\chi_f=\prod\limits_{\lambda\in S_p(f)}(X-\lambda)^{\beta_\lambda}$ \par On va montrer que $\forall\lambda\in S_p(f), \alpha_\lambda\leq \beta_\lambda$ \par Soit $\lambda\in S_p(f)$. On considère $F=Ker(f-\lambda id)^{\alpha_\lambda}$ \par $F$ est stable par $F$, donc on peut considérer $f_\lambda$ l'endomorphisme induit par $f$ sur $F$
\par Donc $(X-\lambda)^{\alpha_\lambda}$ est annulateur de $f_\lambda$ \par Donc $\pi_{f_\lambda} = (X-\lambda)^\gamma$ où $\gamma\leq\alpha_\lambda$ \par Supposons par l'absurde $\gamma<\alpha_\lambda$ :\par Rappel : $\pi_f = (X-\lambda)^{\alpha_\lambda}Q$ avec $Q(\lambda)\neq 0$ \par $\forall x\in E, ((X-\lambda)^\gamma Q)(f)(x) = (f-\lambda id)^\gamma(Q(f)(x))$ \par Or $(f-\lambda id)^{\alpha_\lambda}Q(f)(x)=0$ \par Donc $Q(f)(x)\in F$ \par Donc $(f-\lambda id)^\gamma(Q(f)(x)) = (f_\lambda-\lambda id)^\gamma(Q(f)(x))=0$ \par Donc $(X-\lambda)^\gamma Q$ est un polynôme annulateur de $f$ de degré strictement inférieur à $\deg\pi_f$
\par D'où la contradiction. Donc $\gamma = \alpha_\lambda$ \par Donc $(f_\lambda - \lambda id)$ est donc nilpotent d'ordre $\alpha_\lambda$, donc $\dim F\geq \alpha_\lambda$ \par On prend une base de $F$ $(e_1,...,e_p)$ qu'on complète en une base de $E$. La matrice de $f$ dans cette base est une matrice par blocs triangulaire supérieure.$\begin{pmatrix}A & C \\ 0 & B\end{pmatrix}$ \par Donc $\chi_f = \chi_A\chi_B=(X-\lambda)^p\chi_B$ \par Donc $\alpha_\lambda\leq p\leq\beta_\lambda$ \par Donc le polynôme minimal divise le polynôme caractéristique.
\par Avec $\K=\R$, avec $B$ une base de $E$. On considère $M=Mat_B(f)\in\mathcal{M}_n(\R)$ \par On a que $\pi_f=\pi_M$ et $\chi_f=\chi_M$ \par $M\in\mathcal{M}_n(\R)$ et $\mathcal{M}_n(\R)\subset \mathcal{M}_n(\C)$ \par Donc $\pi_{M,\C}\vert \chi_{M,\C}$ \par $\chi_M = \det(XI-M)$ ne dépend pas du corps considéré. Donc $\chi_{M,\C}=\chi_{M,\R}$ \par $\pi_{M,\C}\vert\pi_{M,\R}$ \par Rappelons que pour $P\in \C[X]$ : $P(M)=0\Rightarrow \bar{P}(\bar{M})$ \par Mais si $M\in\mathcal{M}_n(\R)$, alors $\bar{P}(M)=0$ aussi. \par $\bar{\pi}_{M,C}$ est annulateur de $M$ et de même degré que $\pi_{M,\C}$, donc $\pi_{M,\C}=\bar{\pi}_{M,\C}$ \par Donc $\pi_{M,\C}\in\R[X]$ \par Donc $\pi_{M,\R}\vert\pi_{M,\C}$
}



\chapter{Suites et sérties de fonctions}
\section{Suites de fonctions}
\Def{Convergence simple}{Soit $E,F$, 2 $\K$-ev de dimension finie. Soit $A\subset E$. Soit $(f_n)\in\mathcal{F}(A,F)^\N$.
\par On dit que $(f_n)$ converge simplement vers $g\in\mathcal{F}(A,F)$ si $\forall t\in A, (f_n(t))\to g(t)$}
\Def{Convergence uniforme}{Soit $E,F$, 2 $\K$-ev de dimension finie. Soit $A\subset E$. Soit $(f_n)\in\mathcal{F}(A,F)^\N$.
\par On dit que $(f_n)$ converge uniformément vers $g\in\mathcal{F}(A,F)$ si $\sup\limits_{A}\Vert f_n-g\Vert \to_{n\to\infty} 0$}
\begin{Rem}
$(f_n)$ converge simplement (CVS) vers $g$ si, et seulement si, $\forall t\in A, \forall\varepsilon\in\R_+^*, \exists n_0\in\N, \forall n\in\N, n\geq n_0\Rightarrow \Vert f_n(t)-g(t)\Vert\leq\varepsilon$
\par $(f_n)$ converge uniformément (CVU) vers $g$ si, et seulement si, $\forall\varepsilon\in\R_+^*, \exists n_0\in\N, \forall t\in A,\forall n\in\N, n\geq n_0\Rightarrow \Vert f_n(t)-g(t)\Vert\leq\varepsilon$
\end{Rem}
\begin{Exe}
$f_n:\left\{\begin{array}{rcl} \R & \to & \R \\ x & \mapsto & \begin{array}{rl} 0 & \text{si $x\leq n$} \\ 1 & \text{si $x>n$}\end{array}\end{array}\right.$ converge simplement vers 0, mais ne converge pas uniformément vers 0 : $\sup\limits_A \Vert f_n(t)-g(t)\Vert = 1$
\end{Exe}
\begin{Exe}
$f_n:\left\{\begin{array}{rcl} ]0,1[ & \to & \R \\ x & \mapsto & \begin{array}{rl} 0 & \text{si $x\leq 1-\frac{2}{n}$} \\ n & \text{si $1-\frac{2}{n}<x<\frac{1}{n}-\frac{2}{n}$} \\ 0 & \text{sinon}\end{array}\end{array}\right.$converge simplement vers 0, mais ne converge pas uniformément vers 0 : $\sup\limits_A \Vert f_n(t)-g(t)\Vert\to_{n\to+\infty}+\infty$
\end{Exe}
\begin{Rem}\begin{itemize}
\item Si $f_n$ converge uniformément vers $g$ alors $f_n$ converge simplement vers $g$.
\item La notion de convergence simple n'est pas une notion de convergence dans un EVN (suites non-bornées convergentes)
\item Sur $\mathcal{B}(A,F)$ (fonctions bornées) la notion de convergence uniforme est celle de convergence pour la norme infinie ($\sup\limits_A\Vert f\Vert$)
\end{itemize}\end{Rem}
\begin{Exe}
Prenons un exemple de fonction qui converge uniformément sans notion de norme infinie : 
\par $f_n:\left\{\begin{array}{rcl} \R & \to & \R \\ x & \mapsto & x+\frac{1}{n}\end{array}\right.$ qui converge uniformément vers $x\mapsto x$ car, $\forall x\in\R, \vert f_n(x)-x\vert\leq \frac{1}{n}$ et donc $\sup\vert f_n-id\vert\leq\frac{1}{n}$
\end{Exe}
\begin{Exe}
Etudier la convergence simple et uniforme de $f_n:\left\{\begin{array}{rcl} \R_+^* & \to & \R \\ x & \mapsto & \dfrac{n^\alpha}{1+nx} \end{array}\right.$ \par À $x\in\R_+^*$ fixé : $f_n\sim \frac{n^\alpha}{nx}\sim\frac{n^{\alpha-1}}{x}$ qui converge simplement si $\alpha\leq 1$.\begin{itemize} \item si $\alpha<1$, $f_n(x)$ tend vers 0. \item si $\alpha=1$, $f_n(x)$ tend vers $\frac{1}{x}$ \item si $\alpha>1$, $f_n(x)$ diverge.\end{itemize} \par Donc :\begin{itemize} \item si $\alpha<1$, $(f_n)$ converge simplement vers 0. \item si $\alpha=1$, $(f_n)$ converge simplement vers $\frac{1}{x}$ \item si $\alpha>1$, $(f_n)$ ne converge pas simplement.\end{itemize} 
\par Si $\alpha=1$, pour la convergence uniforme, il y a deux candidats, $x\mapsto\frac{1}{x}$ et $x\mapsto 0$. \par Pour $x\in\R_+^*$ : $f_n(x)-\frac{1}{x}=\frac{n}{1+nx}-\frac{1}{x}=\frac{-1}{x(1+nx)}=g_n(x)$ \par Alors $\forall n\in\N^*,\vert g(\frac{1}{n})\vert=\frac{n}{2}$ et donc $(f_n)$ ne converge pas uniformément vers $x\mapsto\frac{1}{x}$
\par Si $\alpha<1$, étudions $f_n$ pour $n\in\N^*$. $f_n$ est dérivable et $\forall x\in\R_+^*, f_n'(x) = -n^\alpha n\frac{1}{(1+nx)^2} = \frac{-n^{\alpha+1}}{(1+nx)^2}$ et donc $f_n$ est décroissante positive donc $\sup\vert f_n\vert=n^\alpha$ \par Et donc $(f_n)$ converge uniformément vers la fonction nulle si, et seulement si, $\alpha<0$
\end{Exe}

\section{Séries de fonctions}
Dans la suite du chapitre, on aura $E,F$ deux $\K$-ev de dimension finie, et $A\subset E$
\Def{Convergences}{Soit $(u_n)\in\mathcal{F}(A,F)^\N$, on dit que la série de fonction $(\sum u_n)$ converge simplement si la suite des sommes partielles $\left(\sum\limits_{k=0}^n u_k\right)$ converge simplement.
\par On dit que $(\sum u_n)$ converge uniformément si la suite des sommes partielles $\left(\sum\limits_{k=0}^nu_k\right)$ converge uniformément.}
\begin{Rem}
Si $(\sum u_n)$ converge simplement sur $A$, on peut définir la fonction "somme" de $\sum u_n$ sur A : c'est l'application $\begin{array}{rcl} A & \to & F \\ t & \mapsto & \sum\limits_{n=0}^{+\infty}u_n(t)\end{array}$
\end{Rem}
\begin{Exe}
$(\sum( z\mapsto z^n))$ (on notera le plus souvent $(u_n)\in\mathcal{C,C}^\N$ définie par $u_n(z)=z^n$ pour $z\in\C$). Pour quelle valeur de $z$ $(\sum u_n)$ est-elle définie ? $(\sum u_n)$ converge simplement sur $\mathbb{D}(0,1)$ (disque ouvert de centre 0 de rayon 1) et $\forall z\in \mathbb{D}(0,1); \sum\limits_{n=0}^{+\infty}u_n=\frac{1}{1-z}$ \par $(\sum u_n)$ converge simplement vers $z\mapsto \frac{1}{1-z}$
\par Pour la convergence uniforme : soit $z\in\mathbb{D}(0,1)$, $n\in\N$ \par $\vert\sum\limits_{k=0}^nz^k - \frac{1}{1-z}\vert = \vert\sum\limits_{k=n+1}^{+\infty} z^k\vert =\frac{\vert z^{n+1}}{1-z}$ \par On trouver une suite de $z\in\mathbb{D}(0,1)$ telle que le reste diverge : $\vert R_n(1-\frac{1}{n})\vert = (n+1)(1+\frac{1}{n+1})^{n+1}\sim ne^{-1}$ \par Et donc il n'y a pas convergence uniforme sur $\mathbb{D}(0,1)$.
\par Par contre, avec $R\in[0,1[$ : $\forall n\in\N,\forall z\in\mathbb{D}(0,R), \vert R_n(z)\vert\leq \frac{R^{n+1}}{1-R}$ (on minore le dénominateur et on majore le numérateur) \par Donc $\sup\limits_{z\in\mathbb{D}(0,R)}\vert R_n(z)\vert\leq \frac{R^{n+1}}{1-R}$ \par Donc $\sum z\mapsto z^n$ converge uniformémement sur $\mathbb{D}(0,R)$ pour tout $R\in[0,1[$
\end{Exe}
\begin{Rem}
$\cup_{R\in[0,1[}\mathbb{D}(0,R)=\mathbb{D}(0,1)$. Il y a convergence uniforme sur $\mathbb{D}(0,R)$ pour tout $R$ mais pas convergence uniforme sur $\mathbb{D}(0,1)$. Cependant, si on avait une réunion finie, il y aurait convergence uniforme (en prenant le maximum des deux majorants). \par La convergence uniforme n'est pas une notion locale.
\end{Rem}
\Prop{}{Soit $(u_n)\in\mathcal{F}(A,F)^\N$, alors $(\sum u_n)$ converge uniformémement si, et seulement si, $\sum u_n$ converge simplement et $(R_n)=\sum\limits_{k=n+1}^{+\infty}u_k$ converge uniformément vers $0$.}
\begin{Exe}
$f:\begin{array}{rcl}]-1,1] & \to & \R \\ x & \mapsto & \sum\limits_{n=1}^{+\infty}\frac{(-1)^nx^n}{n}\end{array}$ \par Montrer que $f$ est bien définie. La série de fonctions $\sum(-1)^n\frac{x^n}{n}$ converge-t-elle uniformémemnt vers $f$ ? Trouver des domaines sur lesquels $(\sum (-1)^n\frac{x^n}{n})$ converge uniformément.
\par Soit $n\in\N^*$, alors $\vert\frac{(-1)^nx^n}{n}\vert\leq\vert\frac{x^n}{n}\vert\leq\vert x\vert^n$ \par Or $\sum\vert x\vert^n$ converge pour $\vert x\vert <1$ \par Donc $\sum\frac{(-1)^nx^n}{n}$ converge absolument. \par D'autre part, $\left(\frac{1}{n}\right)$ est positive, décroissante et de limite nulle, donc $\sum\frac{(-1)^n}{n}$ converge pour $x\in]-1,1]$ \par Et donc on a bien la convergence simple.
\par Soit $x\in]-1,1]$, alors $\vert R_n(x)\vert = \vert\sum\limits_{k=n+1}^{+\infty}(-1)^k\frac{x^k}{k}\vert$ \par $\vert R_n(-1+\frac{1}{n+1})\vert=\vert\sum\limits_{k=n+1}^{+\infty}(-1)^n\frac{(-1+\frac{1}{n+1})^k}{k}\vert = \vert (-1+\frac{1}{n+1}^{n+1}\vert \vert\sum\limits_{k=0}^{+\infty}\frac{(-1+\frac{1}{n+1})^k}{n+k+1}\vert$ \par $\forall k\in\N^*, (-1+\frac{1}{n+1})^k$ est du signe de $(-1)^k$ \par Donc $\vert B_n\vert=\vert \sum\limits_{k=0}^{+\infty}\frac{(-1)^{n+1+k}(-1+\frac{1}{n+1})^k}{n+k+1}\vert= \sum\limits_{k=0}^{+\infty}\frac{(1-\frac{1}{n+1})^k}{n+k+1}$ \par $\geq \sum\limits_{k=0}^{+\infty}\frac{(1-\frac{1}{n+1})^k}{n}\geq \sum\limits_{k=0}^N\frac{(1-\frac{1}{n+1})^k}{n+k+1}$ pour $N\in\N$ quelconque
\par $\geq \sum\limits_{k=0}^N\dfrac{(\frac{n}{n+1})^k}{n+N+1}\geq \dfrac{1-(\frac{n}{n+1})^{N+1}}{(1-\frac{n}{n+1})(n+N+1)}\geq (n+1)\frac{(1-(\frac{n}{n+1})^{N+1})}{n+N+1}$ \par ceci est vrai en particulier pour $N=n$ et alors $\vert B_n\vert\geq \frac{(n+1)(1-(\frac{n}{n+1})^{n+1})}{2n+1}\sim \frac{1}{2}e^{-1}$
\par Donc $\vert R_n(1-\frac{1}{n+1})\vert\geq (1-\frac{1}{n+1})^{n+1}\vert B_n\vert$ qui ne tend pas vers $0$ : la série de fonctions ne converge pas uniformément sur $]-1,1]$.
\par Soit $a\in]0,1[$, $\forall x\in [-a,a]$, $\forall n\in\N, \vert\sum\limits_{k=n+1}^{+\infty}(-1)^l\frac{x^k}{k}\vert\leq \sum\limits_{k=n+1}^{+\infty}\frac{\vert x\vert^k}{k}\leq \sum\limits_{k=n+1}^{+\infty}\frac{a^k}{k}$ qui tend vers $0$ et est indépendant de $x$, d'où la convergence uniforme. Donc $\sup\limits_{x\in [-a,a]}\vert R_n(x)\vert\to_{n\to+\infty} 0$ donc la série converge uniformémement sur $[-a,a]$ pour $a\in]0,1[$
\par Soit $x\in[0,1]$, soit $n\in\N$ \par $(\frac{x^k}{k}$ est positive, décroissante et de limite nulle. \par Donc $\vert\sum\limits_{k=n+1}^{+\infty}(-1)^k\frac{x^k}{k}\vert\leq \frac{x^{n+1}}{n+1}\frac{1}{n+1}$ \par Donc $\sup\limits_{x\in[0,1]}R_n(x)\leq \frac{1}{n+1}$ et tend donc vers $0$ \par Donc la série converge uniformément sur $[0,1]$ \par Donc $\left(\sum\frac{(-1)^nx^n}{n}\right)$ converge uniformément sur $[-a,1]$ pour tout $a\in]0,1[$
\end{Exe}
\Def{Convergence normale}{Soit $(u_n)\in\mathcal{F}(A,F)^\N$. On dit que $\sum u_n$ converge normalement si $\sum \sup\limits_A \Vert u_n\Vert$ converge.}
\begin{Exe}
$\sum(-1)^n\frac{x^n}{n}$ converge normalement sur $\left[-\frac{3}{4}, \frac{3}{4}\right]$ : soit $x\in\left[-\frac{3}{4},\frac{3}{4}\right],\forall n\in\N, \vert (-1)^n\frac{x^n}{n}\vert\leq \frac{1}{n}\left(\frac{3}{4}\right)^n$ et $\sum \frac{\left(\frac{3}{4}\right)^n}{n}$ converge \par Donc $\sup\limits_{x\in\left[-\frac{3}{4},\frac{3}{4}\right]}\vert(-1)^n\frac{x^n}{n}\vert\leq \frac{1}{n}\left(\frac{3}{4}\right)^n$ donc $\sum\sup\left|\frac{(-1)^nx^n}{n}\right|$ converge par critère de majoration positif.
\end{Exe}
\Prop{}{Tout série de fonction qui converge normalement converge uniformément}
\Pre{Si $\sum u_n$ converge normalement : \par $\forall x\in A,\sum u_(x)$ converge absolument \par Par critère de majoration positif et $\forall x\in A, \Vert R_n(x)\Vert=\Vert \sum\limits_{k=n+1}^{+\infty} u_k(x)\Vert\leq \sum\limits_{k=n+1}^{+\infty}\Vert u_k(x)\Vert\leq \sum\limits_{k=n+1}^{+\infty}\sup\limits_A\Vert u_k\Vert$.
\par Donc $\sup\limits_A \Vert R_n(x)\Vert\leq \sum\limits_{k=n+1}^{+\infty}\sup\limits_A\Vert u_n\Vert$ \par Donc $\sup\limits_A \Vert R_n(x)\Vert\to 0$}
\begin{Rem}
Résumé :\begin{itemize}
\item Pour les suites de fonctions, on a deux notions, et la convergence uniforme entraîne la convergence simple.
\item Pour les séries de fonctions, on a trois notions : la convergence normale, la convergence uniforme, la convergence simple et on peut rajouter la convergence absolue en tout point. On a que la convergence normale entraîne la convergence uniforme. La convergence uniforme entraîne la convergence simple. La convergence absolue en tout point entraîne la convergence simple. Par définition, la convergence normale entraîne la la convergence absolue en tous points. Par transitivité, la convergence normale entraîne la convergence simple. 
\end{itemize}\end{Rem}

\section{Continuité d'une limite uniforme}
\Thr{Continuité uniforme}{Soit $(f_n)\in\mathcal{F}(A,F)^\N$ une suite de fonctions.
\par Si $\forall n\in\N$, $f_n$ est continue sur $A$ et que $(f_n)$ converge uniformémement vers $g$, alors $g$ est continue.
\par Ce qui correspond à : une limite uniforme de fonctions continues est continue.}
\Pre{Soit $x\in A$ \par Soit $\varepsilon\in\R_+^*$\par $f_n$ converge uniformémement vers $g$, on fixe donc $n_0\in\N$ tel que $\sup\limits_{x\in A}\Vert g(x)-f_{n_0}(x)\Vert<\frac{\varepsilon}{3}$ \par $f_{n_0}$ est continue en $x$, on peut donc prendre $\alpha\in\R_+^*$ tel que :\par $\forall h\in E, \Vert h\Vert <\alpha \text{ et } x+h\in A \Rightarrow \Vert f_{n_0}(x+h)-f_{n_0}(x)\Vert<\frac{\varepsilon}{3}$
\par Pour $h\in E$ tel que $\Vert h\Vert<\alpha$ et $x+h\in A$, on a :\par $\Vert g(x+h) - g(x)\Vert\leq\Vert g(x+h)-f_{n_0}(x+h)\Vert + \Vert f_{n_0}(x+h) -f_{n_0}(x) \Vert +\Vert f_{n_0}(x)-g(x)\Vert \leq \varepsilon$ \par Et donc $g(x+h)\to_{h\to 0} g(x)$ \par Donc $g$ est continue sur $A$}
\Thr{Application aux séries de fonctions}{Soit $(u_n)\in \mathcal{C}(A,F)^\N$, si $\sum u_n$ converge uniformément sur $A$, alors $x\mapsto\sum\limits_{n=0}^{+\infty}u_n(x)$ est continue.}
\Pre{On a que la suite des sommes partielles est une suite de sommes finies de fonctions continues, donc de fonctions continues. Et donc par le théorème précédent, on a que $\sum\limits_{n=0}^{+\infty}u_n$ est continue.}
\begin{Exe}
Notons $f:\left\{\begin{array}{rcl} \R_+ & \to & \R \\ x & \mapsto & \sum\limits_{n=1}^{+\infty}\frac{1}{n\sqrt{n+x}}\end{array}\right.$ \par Montrer que $f$ est bien définie et continue. \par Soit $x\in\R_+$, on a donc que $\frac{1}{n\sqrt{n+x}}>0$ et que $\frac{1}{n\sqrt{n+x}}\sim\frac{1}{n^{3/2}}$. Or $\sum \frac{1}{n^{3/2}}$ est convergente \par Donc $\sum\frac{1}{n\sqrt{n+x}}$ converge simplement vers $f$ par critère d'équivalent positif. \par Pour $n\in\N^*$, on a que $x\mapsto \frac{1}{n\sqrt{n+x}}$ est continue.\par Montrons la convergence uniforme.
\par Soient $n\in\N^*, x\in\R_+$. \par Alors on a que $\vert \frac{1}{n\sqrt{n+x}}\leq\frac{1}{n^{3/2}}$ \par Or $\sum\frac{1}{n^{3/2}}$ converge donc $\left(\sum x\mapsto\frac{1}{n\sqrt{n+x}}\right)$ converge normalement sur $\R_+$ donc converge uniformément sur $\R_+$ \par Donc $f$ est continue par théorème de continuité uniforme
\end{Exe}
\begin{Exe}
Notons $f:\left\{\begin{array}{rcl} \R_+ & \to & \R \\ x & \mapsto & \sum\limits_{n=1}^{+\infty}\frac{\sqrt{x^2+n}}{n(n+2)}\end{array}\right.$ \par Montrer que $f$ est bien définie et continue. \par Soit $x\in\R_+$, on a donc que $\frac{\sqrt{x^2+n}}{n(n+2)}>0$ et que $\frac{\sqrt{x^2+n}}{n(n+2)}\sim\frac{1}{n^{3/2}}$. Or $\sum \frac{1}{n^{3/2}}$ est convergente \par Donc $\sum\frac{\sqrt{x^2+n}}{n(n+2)}$ converge simplement vers $f$ par critère d'équivalent positif. \par Pour $n\in\N^*$, on a que $x\mapsto \frac{1}{n\sqrt{n+x}}$ est continue.\par Montrons la convergence uniforme.
\par On a que $u_n(n)\sim\frac{1}{2n}$, et donc $\sup\limits_{\R_+}\Vert u_n\Vert>\frac{1}{2n}$, donc $\sum \frac{\sqrt{x^2+n}}{n(n+2)}$ ne converge pas normalement sur $\R_+$. \par Soit $A\in\R_+$, soit $x\in [0,A]$ \par Soit $n\in\N^*$, alors $\vert \frac{\sqrt{x^2+n}}{n(n+2)}\vert\leq \vert\frac{\sqrt{A^2+n}}{n^2}\sim \frac{1}{n^{3/2}}$ \par Par critère d'équivalent positif, on a que $\sum u_n$ converge normalement sur $[0,A]$ pour tout $A\in\R_+$ \par Donc pour tout $A\in\R_+$, $(\sum u_n)$ converge uniformément sur $[0,A]$ \par Par théorème de continuité uniforme : Pour $A\in\R_+$, $(\sum\limits_{n=1}^{+\infty}u_n)$ est continue sur $[0,A]$.
\par Donc $\sum\limits_{n=1}^{+\infty}u_n$ est continue sur $\R_+$ : en effet, la continuité est une notion locale, l'intersection infinie conserve la propriété.
\end{Exe}
\begin{Exe}
Notons $f:\left\{\begin{array}{rcl} \R_+^* & \to & \R \\ x & \mapsto & \sum\limits_{n=1}^{+\infty}\frac{1}{1+n^2x}\end{array}\right.$ \par Montrer que $f$ est bien définie et continue. \par Soit $x\in\R_+$, on a donc que $\frac{1}{1+n^2x}>0$ et que $\frac{1}{1+n^2x}\sim\frac{1}{n^2x}$. Or $\sum \frac{1}{n^2x}$ est convergente \par Donc $\frac{1}{1+n^2x}$ converge simplement vers $f$ par critère d'équivalent positif. \par Pour $n\in\N^*$, on a que $x\mapsto \frac{1}{1+n^2x}$ est continue.\par Montrons la convergence uniforme.
\par Soit $A\in\R_+$, soit $x\in [0,A]$ \par Soit $n\in\N^*$, alors $\vert \frac{1}{1+n^2x}\vert\leq \vert\frac{1}{1+n^2A}\sim \frac{1}{n^2A}$ \par Par critère d'équivalent positif, on a que $\sum u_n$ converge normalement sur $[0,A]$ pour tout $A\in\R_+$ \par Donc pour tout $A\in\R_+$, $(\sum u_n)$ converge uniformément sur $[0,A]$ \par Par théorème de continuité uniforme : Pour $A\in\R_+$, $(\sum\limits_{n=1}^{+\infty}u_n)$ est continue sur $[0,A]$.
\par Donc $\sum\limits_{n=1}^{+\infty}u_n$ est continue sur $\R_+$.
\end{Exe}
\begin{Exe}
Notons $\exp:\left\{\begin{array}{rcl} \mathcal{M}_n(\K) & \to & \mathcal{M}_n(\K) \\ A & \mapsto & \exp(A)\end{array}\right.$ \par Montrer que $f$ est bien définie et continue. \par Soit $N$ une norme d'algèbre sur $\mathcal{M}_n(\K)$ \par Par récurrence on prouve que $N(A^n)\leq N(A)^n$ \par Et donc $\frac{1}{n!}N(A^n)\leq \frac{1}{n!}N(A)^n$ et donc $\sum\frac{1}{n!}A^n$ converge simplement dans $\mathcal{M}_n(\K)$
\par $\forall n\in\N, \frac{1}{n!}A^n$ est continue \par Montrons la convergence normale : \par Si $R\in\R_+^*$, $A\in\mathcal{B}(0,R)$ alors $\frac{1}{n!}N(A^n)\leq \frac{R^n}{n!}$ et comme $\sum \frac{R^n}{n!}$ converge, alors on a que $\exp$ converge normalement (et donc uniformément) dans $\mathcal{B}(0,R)$. \par Donc par théorème de continuité uniforme, $A\mapsto \exp(A)$ est continue sur $\mathcal{B}(0,R)$ pour tout $R\in\R_+$ \par Et donc $A\mapsto\exp(A)$ est continue sur $\mathcal{M}_n(\K)$
\end{Exe}    
\Thr{Extension de limite uniforme}{Soit $(f_n)\in\mathcal{F}(A,F)^\N$, soit $a\in\bar{A}$. \par Si $\forall n\in\N, f_n(x)\to_{x\to a}l_n\text{ et }f_n\text{ converge uniformement vers }g$ \par Alors $(l_n)$ est convergente et $g(x)\to_{x\to a}\lim\limits_{n\to+\infty}l_n$
\par ie : $g$ a une limite finie en $a$ et $\lim\limits_{x\to a} \lim\limits_{n\to+\infty}l_n = \lim\limits_{n\to+\infty}\lim\limits_{x\to a} f_n$ \par Ce théorème s'étend lorsque $E=\R$ et que $a=\pm\infty$}
\Pre{Sous les hypothèses du théorème : si on admet que $(l_n)$ est convergente, on a une démonstration identique à celle de la continuité. \par Pour $x\in A$, on a : $\Vert g(x)-\lim l_n\Vert\leq \Vert g(x)-f_n(x)\Vert + \Vert f_n(x)-l_n\Vert + \Vert l_n-\lim l_n\Vert$ \par Ces trois termes sont plus petit qu'un $\varepsilon$ donné pour $n$ suffisamment grand pour le premier et troisième terme et pour $\Vert x-a\Vert<\alpha$ pour le deuxième terme.}
\Thr{échange de limites de séries}{Avec $(u_n)\in\mathcal{F}(A,F)^\N$ et $a\in\bar{A}$ \par Si $\forall n\in\N, u_n(x)\to_{x\to a}v_n$ et que $(\sum u_n)$ converge uniformément, alors $\left(\sum\limits_{n=0}^{+\infty}u_n\right)$ a une limite en $a$ et $\lim\limits_{x\to a}\sum\limits_{n=0}^{+\infty}u_n(x)=\sum\limits_{n=0}^{+\infty}v_n$}
\begin{Exe}
Reprenons $f:\left\{\begin{array}{rcl} \R_+ & \to & \R \\ x & \mapsto & \sum\limits_{n=1}^{+\infty}\frac{1}{n\sqrt{n+x}}\end{array}\right.$
\par On rappelle que $f$ converge normalement sur $\R_+$ puisque $\forall n\in\N*, \forall x\in\R_+, \vert u_n(x)\vert\leq \frac{1}{n^{3/2}}$, terme général d'une série convergente.
\par Pour déterminer la limite de $f$ en l'infini, on note que $\forall n\in\N, u_n(x)\to_{x\to+\infty}0$
\par $\sum u_n$ converge normalement donc uniformément sur $\R_+$, donc $\lim\limits_{x\to+\infty} f_x$ existe
\par et alors $f(x)\to \sum\limits_{n=0}^{+\infty}\lim\limits_{x\to+\infty} u_n(x) = 0$
\end{Exe}
\begin{Exe}
Soit $\zeta:x\mapsto \sum\limits_{n=1}^{+\infty}\frac{1}{n^x}$ sur $\R$. Donner son domaine de définition, sa continuité et sa limite en $+\infty$. \par Soit $x\in\R$, alors $\sum\frac{1}{n^x}$ converge si, et seulement si, $x>1$ \par Donc le domaine de définition de $\zeta$ est $D=]1,+\infty[$ \par Soit $a\in D, n\in\N^*, x\in[a, +\infty[$, on a $0<\frac{1}{x}\leq\frac{1}{n^a}$ \par $\sum\frac{1}{n^a}$ converge et donc $\sum\left(x\mapsto\frac{1}{n^x}\right)$ converge normalement sur $[a,+\infty[$
\par De plus, pour tout $n\in\N^*$, on a que $x\mapsto\frac{1}{n^x}$ est uniforme \par Donc par théorème de continuité uniforme, $\zeta$ est continue sur $[a,+\infty[$ pour tout $a>1$ donc sur $]1,+\infty[$ \par Pour la limite en l'infini : $\forall n\in\N^*, \frac{1}{n^x}\to_{x\to\infty} \left\{\begin{array}{rc} 1 & n=1 \\ 0 & n>1\end{array}\right.$ \par $\sum x\mapsto\frac{1}{n^x}$ converge normalement donc uniformément sur $[2022,+\infty[$ \par Donc par théorème d'échange de limites de séries, $\zeta(x)\to_{x\to+\infty}1$ \par Pour la "limite" en $1$ de $\zeta$. Soit $A>0$ \par $\sum\frac{1}{n}$ diverge comme $\frac{1}{n}>0$ et $\left(\sum\limits_{k=1}^n\frac{1}{k}\right)\to+\infty$
\par On peut donc trouver $N$ tel que $\sum\limits_{k=1}^N\frac{1}{k}>A+1$ \par $\sum\limits_{k=1}^N\frac{1}{k^x}\to_{x\to 1}\sum\limits_{k=1}^N\frac{1}{k}$ \par On peut donc fixer $\alpha$ tel que $1<x<\alpha\Rightarrow \left|\sum\limits_{k=1}^N\frac{1}{k^x}-\sum\limits_{k=1}^N\frac{1}{k}\right|<1$ \par Pour $x\in]1,\alpha[$, alors $\zeta(x)\geq \sum\limits_{k=1}^N\frac{1}{k^x}\geq \left|-\vert\sum\limits_{k=1}^N\frac{1}{k^x}-\sum\limits_{k=1}^N\frac{1}{k}\vert + \sum\limits_{k=1}^N\frac{1}{k}\right|\geq A$ \par Donc $\lim\limits_{x\to 1}\zeta(x)=+\infty$
\end{Exe}

\section{Intégration et dérivation}
On s'intéresse aux fonctions de $I\subset\R$ à valeurs dans $F$ où $I$ est un intervalle.
\Thr{Intégration uniforme ou théorème d'échange limite-intégrale uniforme}{Soit ${a,b}$ un segment, $(f_n)\in\mathcal{C}_0([a,b],F)^\N$ \par si $(f_n)$ converge uniformément vers $g$ sur $[a,b]$, alors $\int_a^bf_n\to \int_a^bg$ \par Ce qui correspond à $\lim\limits_{n\to+\infty}\int_a^bf_n = \int_a^b\lim\limits_{n\to+\infty}f_n$}
\Pre{$\left\Vert\int\limits_{[a,b]}f_n-\int\limits_{[a,b]}g\right\Vert\leq \int\limits_{[a,b]}\Vert f_n-g\Vert\leq \vert b-a\vert\Vert f_n-g\Vert_\infty$ \par Comme $f_n$ converge uniformément vers $g$, alors $\Vert f_n-g\Vert_\infty\to 0$ \par Donc $\int\limits_{[a,b]}f_n\to\int\limits_{[a,b]}g$}
\begin{Exe}
$\int_0^1n^2t^n(1-t)dt = n^2(\frac{1}{n+1}-\frac{1}{n+1}) = \frac{n^2}{(n+1)(n+2)}$ \par Et $\dfrac{n^2}{(n+1)(n+2)}\to_{n\to +\infty} 1$ \par La fonction $f_n:\left\{\begin{array}{rcl} [0,1] & \to & \R \\ t & \mapsto & n^2t^n(1-t)\end{array}\right.$ converge simplement vers 0. \par Donc si elle converge uniformément, ça sera vers 0, cependant son intégrale ne tend pas vers $0$, donc il n'y a pas de convergence uniforme.
\end{Exe}
\begin{Rem}
Pour une série de fonctions, le théorème devient : si $(u_n)\in\mathcal{C}_0([a,b],F)^\N$ et si $\sum u_n$ converge uniformément sur $[a,b]$ alors $\int_a^b\sum\limits_{n=0}^{+\infty}u_n(t)dt = \sum\limits_{n=0}^{+\infty}\int_a^bu_n(t)dt$
\end{Rem}
\Thr{}{Soit $(f_n)$ une suite de fonctions continues d'un intervalle $I$ de $\R$ à valeurs dans $F$ convergeant uniformément vers $g$ sur tout segment de $I$ \par Soit $a\in I$, on a alors : $F_n:\begin{array}{rcl}I & \to & F \\ x & \mapsto & \int_a^bf_n(t)dt\end{array}$ et $G:\begin{array}{rcl}I & \to & F \\ x & \mapsto & \int_a^bg(t)dt\end{array}$ \par Alors $F_n$ converge uniformément vers $G$ sur tout segment de $I$.}
\Pre{La définition de $G$ est correcte car $f_n$ converge uniformément sur vers $g$ sur tout segment de $I$ donc $g$ est continue
\par À $x$ fixé, comme $f_n$ converge uniformément vers $g$ sur le segment $[a,x]$, d'après le théorème précédent $\int\limits_{[a,x]}f_n\to \int\limits_{[a,x]}g$ donc $F_n$ converge simplement vers $G$ sur $I$
\par Soit $S$ un segment inclu dans $I$, soit $b$ un point de $S$, alors $\forall x\in S, \Vert F_n(x)-G(x)\Vert =\left\Vert\int_b^x f_n(t) - g(t) dt\right\Vert$ \par $\leq \int_b^x\Vert f_n(t)-g(t)\Vert dt\leq \vert x-b\vert \sup\limits_{t\in[b,x]}\Vert f_(t)-g(t)\Vert\leq diam(S)\sup\limits_{S}\Vert f_n-g\Vert$ \par Qui tend vers 0 quand $n$ tend vers l'infini \par Ce qui conclut}
\Thr{Dérivation uniforme des suites de fonctions}{Avec $I$ un intervalle, si :\begin{itemize}\item $(f_n)\in\mathcal{C}^1(I,F)^\N$ \item $(f_n)$ converge simplement vers $g_0$ \item $(f'_n)$ converge uniformément vers $g_1$ sur tout segment de $I$\end{itemize} alors $g_0$ est $\mathcal{C}^1$ tel que $g_0'=g_1$ et $(f_n)$ converge uniformément sur tout segment de $I$.
\par Ce qui correspond à $g_0'=g_1 \Rightarrow (\lim f_n)=\lim (f_n')$}
\Pre{Pour $n\in\N$, $(f_n)$ est $\mathcal{C}^1$, donc $(f_n')$ est continue. \par Donc par théorème de continuité uniforme, $g_1$ est continue. \par D'autre part, si on fixe $a\in I, \forall x\in I, f_n(x)=f_n(a)+\int_a^xf_n'(t)dt$ \par Donc par théorème d'intégration uniforme, $g_0=\lim\limits_{n\to+\infty}f_n=g_0(a)+\int_a^xg_1(t)dt$ \par Et donc $g_0$ est $\mathcal{C}^1$ et $g_0'=g_1$ \par La suite $(f_n)$ converge uniformément sur tout segment vers $g_0$.}
\Thr{Théorème de dérivation des suites à l'ordre k}{Avec $I$ un intervalle, si :\begin{itemize}\item $(f_n)\in\mathcal{C}^k(I,F)^\N$ \item $\forall j\in\llbracket0,k-1\rrbracket, (f_n^{(j)})$ converge simplement vers $g_j$ \item $(f_n^{(k)})$ converge uniformément vers $g_k$ sur tout segment de $I$\end{itemize} alors $g_0$ est $\mathcal{C}^l$ tel que $\forall j\in\llbracket0,k\rrbracket, g_0^{(j)}=g_j$ et $(f_n^{(j)})$ converge uniformément sur tout segment de $I$.}
\Thr{Théorème de dérivation terme à terme à l'ordre k}{Avec $I$ un intervalle, si :\begin{itemize}\item $(u_n)\in\mathcal{C}^1(I,F)^\N$ \item $(\sum u_n)$ converge simplement \item $(\sum u_n)$ converge uniformément sur tout segment de $I$\end{itemize} alors $\sum\limits_{n=0}^{+\infty}u_n$ est $\mathcal{C}^1$ tel que $\forall j\in\llbracket0,k\rrbracket, \left(\sum\limits_{n=0}^{+\infty}u_n\right)'=\sum\limits_{n=0}^{+\infty}u_n'$ et la somme converge uniformément sur tout segment de $I$.}
\begin{Exe}
$f:\left\{\begin{array}{rcl}\R_+^* & \to & \R \\ x & \mapsto & \sum\limits_{n=0}^{+\infty}e^{-n^2x}\end{array}\right.$ en notant $u_n=e^{n^2x}$ \par Pour $n\in\N$, $u_n$ est $\mathcal{C}^\infty$, $u_n':x\mapsto -n^2e^{n^2x}$ et $u_n'':x\mapsto n^4e^{n^2x}$ \par Soit $x\in\R_+^*$, alors $\vert u_n(x)\vert=o(\frac{1}{n^2}), \vert u_n'(x)\vert=o(\frac{1}{n^2})$ \par $\sum u_n$ et $\sum u_n'$ convergent simplement sur $\R_+^*$ \par Soit $a\in\R_+^*$, soit $n\in\N$, soit $x\in[a,+\infty[$ \par $\vert n^4e^{-n^2x}\vert\leq n^4e^{-an^2}$ \par Or $\sum n^4e^{-an^2}$ est convergente
\par Donc la série $\sum x\mapsto n^4e^{-n^2x}$ converge normalement donc uniformément sur $[a,+\infty[$ pour tout $a$ \par Donc par théorème de dérivation terme à terme d'ordre 2, $f$ est $\mathcal{C}^2$ et $f':x\mapsto \sum\limits_{n=0}^{+\infty}-n^2e^{-n^2x}$ et $f'':x\mapsto \sum\limits_{n=0}^{+\infty}n^4e^{-n^2x}$
\end{Exe}

\section{Résultats de densité}
\Thr{Stone-Weierstrass (admis)}{Toute fonction continue sur un segment à valeurs dans $\K$ est limite uniforme d'une suite de fonctions polynomiales.
\par Ce qui correspond à : l'ensemble des fonctions polynomiales est dense dans $(\mathcal{C}([a,b],\K),\Vert.\Vert_\infty)$}
\begin{Exe}
Soit $E=\mathcal{C}([a,b],\R)$ un espace préhilbertien réel pour le produit scalaire $\langle f,g\rangle = \int\limits_{[a,b]}fg$ \par On a que $\R[X]\subset E$, déterminer $\R[X]^\perp$ \par Soit $f\in\R[X]^\perp$. Alors $f$ vérifie : $\forall P\in \R[X], \langle f,P\rangle = 0$
\par Par théorème de Stone-Weierstrass, on peut prendre $(P_n)\in\R[X]^\N$ qui converge uniformémemnt vers $f$ \par Donc $P_nf$ converge uniformément vers $f^2$ (comme $P_n$ borné sur $[a,b]$, avec $x\in[a,b], \Vert P_n(x)g(x)-f(x)g(x)\Vert_\infty=\Vert g(x)\Vert_\infty\Vert P_n(x)-f(x)\Vert_\infty\leq\Vert g(x)\Vert\sup\Vert f-P_n\Vert$)
\par Donc $\left(\int_a^bP_n(t)f(t)dt\right)\to \int_a^bf^2(t)dt$ \par Or $\forall n\in\N, \langle P_n,f\rangle=0$ \par Donc $\langle f,f\rangle=0$ donc $f=0$ \par Donc $\R[X]^\perp=\{0\}$
\end{Exe}
\begin{Rem}
Dans le théorème de Stone-Weierstrass, l'hypothèse du segment est fondametale.
\end{Rem}
\begin{Exe}
Il n'existe aucune suite de polynôme réels qui converge uniformément vers $\exp$ sur $\R$.
\par Supposons par l'absurde que $(P_n)\in\R[X]^\N$ converge uniformément vers $\exp$ sur $\R_+$ \par On a que $\frac{1}{\exp}$ est bornée sur $\R_+$, donc $t\mapsto P_n(t)e^{-t}$ converge uniformément vers la fonction constante de valeur 1.
\par Poru $n\in\N$ fixé, $P_n(t)e^{-t}\to_{t\to+\infty} 0$ par croissance comparée. Comme $t\mapsto P_n(t)e^{-t}$ converge uniformément sur $\R_+$, on peut appliquer le théorème d'échange des limites \par Donc $\lim\limits_{t\to+\infty}\lim\limits_{n\to+\infty}P_n(t)e^{-t}=\lim\limits_{n\to+\infty}\lim\limits_{t\to+\infty}P_n(t)e^{-t}$ \par Et donc $1=0$, ce qui est absurde. D'où le résultat.
\end{Exe}
\Thr{Approximmation uniforme par des fonctions en escalier}{Toute fonction continue sur un segment à valeurs dans $F$ est limite uniforme d'une suite de fonctions en escaliers \par Ce théorème est encore valable pour les fonctions continues par morceaux sur un segment.}
\Pre{Soit $f\in\mathcal{C}([a,b],F)$. Par théorème de Heine, $f$ est uniformément continue. \par Soit $n\in\N^*$, par uniforme continuité de $f$ on peut trouver $\alpha\in\R_+^*$ tel que $\forall x,y\in[a,b], \vert x-y\vert<\alpha\Rightarrow \Vert f(x)-f(y)\Vert < \frac{1}{n}$
\par On fixe $p\in\N^*$ tel que $\frac{b-a}{p}<\alpha$ (qui existe comme $\R$ est archimédien) \par On considère la subdivision $(x_i)_{0\leq i\leq p} = \left(a+i\frac{b-a}{p}\right)_{0\leq i\leq p}$ \par On définit la fonction en excalier $\varphi_n$ par $\forall t\in [a,b], (\exists i\in\llbracket 0,p\rrbracket \text{ tel que } t=x_i) \Rightarrow \varphi(t)=f(t)$ et $\forall i\in\llbracket 0,n-1\rrbracket,\forall t\in]x_i,x_{i+1}[, \varphi(t) = f\left(\frac{x_{i}+x_{i+1}}{2}\right))$
\par Alors $\forall t\in[a,b], \Vert \varphi_n(t)-f(t)\Vert\leq \frac{1}{n}$ car $\forall t\in[a,b],\exists i\in\llbracket 0,p-1\rrbracket, t\in ]x_{i}, x_{i+1}[, \vert t-\frac{x_i+i_{i+1}}{2}\vert\leq \vert x_{i+1}-x_i\vert \leq \frac{b-a}{p}< \alpha$ \par Donc $\Vert f(t)-f\left(\frac{x_i+x_{i+1}}{2}\right)\Vert<\frac{1}{n}$
\par Donc $\sup\limits_{t\in [a,b]}\Vert\varphi_n(t)-f(t)\Vert\leq \frac{1}{n}$}
\begin{Rem}
Ce résultat s'étend sans difficulté aux fonctions continues par morceaux. Il suffit de prendre une subdivision plus fine qu'une subdivision adaptée sur chaque intervalle continu.
\end{Rem}



\chapter{Séries entières}
\section{Généralités}
\Def{}{Soit $(a_n)\in\C^\N$. On appelle série entière associée à $(a_n)$ (de variable complexe) la série de fonctions $\left(\sum (z\mapsto a_nz^n)\right)$ qu'on notera en général $\left(\sum a_nz^n\right)$
\par $z\mapsto \sum\limits_{n=0}^{+\infty}a_nz^n$ est la somme de cette série entière.}
On va étudier en général le domaine de définition, les paramètres de continuité, de dérivabilité de la restriction à $\R$ se ces fonctions. 
\begin{Exe}
$\exp{z}=\sum\limits_{n=0}^{+\infty}\frac{z^n}{n!}$ est définie sur $\C$
\par $\sum\limits_{n=0}^{+\infty}z^n=\frac{1}{1-z}$ est définie pour $z\in\C, \vert z\vert<1$
\end{Exe}
\Prop{Lemme d'Abel}{Soit $(\sum a_nz^n)$ une série entière. Soit $z_0\in\C^*$. Si $(a_nz_0^n)$ est bornée, alors la série $\sum a_nz^n$ converge absolument pour $z\in\C,\vert z\vert < \vert z_0\vert$}
\Pre{Dans les conditons de l'énoncé : \par soit $n\in\N$, soit $z\in\C$ tel que $\vert z\vert<\vert z_0\vert$ \par Alors $\vert a_nz^n\vert = \vert a_n z_0^n \frac{z^n}{z_0^n}\vert \leq \vert a_nz_0^n\vert \left\vert\frac{z}{z_0}^n\right\vert \leq \sup\limits_{n\in\N}\vert a_nz_0^n\vert \left\vert\frac{z}{z_0}^n\right\vert$
\par $\vert\frac{z}{z_0}\vert<1$ donc $\sum(\frac{z}{z_0})^n$ est une série géométrique convergente \par Donc par critère de majoration positif, $\sum a_nz^n$ converge absolument.}
\Def{Rayon de convergence}{Soit $\sum a_nz^n$ une série entière. On appelle rayon de cette série $R = \sup\{\vert z\vert | z\in\C, (a_nz^n)\text{ est bornée}\}$ \par Par convention, $R=+\infty$ si cet ensemble n'est pas majoré.}
\Prop{}{Soit $\sum a_nz^n$ une série entière de rayon $R\in\bar{\R}$ \begin{itemize} \item $\forall z\in\mathcal{D}(0,R), \sum a_nz^n$ converge absolument \item $\forall z\in\C, \vert z\vert>R, a_nz^n$ est non-bornée.\end{itemize}}
\Pre{Soit $z\in\C, \vert z\vert<R$ \par Par caractérisation de la borne supérieure, on peut trouver $z_0\in\C$ tel que $(a_nz_0^z)$ est bornée et $\vert z\vert < \vert z_0\vert\leq R$ \par Par lemme d'Abel, $\sum a_nz^n$ converge. \par Soit $z\in\C, \vert z\vert > R$, alors $(a_nz^n)$ n'est pas bornée par définition de la borne supérieure, $R$ étant un majorant de l'ensemble des $z$ tels que $(a_nz^n)$ bornée.}
\begin{Exe}
Pour $\sum\frac{z^n}{n!}$, on a $R=+\infty$ \par Pour $\sum z^n$ on a $R=1$ et son domaine de définition n'inclut pas le bord, donc $\mathcal{D}(0,1)$ \par Pour $\sum\limits_{n\geq 1}\frac{z^n}{n^2}$, on a $R=1$ aussi, parce que l'ordre de grandeur de $n^2$ est négligeable face à une exponentielle. Le domaine de convergence inclut le bord du disque, et est donc $\mathcal{D}_f(0,1)$  \par $\sum\limits_{n\geq 1}\frac{z^n}{n}$ a un rayon $R=1$. La série converge pour $z=-1$ et diverge pour $z=1$, les deux appartenant au bord du disque.
\end{Exe}

\section{Calculs de rayon de convergence}
\Prop{}{Le rayon de $\sum a_nz^n$ est le rayon de $\sum \vert a_n z^n\vert$}
\begin{Rem}
Les déterminations de rayon d'une série entière ne font intervenir que des critères de séries numériques positives.
\end{Rem}
\Prop{Relations de comparaison}{Soient $\sum a_nz^n$ et $\sum b_nz^n$ des séries entières de rayons $R_a$ et $R_b$ :\begin{itemize}
\item Si $a_n = o(b_n)$ alors $R_a\geq R_b$
\item Si $a_n = \mathcal{O}(b_n)$ alors $R_a\geq R_b$
\item Si $a_n\sim b_n$ alors $R_a=R_b$
\end{itemize}}
\Pre{Soit $z\in\mathcal{D}(0,R_b)$, on a donc que $\sum \vert b_nz^n\vert$ converge, or $\vert a_nz^n\vert=\mathcal{O}(\vert b_nz^n\vert)$ \par Donc par critère de domination positive, $\sum\vert a_nz^n\vert$ converge \par Donc $z\in\mathcal{D}_f(0,R_a)$ \par Donc $\mathcal{D}(O,R_b)\subset \mathcal{D}_f(0,R_a)$ \par Donc $R_a\geq R_b$
\par On a donc prouvé la propriété pour les $\mathcal{O}$, et elle en découle pour les $o$. Comme un équivalent est aussi un $\mathcal{O}$ symétrique, on a l'égalité.}
\Thr{Critère de D'Alembert}{Si $(a_n)$ ne s'annule pas à partir d'un certain rang : \par si $\left\vert\frac{a_{n+1}}{a_n}\right\vert\to l$ avec $l\in\bar{\R}$, alors $R_a = \frac{1}{l}$
\par On prend la convention de $\frac{1}{+\infty}=0$ et que $\frac{1}{0}=+\infty$}
\Pre{On suppose que $\left\vert\frac{a_{n+1}}{a_n}\right\vert$ tend vers $l\in\bar{\R}$ \par Soit $z\in\C^*$, alors $\left\vert\frac{a_{n+1}z^{n+1}}{a_nz^n}\right\vert\to lz$ \par Par critère de D'Alembert appliqué aux séries numériques, si $\vert lz\vert <1$ alors $\sum a_nz^n$ ocnverge et si $\vert lz\vert>1$ alors $\sum a_nz^n$ diverge. \par Donc $R_a = \frac{1}{l}$.}
\begin{Rem}
Il faut faire attention aux séries lacunaires pour utiliser d'Alembert.
\end{Rem}
\begin{Exe}
Déterminer le rayon de $\sum\frac{(-1)^n(e^n+n)}{3^n+n^2}z^{2n}$ \par La suite associée à la série entière est équivalente à $\left(\frac{e}{3}\right)^n$, qu'on note $u_n(z)$ \par Soit $z\in\C^*$ :\par $\frac{\left\vert (\frac{e}{3})^{n+1}Z^{2n+2}\right\vert}{\left\vert(\frac{e}{3})^nz^{2n}\right\vert} = \left\vert\frac{e}{3}z^2\right\vert \to \frac{e}{3}z^2$ \par Donc $R=\sqrt{\frac{3}{e}}$ \par Donc si $\vert z\vert<\sqrt{\frac{3}{e}}$, $\sum\vert u_n(z)\vert$ converge et diverge sinon.
\end{Exe}
\begin{Exe}
Pour la série entière associée à la suite $a_n = \left\{\begin{array}{rcl} a_{n^2} & = & n! \\ a_p & = & 0 \text{ si p n'est pas un carré}\end{array}\right.$
\end{Exe}
\begin{Exe}
Pour la série entière $\sum\sin(n)z^n$
\end{Exe}

\section{Opérations sur les séries entières}
\Prop{Combinaisons linéaires}{Si $\sum a_nz^n$ et $\sum b_nz^n$ deux séries entières de rayons respectifs $R_a$ et $R_b$, alors : \par $\sum(a_n+b_n)z^n$ converge de rayon $R$ avec $R\geq\min(R_a,R_b)$ \par Si $R_a\neq R_b$, on peut affirmer que $R=\min(R_a,R_b)$ \par De plus, pour $z\in\mathcal{D}(0,R)$, on a $\sum\limits_{n=0}^{+\infty} (a_n+b_n)z^n = \sum\limits_{n=0}^{+\infty}a_nz^n + \sum\limits_{n=0}^{+\infty}b_nz^n$}
\Pre{Avec les notations de l'énoncé :\par Supposons que $R_a=R_b$ : \par Alors  \par Supposons que $R_a\neq R_b$, supposons sans perte de généralité que $R_a<R_b$ : \par Si $z\in\mathcal{D}(0,R_a)$, alors $\sum (a_n+b_n)z^n$ est bien définie, sinon non.}
\Thr{Produit de Cauchy}{Pour $\sum a_nz^n$ et $\sum b_nz^n$ deux séries entières de rayons respectifs $R_a$ et $R_b$ \par On pose pour $n\in\N$, $c_n = \sum\limits_{k=0}^na_kb_{n-k}$ \par La série entière $\sum c_nz^n$ est de rayon de convergence $R\geq\min(R_a,R_b)$ et $\forall z\in\C$ tel que $\vert z\vert < \min(R_a,R_b), \sum\limits_{n=0}^{+\infty}c_nz^n = \sum\limits_{n=0}^{+\infty}a_nz^n\sum\limits_{n=0}^{+\infty}b_nz^n$}
\Pre{Par application du produit de Cauchy à deux séries absolument convergentes sur un domaine.}

\section{Continuité de la somme d'une série entière sur les complexes}
\Prop{}{Soit $(\sum a_nz^n$ une série entière de rayon $r>0$, alors $(\sum a_nz^n)$ converge normalement sur tout disque fermé $\mathcal{B}(0,A)$ pour $A<R$}
\Pre{}

\Prop{Continuité}{La somme d'une série entière est continue sur son disque ouvert de convergence}
\Pre{}
\Prop{Somme de série entière sur $\R$}{Ici, considérons la restriction à $\R$ de la somme des séries entières considérées. \par Alors le domaine de définition dans $\R$, noté $\mathcal{D}$ est l'un des intervalles : $[-R,R], ]-R,R], [-R,R[,]-R,R[$}
\begin{Exe}

\end{Exe}
\Prop{}{Soit $(a_n)\in\C^\N$, alors les séries entières $\sum a_nz^n$ et $\sum na_nz^n$ ont même rayon.}
\Pre{}
\Prop{Corollaire}{Soit $(a_n)\in\C^\N$ $(\sum a_nz^n)$ est de rayon $R>0$ \par Alors $\varphi:\left\{\begin{array}{rcl}]-R,R[ & \to & \C \\ x & \mapsto & \sum\limits_{n=0}^{+\infty}a_nx^n \end{array}\right.$ est $\mathcal{C}^1$ et : \par $\varphi':\left\{\begin{array}{rcl}]-R,R[ & \to & \C \\ x & \mapsto & \sum\limits_{n=0}^{+\infty}na_nx^{n-1} \end{array}\right.$}
\Pre{}
\Prop{Corollaire}{La somme d'une série entière est $\mathcal{C}^\infty$ sur son intervalle ouvert de convergence \par Les dérivées s'obtiennent par dérivation terme à terme}
\Pre{Par récurrence}
\begin{Rem}
    Sur la valeur de dérivée :
\end{Rem}
\Prop{Corollaire}{Deux séries entières dont les sommes coïncident sur un voisinage de $0$ sont identiques \par i.e. si $\exists\alpha\in\R_+^*, \forall x\in ]-\alpha, \alpha[, \sum\limits_{n=0}^{+\infty}a_nx^n = \sum\limits_{n=0}^{+\infty}$, alors $\forall n\in\N, a_n=b_n$}
\Thr{Convergence radiale}{Soit $(a_n)\in\C^\N$, $(\sum a_nz^n)$ de rayon $R>0$ \par Si $\sum a_nR^n$ converge alors : $\sum\limits_{n=0}^{+\infty}a_nx^n\to_{x\to R}\sum\limits_{n=0}^{+\infty}a_nR^n$ pour $x\in]-R,R[$
\par Plus précisément, si $f$ définie en $R$ en tant que somme de série entière, alors $f$ continue sur $\mathcal{D}_f$}
\begin{Rem}
Le théorème reste valable pour $-R$ : par considération de $\sum a_n(-1)^nR^n$ qui est de même rayon.
\end{Rem}



\section{Développements en séries entières}
\subsection{Généralités}
\Def{Développement en série entière}{Soit $I$ un intervalle tel que $0\in\overset{\circ}{I}$. Soit $f\in\mathcal{F}(I,\C)$. \par On dit que $f$ est développable en série entière au voisinage de $0$ (ou en $0$) si :
\par $\exists\alpha\in\R_+^*,\exists (a_n)\in\C^\N, \forall x\in]-\alpha, \alpha[, f(x)=\sum\limits_{n=0}^{+\infty}a_nx^n$}
\Prop{}{\begin{itemize}
\item Si $f$ est développable en série entière (DSE) en $0$, alors $f$ est $\mathcal{C}^\infty$ sur un voisinage de $0$.
\item Si $f$ est développable en série entière, son développement est unique, et donc par unicité du DL on a :\par $\exists\alpha\in\R_+^*, \forall x\in]-\alpha,\alpha[, f(x)=\sum\limits_{n=0}^{+\infty}\frac{f^{(n)}(0)}{n!}x^n$
\item Si $f$ est $\mathcal{C}^\infty$ sur $I$ : on peut écrire que $\forall x\in I, \forall n\in\N, f(x) = \sum\limits_{k=0}^n \frac{f^{(k)}(0)}{k!}x^k + R_n(x)$
\item $f$ est développable en série entière en $0$ s'il existe $\alpha\in\R_+^*$ tel que $R_n$ converge simplement vers $0$ sur $]-\alpha, \alpha[$
\end{itemize}}

\subsection{Rappels sur Taylor}
\Thr{Taylor-Reste-Intégral}{Si $f$ est $\mathcal{C}^{n+1}$ sur $[a,b]$, alors : \par \begin{center} $f(b) = \sum\limits_{k=0}^n \frac{f^{(k)}(a)}{k!}(b-a)^k + R_n(b)$\end{center} \par Où $R_n(b) = \int_a^b\frac{(t-a)^n}{n!}f^{(n+1)}(t)dt$}
\Thr{Taylor-Lagrange}{En posant $t = a+(b-a)u$, on transforme l'expression du reste en $\int_0^1\frac{(b-a)^{n+1}(1-u)^n}{n!}f^{(n+1)}(a+(b-a)u)du$ \par On a donc : $R_n(b) = \frac{(b-a)^{n+1}}{n!}\int_0^1(1-u)^nf^{(n+1)}(a+(b-a)u)du$ \par De là on déduit la majoration de Lagrange, en posant $M_{n+1} = \sup\limits_{[a,b]}\vert f^{(n+1)}\vert$
\par $\vert R_n(b)\vert \leq \frac{\vert b-a\vert^{n+1}}{n!}\int_0^1\vert 1-u\vert^n\vert f^{(n+1)}(a+(b-a)u)\vert du \leq \frac{\vert b-a\vert^{n+1}}{n!}M_{n+1}\int_0^1(1-u)^ndu\leq \frac{\vert b-a\vert^{n+1}}{(n+1)!}M_{n+1}$}

\subsection{Développements en série entière découlant de l'exponentielle sur les réels}
$\forall x\in\R, \exp(x) = \sum\limits_{n=0}^{+\infty}\frac{x^n}{n!}$ \par \Pre{Pour $x\in\R$, on a que $\exp(x) = \sum\limits_{k=0}^{n}\frac{x^k}{k!}+R_n(x)$ par Taylor-Reste-Intégral. \par Alors $\vert R_n(x)\vert\leq \frac{\vert x\vert^{n+1}}{n!}e^{\vert x\vert}$ \par Or $\left(\frac{\vert x\vert^{n+1}}{n!}e^{\vert x\vert}\right)\to 0$ \par Et donc $R_n(x)\to 0$}
Et donc, pour tout $x\in\R$, on a les développemetns suivants :\begin{itemize}
\item $\cosh(x) = \frac{1}{2}(e^x +e^{-x}) = \sum\limits_{n=0}^{+\infty}\frac{x^{2n}}{(2n)!}$
\item $\sinh(x) = \frac{1}{2}(e^x-e^{-x}) =\sum\limits_{n=0}^{+\infty}\frac{x^{2n+1}}{(2n+1)!}$
\item $\cos(x)=\Re(e^{ix}) =\sum\limits_{n=0}^{+\infty}(-1)^n\frac{x^{2n}}{(2n)!}$
\item $\sin(x) = \Im(e^{ix})=\sum\limits_{n=0}^{+\infty}(-1)^n\frac{x^{2n+1}}{(2n+1)!}$
\end{itemize}
\subsection{Développements en série entière découlant de la série géométrique}
On a, $\forall x\in ]-1,1[$ :\begin{itemize}
\item $\frac{1}{1-x} = \sum\limits_{n=0}^{+\infty}x^n$
\item $\frac{1}{1+x} = \sum\limits_{n=0}^{+\infty}(-1)^nx^n$
\item $-\ln(1-x) = \sum\limits_{n=0}^{+\infty}\frac{x^{n+1}}{n+1}$ par primitivation d'un DL. Cette égalité peut être prolongée en $-1$ par théorème de convergence radiale.
\item $\ln(1+x) = \sum\limits_{n=0}^{+\infty}(-1)^n\frac{x^{n+1}}{n+1}=\sum\limits_{n=1}^{+\infty}(-1)^{n-1}\frac{x^n}{n}$. Cette égalité peut être prolongée en $1$ par théorème de convergence radiale.
\end{itemize}
Cette année est aussi au programme le développement d'Arctan :
\par $\forall x\in]-1,1[, \arctan'(x)=\frac{1}{1+x^2} =\sum\limits_{n=0}^{+\infty}(-1)^n x^{2n}$
\par Et donc : $\arctan(x) =\sum\limits_{n=0}^{+\infty}(-1)^n\frac{x^{2n+1}}{2n+1}$

\subsection{Développement de l'autre là}
En notant $f_\alpha :x\mapsto (1+x)^\alpha$, on a :
\par \begin{center} $f_\alpha(x)=1+\sum\limits_{n=0}^{+\infty}\frac{\alpha(\alpha-1)...(\alpha-k+1)}{k!}x^k$ \end{center}
\par "Je vais pas me taper le citron à sortir de $]-1,1[$" - Chakroun, 2022
\Pre{$f_\alpha$ est $\mathcal{C}^\infty$ sur $]-1,1[$. De plus, $\forall x\in]-1,1[, f_\alpha'(x) = \alpha(1+x)^{\alpha-1}$
\par Donc $f_\alpha$ est solution de l'équation différentielle $(1+x)y'=\alpha y : (E)$ \par C'est une équation différentielle linéaire d'ordre 1 résoluble, il y a donc unicité de la solution de cette équation vérifiant une condition initiale. \par Et donc $f_\alpha$ est l'unique solution de cette équation vérifiant $f_\alpha(0)=1$
\par Déterminons la suite de al décomposition en série entière de $f_\alpha$ par analyse-synthèse. \par Analyse : Soit $(a_n)\in\R^\N$, et on suppose que $\sum a_nx^n$ est une série entière de rayon $R>0$. On pose pour $x\in]-R,R[$, on pose $f(x)=\sum\limits_{n=0}^{+\infty}a_nx^n$
\par On a alors que $f$ est $\mathcal{C}^\infty$ sur $)-R,R[$ et que $\forall x\in ]-R,R[, f'(x) = \sum\limits_{n=1}^{+\infty}na_nx^{n-1}$
\par Soit $x\in]-R,R[$ : \par $(1-x)f'(x)-\alpha f(x) =(1+x) \sum\limits_{n=0}^{+\infty} (n+1)a_{n+1}x^n - \alpha \sum\limits_{n=0}^{+\infty}a_nx^n$ \par $=\sum\limits_{n=0}^{+\infty}(n+1)a_{n+1}x^{n+1} + \sum\limits_{n=0}^{+\infty}(n+1)a_{n+1}x^n -\alpha\sum\limits_{n=0}^{+\infty}a_nx^n$ \par $=\sum\limits_{n=1}^{+\infty}na_nx^n + \sum\limits_{n=0}^{+\infty}(n+1)a_{n+1}x^n -\alpha \sum\limits_{n=0}^{+\infty} a_nx^n$ (on peut mettre 0 comme indice de départ du premier terme puisque $na_n =0$ si $n=0$)
\par $=\sum\limits_{n=0}^{+\infty}\left[(n+1)a_{n+1}+(n-\alpha)a_n\right]x^n$ \par Et donc $f$ est solution de $(E)$ si, et suelement si : \par $\forall x\in]-R,R[, \sum\limits_{n=0}^{+\infty}[(n+1)a_{n+1}+(n-\alpha)a_n]x^n=0$ \par Et par unicité du développement, comme $0$ a comme développement la suite nulle, on déduit que $f$ est solution de $(E)$ si, et seulement si :\par $\forall n\in\N, (n+1)a_{n+1}+(n-\alpha)a_n=0$
\par Synthèse : considérons $(a_n)$ définie par $\left\{\begin{array}{r} a_0 =1 \\ a_{n+1} = \frac{\alpha - n}{n+1}a_n\end{array}\right.$ \par Si $\alpha\in\N$ : alors $\forall n\in\N, n\geq \alpha\Rightarrow a_n =0$ et donc $\sum a_nx^n$ est de rayon infini \par Si $\alpha\notin\N, \forall n\in\N, a_n\neq 0$ et $\left\vert\frac{a_{n+1}}{a_n}\right\vert =\left\vert\frac{\alpha-n}{n+1}\right\vert\to 1$ \par Et dans tous les cas, on a bien que pour $x\in]-1,1[, f(x)=\sum\limits_{n=0}^{+\infty}a_nx^n$
\par Les calculs précédents sotn valides sur $]-1,1[$, $f$ est solution de $(E)$ sur $]-1,1[$ et $f(0)=1$ \par Donc par unicité de la solution aux problème de Cauchy : \par $\forall x\in ]-1,1[, f_\alpha(x) =\sum\limits_{n=0}^{+\infty}a_nx^n$
\par Et par récurrence, $\left\{\begin{array}{rcl} a_n & = &\frac{\alpha(\alpha-1)...(\alpha-n+1)}{n!} \\ a_0 & = & 1\end{array}\right.$
}
Il y a certains cas particuliers à savoir refaire facilement, même si pas à savoir par coeur.
\par Pour $(1-x)^{\frac{1}{2}}$, on a : \par $(1-x)^{\frac{1}{2}} = 1+ \sum\limits_{n=1}^{+\infty} \frac{-\frac{1}{2}\left(-\frac{1}{2}-1\right)...\left(-\frac{1}{2}-n+1\right)}{n!}(-x)^n$ \par $=1+ \sum\limits_{n=1}^{+\infty}(-1)^n\frac{\frac{1}{2}\left(\frac{1}{2}+1\right)...\left(\frac{1}{2}+n-1\right)}{n!}(-1)^nx^n$ \par $=1+\sum\limits_{n=1}^{+\infty}\frac{1\times 3\times 5\times...\times (2n+1)}{2^n(n!)}x^n$ \par $=1+\sum\limits_{n=1}^{+\infty}\frac{(2n)!}{2^{2n}(n!)^2}x^n = 1+\sum\limits_{n=1}^{+\infty}\frac{1}{4^n}\binom{2n}{n}x^n$ \par $=\sum\limits_{n=0}^{+\infty}\frac{1}{4^n}\binom{2n}{n}x^n$
\par Et on en déduit, pour $x\in]-1,1[$ :\begin{itemize}
    \item $\arcsin'(x)=\frac{1}{1-x^2} = \sum\limits_{n=1}^{+\infty}\frac{1}{4^n}\binom{2n}{n}x^{2n}$
    \item $\arcsin(x) =\sum\limits_{n=0}^{+\infty}\frac{1}{4^n}\binom{2n}{n}\frac{x^{2n+1}}{2n+1}$
    \item $\arccos(x) =\frac{\pi}{2}-\sum\limits_{n=0}^{+\infty}\frac{1}{4^n}\binom{2n}{n}\frac{x^{2n+1}}{2n+1}$
\end{itemize}

\section{Séries entières dans une algèbre de dimension finie}
\Def{}{Soit $E$ une $\K$-algèbre de dimension finie munie d'une norme d'algèbre $\Vert.\Vert$
\par Soit $(a_n)\in\K^\N$, $\sum a_nz^n$ est de rayon $R>0$
\par Alors on peut définir $\sum\limits_{n=0}^{+\infty}a_nu^n$ pour $u\in E$ si $\Vert u\Vert < R$.}
\Pre{Soit $u\in E$ \par Pour $n\in\N$, $\Vert a_n u^n\Vert \leq \vert a_n\vert \Vert u\Vert^n$ (comme $\Vert.\Vert$ est une norme d'algèbre)
\par Pour $u\in \mathcal{B}(0,R)$, $\sum\vert a_n\vert\Vert u\Vert^n$ converge \par Donc $\sum a_nu^n$ converge absolument donc converge car $E$ est de dimension finie.}
\Prop{}{Et $\left\{\begin{array}{rcl} \mathcal{B}(0,R) & \to & E \\ u & \mapsto & \sum\limits_{n=0}^{+\infty} a_nu^n\end{array}\right.$ est continue}
\Pre{Soit $R'$ tel que $0<R'<R$ \par $\forall n\in\N, \forall u\in\mathcal{B}(0,R'), \Vert a_nu^n\Vert \leq \vert a_n\vert R^n$ \par Or $\sum \vert a_n\vert R^n$ converge normalement sur $\mathcal{B}_f(0, R')$
\par Soit $n\in\N$, $u\mapsto u^n$ est continue sur \par (car $E^n\mapsto E, (u_1,...,u_n)\mapsto u_1...u_n$ est n-linéaire en dimension finie) \par Donc d'après le théorème de continuité par convergence uniforme (valable en dimension finie), $u\mapsto \sum\limits_{n=0}^{+\infty}a_nu^n$ est continue sur $\mathcal{B}_(0,R')$ pour tout $R'<R$ donc sur $\mathcal{B}(0,R)$}
\Prop{}{$\forall u\in E, \exists P_u\in \K[X], \sum\limits_{n=0}^{+\infty}a_n u^n = P_u(u)$}
\Pre{Pour $u\in E$, $\K[u]$ est une sous-algèbre de dimesnion finie donc fermée, or $\sum\limits_{n=0}^{+\infty}a_nu^n = \lim\limits_{n\to+\infty}\sum\limits_{k=0}^na_ku^k$ \par Donc $\sum\limits_{n=0}^{+\infty}a_nu^n$ est la limite d'une suite de $\K[u]$ \par Donc $\sum\limits_{n=0}^{+\infty}a_nu^n\in \K[u]$}
\begin{Rem}
En particulier, $\left\{\begin{array}{rcl}\mathcal{M}_n(\K) & \to & \mathcal{GL}_n(\K) \\ A & \mapsto & \exp(A) = \sum\limits_{n=0}^{+\infty} \frac{1}{n!}A^n\end{array}\right.$ est continue, bien définie, de rayon $+\infty$ et est continue. 
\end{Rem}
\begin{Exe}
$\left\{\begin{array}{rcl}\mathcal{B}(0,1) & \to & \mathcal{GL}_n(\K) \\ A & \mapsto & \sum\limits_{n=0}^{+\infty} \frac{1}{\sqrt{n^2+n+1}}A^n\end{array}\right.$ est définie par exemple sur $\mathcal{B}(0,1)$ pour une norme d'algèbre, et est continue
\end{Exe}
\begin{Exe}
Pour $A\in \mathcal{M}_n(\K)$, on définit $\varphi_A:\left\{\begin{array}{rcl}\R & \to & \mathcal{M}_n(\K) \\ t & \mapsto & \exp(tA) = \sum\limits_{n=0}^{+\infty} \frac{1}{n!}A^n\end{array}\right.$
\par Montrer que $\varphi_A$ est $\mathcal{C}^1$ et exprimer $\varphi'_A$ \par $\sum t\mapsto \frac{t^n}{n!}A^n$ converge simplement sur $\R$ \par Pour $n\in\N$, $t\mapsto \frac{t^n}{n!}A^n$ est $\mathcal{C}^1$ de dérivée $t\mapsto \frac{t^{n-1}}{(n-1)!}A^n$ si $n>0$ et $0$ sinon
\par Soit $\alpha\in\R_+^*, t\in[-\alpha, \alpha]$ \par Soit $n\in\N^*$, alors : $\left\Vert\frac{t^{n-1}}{(n-1)!}A^n\right\Vert\leq\frac{\alpha^{n-1}}{(n-1)!}\Vert A\Vert^n$ \par Et $\sum\frac{\alpha^{n-1}}{(n-1)!}\Vert A\Vert^n$ converge \par Donc la série $\sum t\mapsto \frac{t^{n-1}}{(n-1)!}A^n$ converge normalement donc uniformément sur $[-\alpha, \alpha]$
\par Donc $\varphi_A$ est $\mathcal{C}^1$ sur $]-\alpha,\alpha[$ pour $\alpha\in\R_+^*$ donc sur $\R$ \par Et $\forall t\in\R, \varphi_A'(t) = \sum\limits_{n=1}^{+\infty}\frac{t^{n-1}}{(n-1)!}A^n = A\left(\sum\limits_{n=0}^{+\infty}\frac{t^n}{n!}A^n\right) = A\exp(tA)$
\par Application : Pour $A\in\mathcal{M}_n(\K)$, trouver des solutions de  $Y'-AY =0$ dans $\mathcal{F}(\R,\mathcal{M}_{n,1}(\K))$ \par C'est une équation différentielle linéaire d'ordre 1 à coefficients constants, qui correspond à un système :
\par $\left\{\begin{array}{rcl} y_1'& = &a_{1,1}y_1 + a_{1,2}y_2 +...+a_{1,n}y_n \\ y_2'& = & a_{2,1}y_2+...+a_{2,n}y_2 \\ ... & &  \\ y_n' &=& a_{n,1}y_n+...+a_{n,n}y_n \end{array}\right.$ \par On a que $\varphi:t\mapsto e^{tA}Y_0$ est une solution de l'équation vérifiant $\varphi(0)=Y_0$ \par On démontrera plus tard que c'est l'unique solution.
\end{Exe}


\chapter{Arithmétique dans un anneau euclidien}
\section{Généralités}
\Def{Anneau euclidien}{$A$ est un anneau euclidien si :\begin{itemize}
\item $A$ est un anneau commutatif intègre ($AB=0 \Rightarrow A=0\text{ ou } B=0$)
\item $A$ est muni d'une division euclidienne : il existe $\varphi\in\mathcal{F}(A\backslash\{0\},\N)$ telle que $\forall b\in A\backslash\{0\}, \forall a\in A, \exists q,r\in A, a =bq+r$ et $(r=0\text{ ou }\varphi(r)<\varphi(b)$
\end{itemize}}
$\Z$ et $\K{X}$ sont des anneaux euclidiens
\par Dans $\Z$, $\forall a\in \Z, \forall b\in \Z^*, \exists!(q,r)\in\Z^2, a =bq+r, 0\leq r<\vert b\vert$ \par On peut prendre $\varphi:a\mapsto \vert a\vert$
\par Dans $\K[X]$, $\forall A\in\K[X], \forall B\in\K[X]\backslash\{0\}, \exists !(Q,R)\in\K[X]^2, A=BQ+R$ et $\deg R<\deg B$ \par On peut prendre $\varphi:P\mapsto \deg P$
\Def{Divisibilité}{Soit $A$ un anneau euclidien (ou intègre). Soit $a,b\in A$. \par On dit que $b$ divise $a$ (noté $b|a$) ou que $a$ est un multiple de $b$ si $\exists c\in A, a = bc$}
\begin{Exe}
Tout élément de $A$ divise $0$
\par Tout inversible de $A$ est un diviseur universel : si $u$ est inversible, $\forall a\in A, a = uu^{-1}a$ \par Dans $\Z$, $\{-1,1\}$ sont les diviseurs universels \par Dans $\K[X]$, tout $a\in\K^*$ est un diviseur universel
\end{Exe}
\Prop{Association}{Soit $A$ un anneau euclidien \par Soit $a,b\in A$, $a$ divise $b$ et $b$ divise $a$ si, et seulement si, il existe un inversible tel que $b = au$}
\Pre{Le sens indirect est immédiat.
\par Pour le sens direct, on suppose que $a|b$ et $b|a$ \par On a donc $c\in A$ tel que $b=ca$ \par On a donc $d\in A$ tel que $a =db$ \par Donc $b=cdb$ \par Si $b=0$ : $a=0$ et la propriété est vérifiée \par Si $b\neq 0$ : on peut simplifier par $b$, comme $A$ est intègre : $cd=1$ dsonc $c$ et $d$ sont inversibles et inverses l'un de l'autre, ce qui conclut.}
\begin{Rem}
La relation divise est presque une relation d'ordre : elle est réflexive, transiftive et "presque" antisymétrique.
\end{Rem}
\Def{Idéal}{Soit $A$ un anneau euclidien. Soit $I\subset A$. \par On dit que $I$ est un idéal si : \begin{itemize}
\item $I$ est un sous-groupe de $(A,+)$
\item $\forall i\in I, \forall a\in A, ia\in I$
\end{itemize}}
\begin{Exe}
Soit $a\in A$, alors $aA = \{ax, x\in A\}$ est l'ensemble des multiples de $a$ et est un idéal. C'est même le plus petit idéal qui contient $a$, l'intersection de tous les idéaux contenant $a$.
\end{Exe}

\section{Opérations sur les idéaux}
\Prop{}{Avec $J$ un ensemble quelconque, $(I_j)_{j\in J}$ une famille d'idéaux de $A$, alors $\cap_{j\in J} I_j$ est un idéal}
\Pre{Faisons la stabilité par somme : \par si $x,y\in\cap_{j\in J}I_j$, alors $\forall j\in J, x,y\in I_j$ et par nature d'idéal, $\forall j\in J, x+y\in I_j$ \par Donc $x+y\in \cap_{j\in J}I_j$
\par Faisons la stabilité par produit : \par Si $x\in\cap_{j\in J}$, si $a\in A$ \par Alors $\forall j\in J, x\in I_j$, et par nature d'idéal, $\forall j\in J, ax\in I_j$ \par Donc $ax\in \cap_{j\in J}I_j$}
\begin{Rem}
Une union d'idéaux non-inclus dans les autres n'est pas un idéal, de même que pour des groupes.
\par En effet, avec $I,J$ deux groupes non-inclus l'un dans l'autre, on prend $x\in I, x\notin J, y\in J, y\notin I$ \par Cependant, par propriétés d'un goupe, $x+y\in I\cup J$, supposons sans perte de généralité que $x+y\in J$ \par Et donc $(x+y)-y\in J$ \par Donc $x\in J$, d'où la contradiction
\end{Rem}
\Prop{}{Si $I_1$ et $I_2$ sont des idéaux, alors $I_1+I_2$ est un idéal. \par C'est le plus petit idéal contenant $I_1\cup I_2$.}
\Pre{Montrons que $I_1+I_2$ est un idéal :
\par Soient $x,y\in I_1+I_2$, on note $x=x_1+x_2, y=y_1+y_2$ avec $x_1,y_1\in I_1, x_2, y_2\in I_2$ \par Alors $x+y = (x_1+y_1)+(x_2+y_2)\in I_1=I_2$ \par Donc $I_1+I_2$ est stable par l'addition. \par Soit $x = x_1+x_2\in I_1+I_2$, $a\in A$. Alors $ax = ax_1 + ax_2 \in I_1+I_2$
\par De plus, tout idéal contenant $I_1$ et $I_2$ contient $I_1+I_2$}
\Thr{}{Si $A$ est un anneau euclidien, alors tout idéal de $A$ est principal.
\par Cela signifie que si $I$ est un idéal de $A$, alors $\exists a\in A, I = aA$}
\Pre{On dispose de $\varphi\in\mathcal{F}(A\backslash\{0\}, \N)$ tel que $\forall a\in a, \forall b\in A\backslash\{0\}, \exists (q,r)\in A^2, a=bq+r$ et $(r=0\text{ ou }\varphi(r)<\varphi(b))$
\par Si $I=\{0\}$, c'est l'idéal $0A$
\par Si $I$ est un idéal non-réduit à $0$ : \par On peut considérer $n_0=\min\{\varphi(i)\vert i\in I\backslash\{0\}\}$, qui existe puisqu'on considère une partie non-vide de $\N$ \par On note $i_0$ tel que $\varphi(i_0)=n_0$ \par Soit $i\in I$, on peut alors prendre $q,r\in A$ telq que $i =qi_0+r$ et $(r=0\text{ ou }\varphi(r)<\varphi(b))$
\par $i-qi_0\in I$, et donc $r\in I$. $r=0$, si ce n'était pas le cas alors ça contredirait la définition de $n_0$. \par Donc $i\in i_0A$ \par Donc $I\subset i_0A$  \par $i_0A\subset I$ est immédiat puisque $i_0 \in I$}
\begin{Rem}
Deux générateurs d'un même idéal sont égaux à produit par un inversible près. Pour rendre le générateur unique, on fait un choix :\begin{itemize}
\item Dans $\Z$, on prend en général le générateur positif.
\item Dans $\K[X]$, on prend en général le générateur unitaire.
\end{itemize}
\end{Rem}
\begin{Exe}
Avec $E$ un $\K$-ev de dimension finie, $f\in\mathcal{L}(E), x\in E$ :
\par $I_f =\{P\in\K[X],P(f)=0\}$ est un idéal de $\K[X]$ (ce qui se prouve avec les formules sur $(P+Q)(f)$ et $(PQ)(f)$) \par $\pi_f$ est le générateur unitaire de cet idéal.
\par $I_{f,x} = \{P\in\K[X], P(f)(x)=0\}$ est un idéal de $\K[X]$ \par On note son générateur $\pi_{f,x}$ \par On a que $\pi_f\in I_{f,x}$, donc $\pi_{f,x}=\pi_f$ 
\end{Exe}

\section{PGCD et PPCM}
\Def{PGCD}{Avec $A$ un anneau euclidien, soit $a,b\in A$ \par On appelle $pgcd(a,b)$ un générateur de $aA+bA$ \par Dans $\Z$ ou $\K[X]$, on choisit un générateur spécifique, noté $a\wedge b$, et on l'appelle le pgcd. \par $aA+bA =(a\wedge b)A$}
\Prop{}{Soit $(a,b)\in A^2$, soit $d\in A$. $d$ est un diviseur de $a$ et $b$ si, et seulement si, $d$ divise $a\wedge b$}
\Pre{$\left\{\begin{array}{c} d \vert a \\ d \vert b\end{array}\right.\Leftrightarrow \left\{\begin{array}{c} aA \subset dA \\ bA \subset dA\end{array}\right.\Leftrightarrow aA+bA \subset dA \Leftrightarrow(a\wedge b)A\subset dA \Leftrightarrow d\vert a\wedge b$}
\Def{}{Soit $a,b\in A$, on dit que $a$ et $b$ sont premiers entre eux si $a\wedge b=1$}
\Thr{Bezout}{$a,b\in A$ sont premiers entre eux si, et seulement si, $\exists (u,v)\in A^2, au+bv=1$}
\Pre{S'il existe $(u,v)\in A^2, au+bv=1$ \par Alors $1\in aA+bA$ \par Donc $aA+bA =A$ \par Donc $a\wedge b = 1$ \par Si $a\wedge b = 1$ :\par Alors $1\in aA+bA$ \par Donc $\exists (u,v)\in A^2, au+bv=1$}
\Thr{Bezout étendu}{$\forall a,b\in A, \forall x\in A : \exists u,v\in A, x=au+bv \Leftrightarrow (a\wedge b)\vert x$}
\Def{}{Soit $A$ un anneau euclidien. Soit $a_1,...,a_n\in A$ \par On appelle $pgcd$ de $a_1,...,a_n$ le générateur de $a_1A + a_2A+...+a_nA$ \par On note que $(a_1\wedge a_2\wedge...\wedge a_n)A = a_1A+a_2A+...a_3A$}
\Prop{}{$\forall a_1,...,a_n\in A, \forall d\in A, [\forall i\in \llbracket 1,n\rrbracket, d\vert a_i] \Leftrightarrow d\vert(a_1\wedge a_2\wedge...\wedge a_n)$}
\Prop{}{$\forall a,b,c\in A, (a\wedge b)\wedge c = a\wedge (b\wedge c)$}
\Def{}{Avec $(a_1,a_2,...a_n)\in A^n$, on dit que $(a_1,a_2,...,a_n)$ sont premiers entre eux si $(a_1\wedge a_2\wedge...\wedge a_n)=1$}
\begin{Rem}
Attention à ne pas confondre premiers deux-à-deux et premiers entre eux dans leur ensemble. Par exemple, $(6, 15, 35)$ sont premiers dans leur ensemble mais pas premiers deux-à-deux.
\end{Rem}
\Thr{Bézout généralisé}{$(a_1,a_2...,a_n)\in A^n$ sont premiers entre eux si, et seulement si, $\exists (u_1,...,u_n)\in A^n, 1=a_1u_1+a_2u_2+...+a_nu_n$}
\Thr{}{Tout élément de $\Z$ et de $\K[X]$ peut se décomposer en un produit unique d'éléments irréductibles (éléments qui, à un inversible près, ont un seul diviseur) et d'un inversible.}
\Pre{Par récurrence forte sur la valeur absolue ou le degré pour l'existence, et par Lemme de Gauss pour l'unicité. Revoir le cours de sup.}
\Def{PPCM}{Soit $A$ un anneau euclidien, soit $a,b\in A$. On appelle ppcm de $a$ et $b$ un générateur de $aA\cap bA$, noté $a\vee b$
\par Dans $\Z$ ou $\K[X]$, on peut parler du ppcm pour l'entier positif ou polynôme unitaire ppcm. \par Et donc on a : $aA\cap bA=(a\vee b)A$}
\Prop{}{Soit $a,b\in A$, soit $p\in A$ \par $p$ est un multiple de $a$ et de $b$ si, et seuelement si, $p$ est un multiple du ppcm.}
\Pre{Dans le cadre de l'énoncé : \par $\left\{\begin{array}{r}a\vert p \\ b\vert p\end{array}\right.\Leftrightarrow\left\{\begin{array}{r}pA\subset aA \\ pA\subset bA\end{array}\right.\Leftrightarrow pA\subset aA\cap bA \Leftrightarrow pA\subset (a\vee b)A \Leftrightarrow a\vee b\vert p $}
\Prop{ppcm de n éléments}{Avec $a_1,...,a_n\in A$, alors $a_1\vee a_2\vee...\vee a_nA$ est bien un générateur de $a_1A\cap a_2A\cap...\cap a_nA$}
\Prop{}{$\forall a,b,c\in A, (a\vee b)\vee c = a\vee(b\vee c)$}
\Prop{}{Si $a,b,d\in A$ : $\left\{\begin{array}{r}a = da' \\ b = db'\end{array}\right. \text{ et } a'\wedge b' = 1\Leftrightarrow d\equiv a\wedge b$}
\Pre{Sens direct : on suppose que $d\vert a$ et $d\vert b$. \par $a'$ et $b'$ sont premiers entre eux, on peut donc prendre $u,v\in A$ tels que $a'u + b'v = 1$ \par Donc $da'u +db'u = au + bv = d$ \par Donc $d\in aA+bA$ \par Donc $a\wedge b\vert d$ \par Et donc $d\equiv a\wedge b$
\par Pour l'autre implication : on peut écrire $a =da'$ et $b=db'$. Par l'absurde, si $a'$ et $b'$ ne sont pas premiers, alors $d$ ne serait pas congru au pgcd.}

\section{Application à la réduction des endomorphismes}
\Thr{Lemme des noyaux}{Soit $E$ un $\K$-ev, $f\in \mathcal{L}(E)$.
\par Soit $P\in \K[X]$ tel que $P=P_1...P_n$ où $P_1,...,P_n$ sont premiers entre eux deux à deux.
\par $\ker P(f) = \bigoplus\limits_{i=1}^n\ker(P_i(f))$}
\Pre{Dans le cadre de l'énoncé : \par On pose pour $i\in\llbracket 1,n\rrbracket$ : $Q_i = \prod\limits_{1\leq j\leq n, j\neq i} P_j = \frac{P}{P_i}$ \par Montrons que $(Q_1,..., Q_n)$ sont premiers entre eux dans leur ensemble. \par Supposons par l'absurde que $Q_1,...,Q_n$ ne sont pas premiers entre eux. \par Donc $Q_1\wedge...\wedge Q_n \neq 1$ \par Donc $Q_1\wedge...\wedge Q_n$ possèdent un diviseur irréductible $Q$ \par En particulier, $Q\vert Q_1$ \par Donc $Q\vert P_2P3...P_n$
\par Comme $Q$ est irréductible, $\exists i_0\in \llbracket 2,n\rrbracket Q\vert P_{i_0}$ \par Mais $Q\vert Q_{i_0}$, donc $Q\vert \prod\limits_{j\neq i_0}P_j$ \par Donc $Q\vert P_{j_0}$ avec $j_0\neq i_0$ \par Mais $P_{j_0}\wedge P_{i_0}$ \par D'où la contradiction.
\par Par Bezout généralisé, on peut trouver $U_1,...,U_n\in\K[X]$ tels que $\sum\limits_{i=1}^nQ_iU_i=1$ \par On a que $\forall i\in\llbracket 1,n\rrbracket, \ker P_i(f) \subset\ker P(f)$ donc $\ker P_1(f)+\ker P_2(f)+...+\ker P_n(f)\subset \ker P(f)$ \par Analyse : soit $x\in \ker P(f)$, on suppose qu'il existe $(x_1,...,x_n)\in \ker P_1(f)\times...\times\ker P_n(f)$ tel que $x=x_1+...+x_n$ \par Soit $i\in \llbracket 1,n\rrbracket$, alors $Q_i(f)(x)=Q_i(f)(x_i)$ \par $U_i(f)\circ Q_i(f)(x) = (U_i(f)\circ Q_i(f))(x_i)$
\par Remarque : $\left(\sum\limits_{j=1}^nU_j(f)\circ Q_j(f)\right)(x_i) = (U_i(f)\circ Q_i(f))(x_i)$ \par Or $\sum\limits_{j=1}^nU_iQ_j=1$ \par Donc $\sum\limits_{j=1}^nU_j(f)Q_j(f)=id$ \par Donc $x_i = Q_i(f)\circ U_i(f)(x_i)$ \par Donc $x_i = (U_iQ_i)(f)(x)$ \par Donc $\ker P_1(f),..., \ker P_n(f)$ sont en somme directe.
\par Synthèse : soit $x\in \ker P(f)$ \par Alors $x = id(x) = \sum\limits_{i=1}^n(Q_iU_i)(f)(x)$ \par Et $\forall i\in\llbracket1,n\rrbracket, P_i(f)((Q_iU_i)(f)(x)) = (P_iQ_iU_i)(f)(x)=(U_iP)(f)(x)= U_i(f)\circ P(f)(x)=0$}
\begin{Exe}
$E$ un $\K$-ev, $f\in\mathcal{L}(E)$ telle que $f^3-id=0$ \par Si $\K=\C$ : $E =\ker(f-id)\oplus\ker(f-jid)\oplus\ker(f-j^2id)$ puisque $X^3-1=(X-1)(X-j)(X-j^2)$ \par Si $\K=R$, on peut écrire par le lemme des noyaux : $E=\ker(f-id)\oplus\ker(f^2+f-id)$
\end{Exe}
\Def{Espaces caractéristiques}{Soit $E$ un $\K$-ev de dimension finie, soit $f\in\mathcal{L}(E)$ telle que $\chi_f$ est scindé. \par $\chi_f = \prod\limits_{i=1}^p(x-\lambda_i)^{\alpha_i}$ avec $S_p(f)=\{\lambda_1,...,\lambda_p\}$ et $(\alpha_1,...,\alpha_p)\in\N^{*p}$ \par Alors $E=\bigoplus\limits_{1\leq i\leq p}\ker((f-\lambda_iid)^{\alpha_i})$ par lemme des noyaux.
\par Les sev de $E$, $F_i = \ker((f-\lambda_i)^{\alpha_i})$ sont appelés sous-espaces caractéristiques de $f$.\begin{itemize}
\item les sous-espaces caractéristiques sont stables par $f$ ;
\item $f$ est entièrement caractérisée par $f_1,f_2,...,f_p$ les endomorphismes induis par $f$ sur $F_1,...,F_p$ ;
\item $\forall i\in\llbracket 1,p\rrbracket, (f_i-\lambda_iid)^{\alpha_i}=0$, ou $f_i-\lambda_iid$ est nilpotent, donc $f_i$ a pour unique valeur propre $\lambda_i$ ;
\item les $f_i$ sont trigonalisables (leur polynôme caractéristique est scindé) ;
\item dans une base $\mathcal{B}=(\mathcal{B}_1,...,\mathcal{B}_p)$ où $\forall i\in\llbracket 1,p\rrbracket$, $\mathcal{B}_i$ est une base de diagonalisation de $f_i$, alors $Mat_\mathcal{B}(f)$ est diagonale par blocs, chaque bloc étant triangulaire de taille $\alpha_i\times \alpha_i$ ;
\item pour $i\in\llbracket 1,p\rrbracket$, $\dim F_i = \alpha_i$.
\end{itemize}}



\chapter{Intégration}
\section{Intégrale sur un segment (rappels)}
\Def{}{Soient $c,d$ deux réels avec $c<d$ \par Soit $f$ une fonction définie sur $[c,d]$ et à valeurs dans $\K$. La fonction $f$ est dite continue par morceaux s'il existe une subdivision $\sigma=(x_i)_{k\in\llbracket 0,n\rrbracket}$ telle que pour tout $i\in\llbracket 0,n-1\rrbracket$, la fonction $f_{]x_i,x_{i+1}[}$ se prolonge en une fonction fontinue sur $[x_i, x_{i+1}]$ \par Une telle subdivision est dite adaptée à $f$}
\begin{Rem}
Une fonction continue par morceaux sur un segment est bornée
\end{Rem}
\Def{}{Une fonction $f$ définie sur un intervalle $I$ et à valeurs dans $\K$ est dite continue par morceaux sur l'intervalle $I$ si sa restriction à tout segment inclus dans $I$ est continue par morceaux. \par On note $\mathcal{CM}(I, \K)$ l'ensemble des fonctions continues par morceaux de $I$ dans $\K$}
\Prop{}{Pour $f,g$ continues par morceaux sur $[a,b]$ : \begin{itemize}
\item Linéarité : pour $\lambda\in\K$ \par \begin{center}$\int_{[a,b]}f+\lambda g=\int_{[a,b]}f + \lambda \int_{[a,b]}g$ \end{center}
\item Positivité : pour $f$ rélle positive \par \begin{center} $\int_{[a,b]}f\geq 0$\end{center}
\item Croissante : pour $f,g$ réelles \par \begin{center}$f\leq g\Rightarrow \int_{[a,b]}f\leq\int_{[a,b]}g$\end{center}
\item Inégalité triangulaire : \par \begin{center} $\left\vert\int_{[a,b]}f\right\vert\leq\int_{[a,b]}\vert f\vert$\end{center}
\item Positivité améliorée : pour $f$ continue et positive \par \begin{center} $\int_{[a,b]}f=0\Rightarrow f=0$\end{center}
\end{itemize}}
\Prop{Chasles}{Soit $I$ un intervalle, $f$ une fonction continue par morceaux sur $I$, $a,b,c\in I$. \par\begin{center}$\int_a^bf(t)dt=\int_a^cf(t)dt+\int_c^bf(t)dt$\end{center}}
\Prop{}{Soit $I$ un intervalle. Soit $a\in I$ et $f$ une fonction continue par morceaux sur $I$ à valeur dans $\K$, on définit la fonction $F_a$ intégrale dépendant de la borne supérieur s'annulant en $a$ par : \par \begin{center}$F_a:\left\{\begin{array}{rcl} I & \to & \K \\ x & \mapsto & \int_a^xf(t)dt\end{array}\right.$\end{center} \begin{itemize}
\item $F_a$ est continue sur $I$
\item Si $f$ est continue, $F_a$ est $\mathcal{C}^1$ de dérivée $f$
\end{itemize}}
\begin{Rem}
Cette proposition permet d'énoncer le théorème fondamental de l'analyse : toute fonction continue sur un intervalle possède une primitive.
\end{Rem}
\Prop{Intégration par parties}{Soit $[a,b]$ un segment, $f,g$ des fonctions $\mathcal{C}^1$ sur $[a,b]$ \par\begin{center} $\int_a^bf'(t)g(t)dt = [f(t)g(t)]_a^b - \int_a^bf(t)g'(t)dt$\end{center}}
\Prop{Changement de variable}{Soit $[a,b]$ un segment, $\varphi$ $\mathcal{C}^1$ sur $[a,b]$ à valeurs réelles et $f$ une fonction continue sur $\varphi([a,b])$ \par \begin{center} $\int_a^bf(\varphi(t))\varphi'(t)dt = \int_{\varphi(a)}^{\varphi(b)} f(u)du$ \end{center}}
On dispose du résultat suivant, conséquence du théorème de Heine :
\Prop{Sommes de Riemann régulières}{Soit $[a,b]$ un segment et $f$ continue par morceaux sur $[a,b]$\par \begin{center}$\frac{b-a}{n}\sum\limits_{k=0}^{n-1}f\left(a+k\frac{b-a}{n}\right)\to_{n\to+\infty} \int_a^bf(t)dt$\end{center}}
\Pre{Dans le cas continu : \par Soit $\varepsilon\in\R_+^*$ \par $f$ est continue sur $[a,b]$ donc $f$ est uniformément continue \par On peut donc fixer $\alpha>0, \forall t_1,t_2\in[a,b], \vert t_1-t_2\vert<\alpha \Rightarrow \vert f(t_1)-f(t_2)\vert<\varepsilon$ \par \par Pour $k\in\rrbracket 0, n-1\rrbracket$, on note $x_k = a+k\frac{b-a}{n}$ \par On choisit $n_0\in \N$ tel que $\frac{b-a}{n_0}<\alpha$ \par Donc $\forall n\geq n_0, \frac{b-a}{n}<\alpha$
\par Pour $n\geq n_0$, \par $\left\vert\int_a^bf(t)dt-\frac{b-a}{n}\sum\limits_{k=0}^{n-1}f(x_k)\right\vert = \left\vert\sum\limits_{k=0}^{n-1}\int_{x_k}^{x_{k+1}}f(t)-f(k)dt\right\vert\leq \sum\limits_{k=0}^{n-1}\int_{x_k}^{x_{k+1}}\vert f(t)-f(x_k)\vert dt\leq\varepsilon(b-a)$ \par Dans le cas continu par morceaux, on se sert du fait qu'on soit sur un segment pour avoir une subdivision adaptée finie et ensuite appliquer Chasles.g}
\begin{Exe}
Donner un équivalent de $(S_n) = \left(\sum\limits_{k=1}^n\frac{1}{n^2+k^2}\right)$ \par $S_n = \frac{1}{n^2}\sum\limits_{k=1}^n\frac{1}{1+\frac{k^2}{n^2}}$ \par $t\mapsto \frac{1}{1+t^2}$ est continue sur $[0,1]$ \par Alors $\frac{1}{n}\sum\limits_{k=1}^n\frac{1}{1+\frac{k^2}{n^2}}\to\int_0^1\frac{dt}{1+t^2}$ \par Et donc $S_n\sim \frac{1}{n}\int_0^1\frac{dt}{1+t^2}$ \par Donc $S_n\sim \frac{\pi}{4n}$
\end{Exe}
\Prop{formule de Taylor avec reste intégral et majoration de Lagrange}{Soit $[a,b]$ un segment et $f$ une fonction $\mathcal{C}^{n+1}$ sur $[a,b]$ à valeurs dans $\K$. Alors : \par \begin{center}$\int_a^bf(t)dt=\sum\limits_{k=0}^nf^{(k)}(a)+R_n(f)$ \end{center} \par Où $R_n(f) =\frac{1}{n!}\int_a^b(b-t)^nf^{(n+1)}(t)dt = \frac{(b-a)^{n+1}}{n!}\int_0^1(1-u)^nf^{(n+1)}(a+u(b-a))du$ \par En majorant la deuxième forme du reste intégrale : \par \begin{center} $\vert R_n(f)\vert \leq \frac{\vert b-a\vert^{n+1}}{(n+1)!}\sup\limits_{[a,b]}\vert f^{(n+1)}\vert$\end{center}}


\section{Intégration sur un segment d'une fonction vectorielle}
La définition de l'intégrale sur un segment s'étend sans difficultés aux fonctions "vectorielles" continues par morceaux sur un segment, c'est à dire aux fonctions continues par morceaux sur un segment, à valer dans un $\K$-espace vectoriel de \textbf{dimension finie} $E$ \par Il suffit de fixer une base $\mathcal{B}$ \par Pour une fonction vectorielle $f$, de fonctions coordonnées $f_i$ dans $\mathcal{B}$ \par \begin{center} $\int_{[a,b]}f$\end{center} \par est le vecteur de $E$ de coordonnées dans $\mathcal{B}$ \par\begin{center}$\left(\int_{[a,b]}f_i\right)$\end{center}
\par On prouve la majorité des propriétés équivalentes à celles dans $\K$ en le faisant coordonnées par coordonnées, et on perd toutes les propriétés liées à la relation d'ordre, sauf l'inégalité triangulaire. \par Dans cette partie, $E$ désigne un $\K$-espace vectoriel de dimension finie.
\Prop{}{Pour $f,g$ continues par morceaux de $[a,b]$ dans $E$ :\begin{itemize}
\item Linéarité : pour $\lambda\in\K$ \par\begin{center} $\int_{[a,b]}f+\lambda g=\int_{[a,b]}f+\lambda \int_{[a,b]}g$\end{center}
\item Composition linéaire : si $L$ est une application linéaire de $E$ dans un $\K$-ev de dimension finie $F$ \par \begin{center}$\int_{[a,b]}L(f)=L\left(\int_{[a,b]}f\right)$\end{center}
\item Inégalité triangulaire : pour une norme quelconque $\Vert.\Vert$ \par \begin{center}$\left\Vert\int_{[a,b]}f\right\Vert\leq\int_{[a,b]}\Vert f\Vert$\end{center}
\end{itemize}}
\Pre{Preuve 8 : Pour l'inégalité triangulaire \par Soit $\Vert.\Vert$ une norme sur $E$ \par Soit $f$ continue par morceaux sur $[a,b]$, $a<b$ \par Soit $n\in\N$, alors $\left\Vert\frac{b-a}{n}\sum\limits_{k=0}^{n-1}f\left(a+k\frac{a-b}{n}\right)\right\Vert\leq \frac{b-a}{n}\sum\limits_{k=0}^{n-1}\left\Vert f\left(a+k\frac{b-a}{n}\right)\right\Vert$ \par On passe à la limite, par continuité de la norme et par sommes de Riemann. \par Alors $\left\Vert \int_{[a,b]}f\right\Vert \leq \int_{[a,b]}\Vert f\Vert$}
\Prop{Chasles}{Soit $I$ un intervalle, $f$ une fonction continue par morceaux sur $I$, $a,b,c\in I$. \par\begin{center}$\int_a^bf(t)dt=\int_a^cf(t)dt+\int_c^bf(t)dt$\end{center}}
\Prop{}{Soit $I$ un intervalle. Soit $a\in I$ et $f$ une fonction continue par morceaux sur $I$ à valeur dans $\K$, on définit la fonction $F_a$ intégrale dépendant de la borne supérieur s'annulant en $a$ par : \par \begin{center}$F_a:\left\{\begin{array}{rcl} I & \to & \K \\ x & \mapsto & \int_a^xf(t)dt\end{array}\right.$\end{center} \begin{itemize}
\item $F_a$ est continue sur $I$
\item Si $f$ est continue, $F_a$ est $\mathcal{C}^1$ de dérivée $f$
\end{itemize}}
\begin{Rem}
Le théorème fondamental de l'analyse reste valable pour une fonction vectorielle : toute fonction continue sur un intervalle possède une primitive.
\end{Rem}
\Prop{Intégration par parties}{Soit $[a,b]$ un segment, $\lambda$ une fonction à valeur dans $\K$, $\mathcal{C}^1$ sur $[a,b]$, $f$ une fonction $\mathcal{C}^1$ sur $[a,b]$ à valeurs dans $E$ \par\begin{center} $\int_a^b\lambda'(t)f(t)dt = [\lambda(t)f(t)]_a^b - \int_a^b\lambda(t)f'(t)dt$\end{center}}
\Prop{Changement de variable}{Soit $[a,b]$ un segment, $\varphi$ une fonction $\mathcal{C}^1$ sur $[a,b]$ à valeurs réelles et $f$ une fonction continue sur $\varphi([a,b])$ à valeurs dans $E$ \par\begin{center} $\int_a^bf(\varphi(t))\varphi'(t)dt = \int_{\varphi(a)}^{\varphi(b)}f(u)du$\end{center}}
\Prop{Sommes de Riemann régulières}{Soit $[a,b]$ un segment et $f$ continue par morceaux sur $[a,b]$\par \begin{center}$\frac{b-a}{n}\sum\limits_{k=0}^{n-1}f\left(a+k\frac{b-a}{n}\right)\to_{n\to+\infty} \int_a^bf(t)dt$\end{center}}
\Prop{formule de Taylor avec reste intégral et majoration de Lagrange}{Soit $[a,b]$ un segment et $f$ une fonction $\mathcal{C}^{n+1}$ sur $[a,b]$ à valeurs dans $\K$. Alors : \par \begin{center}$\int_a^bf(t)dt=\sum\limits_{k=0}^nf^{(k)}(a)+R_n(f)$ \end{center} \par Où $R_n(f) =\frac{1}{n!}\int_a^b(b-t)^nf^{(n+1)}(t)dt = \frac{(b-a)^{n+1}}{n!}\int_0^1(1-u)^nf^{(n+1)}(a+u(b-a))du$ \par En majorant la deuxième forme du reste intégrale : \par \begin{center} $\Vert R_n(f)\Vert \leq \frac{\vert b-a\vert^{n+1}}{(n+1)!}\sup\limits_{[a,b]}\Vert f^{(n+1)}\Vert$\end{center}}
\begin{Rem}
La majoration de Lagrange à l'ordre $0$ constitue ce qu'on appelle l'inégalité des accroissements finis : Soit $[a,b]$ un segment, et $f$ une fonction $\mathcal{C}^1$ sur $[a,b]$ à valeur dans $E$ \par\begin{center}$\Vert f(b)-f(a)\Vert \leq\vert b-a\vert\sup\limits_{[a,b]}\Vert f'\Vert$\end{center}
\end{Rem}


\section{Intégration sur un intervalle quelconque}
Dans tout ce chapitre, $\K$ désigne $\R$ ou $\C$ et les fonctions sont à valeurs dans $\K$
\subsection{Intégration sur un intervalle semi-ouvert}
\Def{}{Soit $f:[a,b[\to\K$ une fonction continue par morceaux avec $b\in\R\cup\{+\infty\}$ \par Notons $F$ la fonction : \par \begin{center} $F :\left\{\begin{array}{rcl}[a,b[ & \to & \K \\ x & \mapsto & \int_a^x f\end{array}\right.$ \end{center}
\par On dit que $\int_a^bf$ est convergente si $F(x)$ a une limite finie quand $x$ tend vers $b$. \par Dans ce cas, on note : \par \begin{center} $\int_a^bf=\lim\limits_{x\to b}\int_a^xf$\end{center} \par Dans le cas contraire, on dit que $\int_a^bf$ est divergente. \par Etudier la nature de $\int_a^bf$, c'est étudier si l'intégrale est convergente ou divergente.}
\begin{Exe}
Pour $\int_1^{+\infty}\frac{dt}{t^2}$ \par $t\mapsto\frac{1}{t^2}$ est continue sur $[1,+\infty[$, donc continue par morceaux. \par Pour $x\geq 1$ : $\int_1^x \frac{dt}{t^2} = \left[\frac{-1}{t}\right]_1^x = 1-\frac{1}{x}$\par Et $1-\frac{1}{x}\to_{x\to+\infty} 1$ \par Donc $\int_1^{+\infty}\frac{dt}{t^2}$ converge et $\int_1^{+\infty}\frac{dt}{t^2}=1$
\par Pour $\int_1^{+\infty}\frac{dt}{\sqrt{t^4+t^{3/2}+1}}$, on a aussi la convergence par des critères qu'on verra plus loin.
\par Pour calculer $\int_0^1-ln(1-t)dt$, on commence par établir que la valeur de $-ln(1-t)$ en $1$ est infinie, et continue en $[0,1[$ \par Pour $x\in[0,1[$ :\par $\int_0^x -ln(1-t) = \int_1^{1-x}ln(u)du$ en faisant un changement de variable $u=(1-t)$ \par $ = -\int_{1-x}^1 ln(u)du = \left[tln(t)-t\right]_{1-x}^1 = ln(1)-1 -( (1-x)ln(1-x) - (1 - x))$ \par $= x+(1-x)ln(1-x) = F(x)$ \par Et donc $F(x)\to_{x\to 1}1$ \par Donc $\int_0^1-ln(1-t)dt$ converge et $\int_0^1-ln(1-t)dt=1$
\par $\int_1^{+\infty}\frac{dt}{t}$ diverge. \par $\int_0^{+\infty}cos(t)dt$ diverge.
\end{Exe}
\begin{Rem}
La notation $\int_a^bf$ désigne à la fois l'intégrale impropre qui peu têtre convergente ou divergente, et en cas de convergence la "valeur" de l'intégrale qui est un élément de $\K$. Il n'y a pas comme pour les séries deux notations différentes. \par On peut employer aussi les notation : \begin{itemize}
\item $\int_{[a,b[}f$
\item $\int_a^bf(t)dt$
\end{itemize} 
\end{Rem}
\Def{}{Soit $f:]a,b]\to \K$ une fonction continue par morceaux avec $a\in\R\cup\{-\infty\}$ \par On dit que $\int_a^bf$ est convergente si $x\mapsto \int_x^bf$ admet une limite finie en $a$ \par Dans ce cas, on note \par\begin{center}$\int_a^bf=\lim\limits_{x\to a}\int_x^bf$\end{center} \par Dans le cas contraire, on dit que $\int_a^bf$ est divergente.}
\Prop{}{Soit $f:[a,b[\to\K$ une fonction continue par morceaux avec $b\in\R$\par Si $f$ admet une limite finie en $b$ alors $\int_a^bf$ est convergente \par On dit que c'est une intégrale faussement impropre (ou faussement généralisée).}
\begin{Rem}
On peut énoncer la même propriété pour la borne inférieure de l'intégrale.
\end{Rem}
\Prop{}{Soit $f:[a,b[\to\K$ une fonction continue par morceaux, soit $c\in[a,b[$, alors $\int_a^bf$ et $\int_c^bf$ sont de même nature.}
\Pre{Preuve 16 : $\forall x\in[a,b[, \int_a^xf(t)dt = \int_a^cf(t)dt + \int_c^xf(t)dt$ \par Et donc si $\int_a^x$ admet une limite quand $x$ tend vers $b$, alors comme $\int_c^xf(t)dt$ aussi.}
\begin{Rem}
La convergence de l'intégrale ne dépend que du comportement local en $b$ de la fonction. \par On peut énoncer la même propriété pour une intégrale impropre en $a$.
\par Attention : L'intégrale $\int_0^{+\infty}f$ d'une fonction $f$ peut converger sans que la fonction tende vers $0$ en $+\infty$
\end{Rem}

\subsection{Intégration sur un intervalle ouvert}
\Def{Intégrale sur un intervalle ouvert}{Soit $f\in\mathcal{CM}(]a,b[,\K)$ avec $-\infty\leq a<b\leq +\infty$
\par On dit que $\int_a^bf$ est convergente si pour $c\in]a,b[$, $\int_a^cf$, $\int_c^bf$ sont toutes les deux convergentes. \par On note alors $\int_a^bf = \int_a^cf + \int_c^bf = \lim\limits_{x\to a^+, y\to a^-}\int_x^yf$}
\begin{Exe}
Etudier la convergence de $\int_{-\infty}^{+\infty}\sin(t)dt$ \par $\int_0^{+\infty}\sin(t)dt$ diverge donc $\int_{-\infty}^{+\infty}\sin(t)dt$ diverge.
\end{Exe}

\subsection{Intégration des fonctions positives}
Notons qu'on peut énoncer le même type de proposition pour des fonctions définies sur un intervalle semi-ouvert à gauche.
\Thr{Rappel sur les limites monotones}{Avec $a<b$ : soit $g\in\mathcal{F}(]a,b[,\R)$ \begin{itemize}
\item S $g$ est croisssante majorée, alors $g$ a une limite en $b^-$
\item Si $g$ est croissante non-majorée, alors $g\to_{b^-}+\infty$
\item Si $g$ est croissante minorée, alors $g$ a une limite en $a^+$
\item Si $g$ est croissante non-minorée, alors $g\to_{a^-}-\infty$
\item S $g$ est décroisssante majorée, alors $g$ a une limite en $a^+$
\item Si $g$ est décroissante non-majorée, alors $g\to_{a^+}+\infty$
\item Si $g$ est décroissante minorée, alors $g$ a une limite en $b^-$
\item Si $g$ est décroissante non-minorée, alors $g\to_{b^+}-\infty$
\end{itemize}}
\Prop{}{Soit $a\in \R$, $b\in\R\cup\{+\infty\}$ avec $a<b$. Soit $f\in\mathcal{CM}([a,b[, \R)$ \par Si $f$ est positive, l'intégrale $\int_a^bf$ converge si, et seulement si, $x\mapsto\int_a^xf$ est majorée.}
\Pre{Proposition 17 : pour $f\geq 0$ \par $x\mapsto \int_a^bf$ est croissante : \par Soient $x,y\in[a,b[, x<y$ \par Alors $\int_a^yf -\int_a^x = \int_x^yf(t)dt\geq 0$ \par Donc on a la convergence si, et seulement si, $x\mapsto \int_a^x$ est majorée par théorème des limites monotones.}
\begin{Rem}
L'intégrale diverge si, et seulement si, $x\mapsto \int_a^xf$ tend vers $+\infty$ quand $x$ tend vers $b$
\par Par analogie avec la définition de la sommem d'une famille de réels positifs, on peut définir la valeur de l'intégrale d'une fonction positive continue par morceaux sur $[a,b[$ dans tous les cas :\begin{itemize}
\item $\int_a^bf=+\infty$ lorsque $\int_a^bf$ diverge
\item $\int_a^bf\in \R$ ou $\int_a^bf<+\infty$ lorsque $\int_a^bf$ converge
\end{itemize}
\end{Rem}
\Prop{}{Soit $a\in\R, b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f,g\in\mathcal{CM}([a,b[,\R)$ deux fonctions à valeurs positives telles que $0\leq f\leq g$:\begin{itemize}
\item Si $\int_a^bg$ converge alors $\int_a^bf$ converge.
\item Si $\int_a^bf$ diverge alors $\int_a^bg$ diverge. \end{itemize}}
\Pre{Même démonstration que pour la proposition précédente}
On peut l'appeler le critère de majoration positive, comme pour les séries.
\Prop{Corollaire}{Soit $a\in\R, b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f,g\in\mathcal{CM}([a,b[,\R)$ deux fonctions à valeurs positives telles que $f=_bo(g)$ ou $f=_b\mathcal{O}(g)$ :\begin{itemize}
\item Si $\int_a^bg$ converge alors $\int_a^bf$ converge.
\item Si $\int_a^bf$ diverge alors $\int_a^bg$ diverge.\end{itemize}}
\Pre{Démonstration équivalente à celle sur les séries.}
\Prop{Corollaire}{Soit $a\in\R, b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f,g\in\mathcal{CM}([a,b[,\R)$ deux fonctions à valeurs positives telles que $f\sim_b g$ \par Alors les intégrales $\int_a^bf$ et $\int_a^bg$ sont de même nature}
\Pre{Démonstration équivalente à celle sur les séries.}
\begin{Rem}
Attention : le résultat du commentaire s'applique également pour deux fonctions négatives et équivalentes. En revanche, si les fonctions ne sont pas de signe constant, les intégrales peut être de nature différentes.
\end{Rem}

\subsection{Intégrales de référence de Riemann}
\Prop{}{$\int_1^{+\infty}\frac{dt}{t^\alpha}$ converge si, et seulement si, $\alpha>1$}
\Pre{Par calcul de primitives.}
\Prop{}{$\int_0^{+\infty}e^{-\lambda t}dt$ converge si, et seulement si, $\lambda>0$}
\Prop{}{$\int_0^1\frac{dt}{t^\alpha}$ converge si, et seulement si, $\alpha<1$}
\Prop{}{$\int_0^1ln(t)dt$ converge}
\begin{Rem}
On a aussi que $\int_0^1\vert ln(t)\vert^\alpha$ converge, mais ce n'est pas au programme, donc on doit le retrouver à partir de $\vert ln(t)\vert^\alpha =o\left(\frac{1}{\sqrt[\alpha]{t}}\right)$ en $0$
\end{Rem}
\Prop{}{Si $a<b$ : \par $\int_a^b\frac{dt}{\vert t-a\vert^\alpha}$ converge si, et seulement si, $\alpha<1$}
\Prop{}{Si $a>b$ : \par $\int_b^a\frac{dt}{\vert b-t\vert^\alpha}$ converge si, et seulement si, $\alpha<1$}
\begin{Exe}
Existence et calcul éventuel de $\int_1^{+\infty}\frac{dt}{(t-1)(t+2)}$ et de $\int_2^{+\infty}\frac{dt}{(t-1)(t+2)}$ \par $f:t\mapsto \frac{1}{(t-1)(t+2)}$ est continue donc continue par morceaux sur $]1,+\infty[$ \par Etude en 1 : $\frac{1}{(t-1)(t+2)}\sim\frac{1}{3(t-1)}$ \par $f$ est positive, $\int_1^2\frac{dt}{t-1}$ diverge donc $\int_1^2f(t)dt$ diverge par critère d'équivalent positif. \par donc $\int_0^{+\infty}\frac{dt}{(t-1)(t+2)}$ diverge
\par $f(t)\sim_{+\infty}\frac{1}{t^2}$ \par $\int_2^{+\infty}\frac{dt}{t^2}$ converge, $f$ est positive et donc par critère d'équivalent positif, $\int_2^{+\infty}f(t)dt$ converge \par Pour le calcul : Faisons d'abord une décomposition en éléments simples \par $\int_2^{+\infty}f(t)dt = \int_2^{+\infty} \frac{1}{3(t-1)}-\frac{1}{3(t+2)}$ \par (L'intégrale de la somme est convergente mais l'intégrale de chaque élément simple est divergente) \par Soit $x\in[2,+\infty[$ : $\int_2^x\frac{1}{3(t-1)}-\frac{1}{3(t+2)}dt$ \par $=\left[\frac{1}{3}\ln\left(\frac{t-1}{t+2}\right)\right]_2^x = \frac{1}{3}\ln(4)-\frac{1}{3}\ln\left(\frac{x-1}{x+2}\right)$ \par Or $\ln\left(\frac{x-1}{x+2}\right)\to_{+\infty}0$ donc $\int_2^{+\infty}f(t)dt =\frac{1}{3}\ln(4)$
\end{Exe}
\begin{Exe}
Convergence de :\begin{enumerate} \item $\int_0^{+\infty} t^{12}e^{-t}dt$
\par $f\geq 0$, $f$ est continue donc continue par morceaux sur $[0,+\infty[$ \par $f=_\infty o(e^{\frac{-t}{2}})$ or $f=_\infty o(\frac{1}{t^2})$ \par $\int_1^{+\infty}\frac{dt}{t^2}$ converge donc par critère de domination positif, $\int_0^{+\infty}f(t)dt$ converge.
\item $\int_0^{+\infty}\frac{e^{-\sqrt{t}}}{t^{2/3}}dt$
\par $f\geq 0$, $f$ continue sur $]0,+\infty[$ \par En $0$ : $f\sim\frac{1}{t^{2/3}}$ et donc $\int_0^1f(t)dt$ converge par comparaison à du Riemann \par En $+\infty$ : $f=o\left(\frac{1}{t^2}\right)$ et on a la convergence aussi.
\item $\int_2^{+\infty}\frac{dt}{t^\alpha\ln(t)^\beta}$
\par $f$ est continue sur $[2,+\infty[$ positive : \par Si $\alpha>1$, alors $\frac{1}{t^\alpha(ln(t))^\beta}=o\left(\frac{1}{t^{\frac{1+\alpha}{2}}}\right)$ par croissance comparée \par $\int_1^{+\infty}\frac{dt}{t^{\frac{1+\alpha}{2}}}$ converge car $\frac{1+\alpha}{2}>1$ converge \par Donc par critère de domination positif, $\int_2^{+\infty}\frac{dt}{t^\alpha(ln(t))^\beta}$ converge \par Si $\alpha<1$, alors $\frac{1}{t^\alpha(ln(t))^\beta}\gg\frac{1}{t^{\frac{1+\alpha}{2}}}$ \par Donc $\int_2^{+\infty}\frac{dt}{t^\alpha(ln(t))^\beta}$ diverge.
\par Si $\alpha = 1$ : \par Pour $x>2$, $\int_2^{+\infty}\frac{dt}{t(ln(t))^\beta}=\int_{ln(2)}^{ln(x)}\frac{du}{u^\beta}$ (Changement de variable avec $u = ln(t), du = \frac{dt}{t}$) \par Et donc $\int_2^{+\infty}\frac{dt}{tln(t)^\beta}$ converge si, et seulement si, $\beta>1$ \par Et donc, $\int_2^{+\infty}\frac{dt}{t^\alpha\ln(t)^\beta}$ converge si, et seulement si, $\alpha>1$ ou ($\alpha = 1$ et $\beta>1$)
\item $\int_0^{1/2}\frac{dt}{t^\alpha\vert\ln(t)\vert^\beta}dt$ \par On a convergence si, et seulement si, $\alpha<1$ ou ($\alpha = 1$ et $\beta>1$)
\end{enumerate}
\end{Exe}

\subsection{Intégrabilité}
\Def{}{Soit $I$ un intervalle, $f$ continue par morceaux sur $I$ à valeurs dans $\K$ \par On dit que $\int_If$ est absolument convergente si $\int_I\vert f\vert$ converge.}
\begin{Rem}
On peut comme dans le cadre de la théorie de la sommabilité des familles de $\K$ utiliser la définition de la valeur de l'intégrale d'une fonction positive. Ainsi, la définition précédente peut s'écrire : \par\begin{center} $f$ est intégrable sur $I$ si $\int_I\vert f\vert<+\infty$ \end{center}
\end{Rem}
\Thr{}{Soit $f$ continue par morceaux sur un intervalle $I$. \par Si $\int_If$ est absolument convergente alors $\int_If$ est convergente.}
\Pre{Même démonstration que pour les séries : \par Pour $f$ réelle, $0\leq \vert f\vert-f\leq 2\vert f$ et on intègre \par Ensuite pour les fonctions complexes, on écrit que $\vert \Re(f)\vert \leq f$ et que $\vert \Im(f)\vert\leq\vert f\vert$}
\Prop{}{Une intégrale absolument convergente est convergente et pour $f$ intégrable sur un intervalle $I$ \par \begin{center}$\left\vert\int_If\right\vert\leq \int_I\vert f\vert$\end{center}}

\subsection{Propriétés des intégrales impropres}
\Prop{Propriétés des intégrales impropres}{\begin{itemize}
\item Linéarité : si $I$ un intervalle, $f,g$ continues par morceaux sur $I$ d'intégrales convergentes sur $I$, $\lambda\in\K$, alors : \par \begin{center}$\int_If + \lambda g$ converge et $\int_If+\lambda g = \int_I f+\lambda\int_Ig$ \end{center}
\item Positivité : si $f$ continue par morceaux sur $I$ réelle positive, d'intégrale sur $I$ convergente, alors $\int_If\geq 0$
\item Croissance : si $f$ et $g$ sont continues par morceaux sur $I$i réelles positives d'intégrales sur $I$ convergentes avec $f\leq g$, alors $\int_If\leq \int_Ig$
\item Inégalité triangulaire : si $f$ continue par morceaux sur $I$ intégrable sur $I$, alors $\left\vert\int_If\right\vert \leq\int_I\vert f\vert$ 
\item Positivité améliorée : si $I$ un intervalle, $f$ continue réelle positive sur $I$ d'intégrale sur $I$ convergente, alors $\int_If = 0 \Rightarrow \forall x\in I, f(x)=0$
\end{itemize}}
\Pre{Pour la positivité améliorée : cas $I = [a,b[$ \par $f\geq 0, \int_{[a,b[}f = \lim_{x\to b^-}\int_a^xf(t)dt = \sup\limits_{x\in[a,b[}\int_a^xf(t)dt$ \par (car $x\mapsto \int_a^xf(t)dt$ est croissante) \par Donc $\int_{[a,b[}f = 0 \Rightarrow \forall x\in [a,b[, 0\leq \int_a^x f(t)dt\leq 0$ \par Donc $\forall x\in[a,b[\int_a^xf(t)dt=0$ donc par stricte positivité d'une intégrale sur un segment, $f$ est nulle sur tout segment de $[a,b[$ donc nulle sur $[a,b[$}

\subsection{Relation de Chasles}
\Def{}{Soit $a,b\in\bar{\R}$, tels que $a<b$ et $f\in\mathcal{CM}(]b,a[,\K)$ telle que $\int_b^af$ converge. \par On définit $\int_a^bf(t)dt$ par $-\int_b^af(t)dt$ (comme pour une intégrale classique)}
\Prop{Relation de Chasles}{Soit $I$ un intervalle, soit $f\in\mathcal{CM}(I,\K)$. Soit $a,b,c$ dans l'adhérence de $I$ dans $\R\cup\{+\infty\}$ \par Si $\int_If$ converge, alors $\int_a^cf, \int_c^bf$ et $\int_a^bf$ convergent et \par\begin{center}$\int_a^bf=\int_a^cf+\int_c^bf$\end{center}}

\subsection{Fonction dépendantd de la borne supérieure}
\Prop{}{Pour $f\in\mathcal{CM}(I,\K)$ telle que $\int_If$ converge \par Soit $a\in\bar{I}$ (l'adhérence est prise dans $\bar{\R}$) \par Soit $F_a:\left\{\begin{array}{rcl} I & \to & \K \\ x & \mapsto & \int_a^xf \end{array}\right.$ la fonction intégrale dépendant de la borne supérieure\begin{itemize}
\item $F_a$ est continue sur $I$
\item Si $f$ est continue sur $I$, $F_a$ est $\mathcal{C}^1$ de dérivée $f$
\end{itemize}}

\subsection{Intégration par parties}
\Thr{Intégration par parties}{Avec $f,g$ $\mathcal{C}^1$ sur $]a,b[$,  si $fg$ a une limite en $a^+$ et en $b^-$ alors $\int_a^bf(t)g'(t)dt$ et $\int_a^bf'(t)g'(t)$ sont de même nature et en cas de convergence :\par \begin{center}$\int_a^bf(t)g'(t) = [f(t)g(t)]_a^b -\int_a^bf'(t)g(t)dt$ \end{center}}
\Pre{Avec $x,y$ tels que $a<x<y<b$, alors $\int_x^yfg' = [fg]_x^y - \int_x^yf'g$}
\begin{Exe}
$\int_1^{+\infty}\frac{\sin(t)}{t}dt$ \par $t\mapsto \frac{\sin(t)}{t}$ est continue sur $[1,+\infty[$ \par $\frac{-\cos(t)}{t}$ tend vers $0$ en l'infini : $\forall t\in [1, +\infty[, \left\vert\frac{-\cos(t)}{t}\right\vert\leq \frac{1}{t}$, d'où la limite \par Donc $\int_1^{+\infty}\frac{\sin(t)}{t}dt$ est de même nature que $\int_1^{+\infty}\frac{\cos(t)}{t^2}$ \par Et $\forall t\in[1, +\infty[,\left\vert\frac{\cos(t)}{t^2}\right\vert<\frac{1}{t^2}$ \par Donc par théorème de conparaison positive, $\int_1^{+\infty}\frac{\cos(t)}{t}dt$ converge absolument \par Donc $\int_1^{+\infty}\frac{\sin(t)}{t}$ converge et $\int_1^{+\infty}\frac{\sin(t)}{dt} = \left[\frac{-\cos(t)}{t}\right]_1^{+\infty} - \int_1^{+\infty}\frac{\cos(t)}{t^2}dt$
\end{Exe}

\subsection{Changement de variable}
\Thr{}{Soient $a,b,\alpha, \beta$ tels que $-\infty\leq a<b\leq +\infty, -\infty\leq\alpha<\beta\leq+\infty$ \par Soit $f\in]a,b[\to \K$ une fonction continue. \par SOit $\varphi:]\alpha,\beta[\to]a,b[$ une fonction bijective, strictement croissante et de classe $\mathcal{C}^1$
\par Les intégrales $\int_a^bf(t)dt$ et $\int_\alpha^\beta(f\circ\varphi)(u)\varphi'(u)du$ sont de même nature, et en cas de convergence \par\begin{center}$\int_a^bf(t)dt=\int_\alpha^\beta(f\circ\varphi)(u)\varphi'(u)du$\end{center}}
\begin{Rem}
On remarque que $a=\lim\limits_{x\to\alpha}\varphi(x)$ et $b=\lim\limits_{x\to\beta}\varphi(x)$
\end{Rem}
\begin{Rem}
On peut énoncer le même type de théorème avec $\varphi$ strictement décroissant. On a alors dans le cas convergent : \par\begin{center}$\int_a^bf(t)dt=-\int_\alpha^\beta(f\circ\varphi)(u)\varphi'(u)du$\end{center}
\end{Rem}

\subsection{Intégration des relations de comparaisons}
\Thr{Intégration des ordres de grandeur}{Soit $a\in\R$ et $b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f\in\mathcal{CM}([a,b[,\K)$ \par Soit $\varphi\in\mathcal{CM}([a,b[,\R)$ une fonction positive sur $[a,b[$\begin{itemize}
\item Si $\varphi$ est intégrable :\begin{itemize}
    \item Si $f=_b\mathcal{O}(\varphi)$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf=_b\mathcal{O}(\int_x^b\varphi)$
    \item Si $f=_bo(\varphi)$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf=_bo(\int_x^b\varphi)$
    \item Si $f\sim_b\varphi$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf\sim_b\int_x^b\varphi$
\end{itemize}
\item Si $\varphi$ n'est pas intégrable :\begin{itemize}
    \item Si $f=_b\mathcal{O}(\varphi)$ alors $\int_a^xf=_b\mathcal{O}(\int_a^x\varphi)$
    \item Si $f=_bo(\varphi)$ alors $\int_a^xf=_bo(\int_a^x\varphi)$
    \item Si $f\sim_b\varphi$ alors $f$ n'est pas intégrable sur $[a,b[$ et $\int_a^xf\sim_b\int_a^x\varphi$
\end{itemize}
\end{itemize}}
\Pre{Rappel sur la sommation des ordres de grandeurs : \par $u_n = o(w_n) \Leftrightarrow u_n = w_n\varepsilon_n$ avec $(\varepsilon_n)\to 0$ \par Dans le cas convergent : soit $\varepsilon>0$, prenons $n_0\in\N$ tel que $\forall n\in\N, n\geq n_0 \Rightarrow \vert\varepsilon_n\vert\leq \varepsilon_0$ \par Alors $\forall n\geq n_0, \left\vert\frac{\sum\limits_{k=n}^{+\infty}u_n}{\sum\limits_{k=n}^{+\infty}w_n}\right\vert \leq \left\vert\frac{\sum\limits_{k=n}^{+\infty}\varepsilon_nw_n}{\sum\limits_{k=n}^{+\infty}w_n}\right\vert\leq \varepsilon$
\par $f,g$ deux fonctions continues par morceaux sur $[a,b[$ : \par Si $f=_bo(g)$, prenons $\psi$ telle que $f=\psi g$ et $\psi\to_b 0$ \par Si $\int_a^bg(t)dt$ converge alors on a : \par Soit $\varepsilon>0$, prenons $\alpha>0$ tel que $\forall x\in ]b-\alpha,b[, \vert\psi(x)\vert<0$ \par Pour un tel x : $\left\vert\frac{\int_x^bf(t)}{\int_x^bg(t)}\right\vert\leq \left\vert\frac{\int_x^b\psi(t)g(t)}{\int_x^bg(t)}\right\vert\leq \varepsilon\left\vert\frac{\int_x^bg(t)}{\int_x^bg(t)}\right\vert\leq \varepsilon$ 
\par Dans le cas divergent :
\par Dans le cas où $f$ et $g$ sont continues par morceaux sur $]a,b[$, on reprend la même démonstration mais en prenant $x\in ]a, a+\alpha[$}
\begin{Rem}
Attention : dans le cas où $\varphi$ n'est pas intégrable, on ne sait rien sur l'intégrabilité de $f$ 
\end{Rem}
\begin{Rem}
On peut énoncer le même type de proposition pour des fonctions définies sur un intervalle semi-ouvert à gauche.
\end{Rem}

\section{Intégrale dépendant d'un paramètre}
\subsection{Cas d'un paramètre entier}
\Thr{de convergence dominée}{Soit $(f_n)$ une suite de fonctions continues par morceaux de $I$ intervalle de $\R$ dans $\K$. On suppose que :\begin{itemize}
\item La suite $(f_n)$ converge simplement sur $I$ vers une fonction $f$ continue par morceaux
\item Il existe une fonction $\varphi$ positive et intégrable sur $I$ telle que \par\begin{center}$\forall n\in\N, \vert f_n\vert\leq\varphi$ (hypothèse de domination)\end{center}
\end{itemize}
Alors les fonctions $f_n$ pour $n\in\N$ et la fonction $f$ sont intégrables sur $I$ et \par\begin{center}$\int_If_n\to\int_If$\end{center}}
\Pre{On admet la preuve car le théorème repose sur la théorie d'intégration de Lebesgue. \par On peut cependant le prouver dans le cas où $f_n$ converge uniformément vers $f$ sur tout segment de $[a,b[$ \par Pour tout $x\in[a,b[$, on a :\par $\left\vert\int_a^bf_n -\int_a^bf\right\vert\leq \int_a^x\vert f_n-f\vert + \int_x^b\vert f_n-f\vert\leq \vert x-a\vert\Vert f_n-f\Vert_\infty +\int_x^b(\varphi+\vert f\vert)$}
\begin{Rem}
On suppose que $f$ est continue par morceaux pour rester dans le cadre du programme où les intégrales sont définies uniquement pour de telles fonctions. Cette hypothèse n'a pas la même importance que l'hypothèse de domination qui est cruciale dans ce théorème.
\end{Rem}
\begin{Rem}
Si l'intervalle $I$ est borné, une fonction constante peut parfois jouer le rôle de la fonction de domination $\varphi$
\end{Rem}

\Thr{Intégration terme à terme positif}{Soit $I$ une intervalle \par Soit $(u_n)$ une suite de fonctions réelles positives continues par morceaux et intégrables sur $I$. On suppose que $\sum u_n$ converge simplement et que $\sum\limits_{n=0}^{+\infty}u_n$ est continue par morceaux sur $I$
Alors : \par \begin{center}$\int_I\sum\limits_{n=0}^{+\infty}u_n = \sum\limits_{n=0}^{+\infty}\int_I u_n$ \end{center}}
\begin{Rem}
Cette égalité a lieu dans $\bar{\R}$, c'est à dire que dans le cas d'une série de fonction de terme général positif $(u_n)$, $\sum\limits_{n=0}^{+\infty}u_n$ est intégrable sur $I$ si, et seulement si, $\sum\limits_{n=0}^{+\infty}\int_Iu_n<+\infty$
\end{Rem}
\Thr{Intégration terme à terme}{Soit $I$ un intervalle \par Soit $(u_n)$ une suite de fonctions à valeur dans $\K$. On suppose que :\begin{itemize}
\item Pour tout entier $n\in\N$, $u_n$ est continue par morceaux et intégrable sur $I$
\item La série $\sum u_n$ converge simplement et $\sum\limits_{n=0}^{+\infty}u_n$ est continue par morceaux sur $I$
\item La série $\sum\int_I\vert u_n\vert$ converge
\end{itemize}
Alors $\sum\limits_{n=0}^{+\infty}u_n$ est intégrable sur $I$ et \par \begin{center}$\int_I\sum\limits_{n=0}^{+\infty}u_n=\sum\limits_{n=0}^{+\infty}\int_Iu_n$\end{center}}
\Pre{Ce théorème est encore une fois admis.}
\begin{Rem}
Là encore, on suppose que $f$ est continue par morceaux pour rester dans le cadre du programme. Cette hypothèse n'a pas la même importance que l'hypothèse de convergence de $\sum\int_I\vert u_n\vert$ qui est cruciale dans ce théorème.
\end{Rem}
\begin{Rem}
Si chaque $u_n$ est continue et que l'on ne connait pas d'expression simple de $\sum\limits_{n=0}^{+\infty}u_n$, on peut parfois s'assurer que c'est une fonction continue (et donc continue par morceaux) par convergence uniforme/normale sur tout segment.
\end{Rem}

\subsection{Cas d'un paramètre réel}
\Thr{échange des limites non-discrètes}{Soient $I$, $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$. Soit $\lambda_0$ dans l'adhérence de $J$ ($\in\bar{R}$). On suppose que :\begin{itemize}
\item pour tout $\lambda\in J$, la fonction $t\mapsto f(\lambda,t)$ est continue par morceaux sur $I$
\item il existe une fonction $l$ continue par morceaux de $I$ dans $\K$ telle que pour tout $t\in I, \lim_{\lambda\to \lambda_0}f(\lambda,t)=l(t)$
\item Il existe une fonction $\varphi$ continue par morceaux positive et intégrable sur $I$ telle que : \par \begin{center}$\forall (\lambda, t)\in J\times I, \vert f(\lambda, t)\vert\leq\varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
Alors les fonctions $t\mapsto f(\lambda, t)$ (pour tout $\lambda\in J$) et la fonction $l$ sont intégrables sur $I$ et : \par \begin{center}$\lim_{\lambda\to\lambda_0}\int_If(\lambda, t)dt = \int_I l(t)dt$ \end{center}}
\Pre{Pour une fonction $f$ dans les conditions de l'énoncé, dire que que $f\to_{\lambda_0}l$ équivaut à $\forall (u_n)\in J^\N, (u_n)\to \lambda_0 \Rightarrow f(u_n, t)\to l(t)$
\par Soit $(u_n)\in J^\N$ de limite $\lambda_0$. On note  tout $n\in\N$ et $t\in I$, $f_n = f(u_n,t)$ \par $\forall n\in\N, f_n\in\mathcal{CM}(I, \K)$ \par $\forall t\in I, f_n(t)\to_{n\to+\infty} l(t)$ et il existe par hypothèse $\varphi$ intégrable sur $I$ telle que $\forall t\in I, \forall n\in\N, \vert l(t)\vert\leq \varphi(t)$ \par Donc par théorème de convergence dominée discret : \par \begin{center} $\int_I f(u_n,t)\to\int_Il(t)dt$\end{center}
\par Donc par caractérisation séquentielle de la convergence : \par \begin{center} $\int_If(\lambda, t)\to_{\lambda\to\lambda_0}\int_Il(t)dt$\end{center}}
\Thr{Autre formulation}{Soit $I$ et $J$ deux intervalles de $\R$, $(f_\lambda)_{\lambda\in J}$ une famille de fonctionns définie sur $J$ dans $\K$. Soit $\lambda_0$ dans l'adhérence de $J$ (dans $\bar{\R}$). On suppose que :\begin{itemize}
\item pour tout $\lambda\in J$, la fonction $f_\lambda$ est continue par morceaux sur $I$
\item il existe une fonction $l$ continue par morceaux de $I$ dans $\K$ telle que pour tout $t\in I$, $\lim\limits_{\lambda\to\lambda_0}f_\lambda(t)=l(t)$
\item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que \par \begin{center}$\forall (\lambda, t)\in J\times I, \vert f_\lambda(t)\vert\leq \varphi(t)$ (hypothèse de domination)\end{center}
\end{itemize}
Alors les fonctions $f_\lambda$ (pour $\lambda\in J$) et $l$ sont intégrables sur $I$ et \par \begin{center} $\lim\limits_{\lambda\to\lambda_0}\int_If_\lambda=\int_Il$\end{center}}


\Thr{Continuité dominée }{Soit $A$ une partie d'un EVN de dimension finie, $I$ un intervalle de $\R$, $f$ une fonction définie sur $A\times I$ à valeurs dans $\R$. On suppose que :\begin{itemize}
\item pour tout $x\in A$, la fonction $t\mapsto f(x,t)$ est continue par morceaux sur $I$
\item pour tout $t\in I$, la fonction $x\mapsto f(x,t)$ est continue sur $A$
\item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que \par \begin{center}$\forall (x,t)\in A\times I, \vert f(x,t)\vert\leq \varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
Alors, pour tout $x\in A$, la fonction $t\mapsto f(x,t)$ est intégrable sur $I$ et la fonction \par \begin{center} $g:\left\{\begin{array}{rcl}A & \to & \K \\ x & \mapsto & \int_If(x,t)dt\end{array}\right.$\end{center} \par est continue.}
\Prop{Extension}{Si l'hypothèse de domination est satisfaite au voisinage d'un point $a$ de $A$, on peut en conclure la continuité de $x\mapsto \int_If(x,t)dt$ en $a$ \par Si $A$ est un intervalle de $\R$, et que l'hypothèse de domination est satisfaite sur tout segment de $A$, alors \par \begin{center} $g:\left\{\begin{array}{rcl}A&\to&\K\\x&\mapsto&\int_If(x,t)dt\end{array}\right.$ est continue\end{center}}

\subsection{Dérivabilité d'une intégrale à paramètres}
\Thr{Dérivabilité}{Soit $I$ et $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$. On suppose que :\begin{itemize}
\item pour tout $x\in J$, la fonction $t\mapsto f(x,t)$ est continue par morceaux et dérivable sur $I$
\item la fonction $f$ admet sur $J\times I$ une dérivée parielle par rapport à la première variable, $\dfrac{\partial f}{\partial x}$
\item la fonction $\dfrac{\partial f}{\partial x}$ verifie les hypothèses du théorème 36 :\begin{itemize}
    \item pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est continue par morceaux sur $I$
    \item pour tout $t\in I$, la fonction $x\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est continue sur $J$
    \item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que \par \begin{center}$\forall (x,t)\in J\times I, \left\vert\dfrac{\partial f}{\partial x}(x,t)\right\vert\leq\varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
\end{itemize}
Alors pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est intégrable sur $I$, \par la fonction $g:x\mapsto \int_If(x,t)dt$ est de classe $\mathcal{C}^1$ sur $J$ et vérifie : \par\begin{center}$\forall x\in J, g'(x)=\int_I\dfrac{\partial f}{\partial x}(x,t)dt$\end{center}}
\Prop{Extension}{Si l'hypothèse de domination de $\dfrac{\partial f}{\partial x}$ est satisfaite sur tout segment de $J$, la conclusion du théorème subsiste.}
\begin{Rem}
Comme toujours, l'hypothèse de domination est essentielle.
\end{Rem}
\Thr{Classe d'une intégrale à paramètre}{Soit $I$ et $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$ et $k\in\N^*$. On suppose que :\begin{itemize}
\item pour tout $j\in\llbracket 0,k\rrbracket$, la fonction $f$ admet sur $J\times I$ une dérivée partielle d'ordre $j$ par rapport à la première variable, $\dfrac{\partial^j f}{\partial x^j}$
\item pour tout $j\in\llbracket 0,k-1\rrbracket$, pour tout $x\in J$, la fonction $\dfrac{\partial^j f}{\partial x^j}(x,t)$ est continue par morceaux et intégrable sur $I$
\item pour tout $t\in I$, pour tout $j\in\llbracket 0,k\rrbracket$, $x\mapsto \dfrac{\partial^j f}{\partial x^j}(x,t)$ est continue sur $J$
\item pour tout segment $K$ inclus dans $J$, il existe une fonction $\varphi_K$ continue par morceaux, positive et intégrable sur $I$ telle que \par \begin{center}$\forall (x,t)\in K\times I, \left\vert\dfrac{\partial^k f}{\partial x^k}(x,t)\right\vert\leq \varphi_K(t)$ (hypothèse de domination sur tout segment)\end{center}
\end{itemize}
Alors, pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial^kf}{\partial x^k}(x,t)$ est intégrable sur $I$, la fonction $g:x\mapsto\int_If(x,t)dt$ est de classe $\mathcal{C}^k$ sur $J$ et vérifie \par \begin{center}$\forall x\in J, g^{(k)}(x)=\int_I\dfrac{\partial^k f}{\partial x^k}(x,t)dt$\end{center}}


\chapter{Probabilités}
\section{Espace probabilisé}
\subsection{Univers, tribu}
\Def{}{Une expérience aléatoire est une expérience renouvelable, et qui renouvelée dans des conditions identiques ne donne pas le même résultat à chaque renouvellement.}
\Def{}{On appelle univers l'ensemble des issues possibles d'une expérience aléatoire donnée. On le note en général $\Omega$}
En première année, le programme ne traite que les expériences aléatoires associées à un univers fini, on peut poser \par\begin{center}$\Omega=\{\omega_1,...,\omega_n\}$\end{center}
\par Un événement est une partie de $\Omega$, c'est-à-dire un ensemble d'issues de l'expérience. L'ensemble des événements est donc $\mathcal{P}(\Omega)$
\par Pour tout $\omega\in\Omega$, le singleton $\{\omega\}$ est appelé événement élémentaire. On peut décrire tout événement comme réunion finie d'événements élémentaires.
\par On considère maintenant une expérience aléatoire ayant un nombre infini d'issues, donc avec un univers $\Omega$ infini.
\par Contrairement au cours de première année, tout sous-ensemble de $\Omega$ ne définit par forcément un événement.
\par On se limite alors aux parties d'un sous-ensemble $\mathcal{A}$ de $\mathcal{P}(\Omega)$.
\par Pour pouvoir définir une probabilité, on demande à $\mathcal{A}$ d'être une tribu, au sens de la définition suivante :
\Def{}{Soit $\Omega$ l'univers d'une expérience aléatoire.\par On appelle tribu sur $\Omega$ une partie de $\mathcal{A}$ vérifiant les trois hypothèses suivantes :\begin{enumerate}
\item $\Omega\in \mathcal{A}$
\item $\forall A\in \mathcal{A},\overline{A}\in \mathcal{A}$ (stabilité par passage au complémentaire)
\item Pour toute suite $(A_n)_{n\in\N}$ d'éléments de $\mathcal{A}$, $\bigcup_{n=0}^{+\infty}A_n\in\mathcal{A}$ (stabilité par réunion dénombrable)
\end{enumerate}
Le couple $(\Omega, \mathcal{A})$ est dit espace probabilisable.
\par $\mathcal{A}$ est l'ensemble des événements (on rappelle qu'un événement est une partie de $\Omega$)}
\begin{Rem}
$\mathcal{C}_\Omega\left(\bigcap_{n\in\N}A_n\right) = \bigcap_{n\in\N}\mathcal{C}_\Omega A_n$
\par Donc les intersections infinies sont possibles dans une tribu.
\end{Rem}
\begin{Rem}
La définition d'une tribu et la proposition s'énoncent en termes probabilistes :\begin{itemize}
\item $\Omega$ et $\emptyset$ sont des événements, ce sont les événements certains et impossibles.
\item L'événement contraire d'un événement est un événement.
\item Une réunion finie d'ou dénombrable d'événements est un événement.
\item Une intersection finie ou dénombrable d'événements est un événement.
\end{itemize}
\end{Rem}
\begin{Rem}
Soit $A$ et $B$ deux événements et $(A_n)_{n\in\N}$ une suite d'événements. Comme en sup, on peut traduire les opérations élémentaires sur les ensembles en termes probabilistes et vice-versa :
\par Insérer le tableau.
\end{Rem}
\Def{}{Soit $(A_n)_{n\in\N}$ une suite d'événements.\begin{itemize}
\item La suite $(A_n)$ est dite croissante lorsque \par\begin{center}$\forall n\in\N, A_n\subset A_{n+1}$\end{center} \par ie : pour tout $n\in\N$, la réalisation de $A_n$ implique celle de $A_{n+1}$
\item La suite $(A_n)$ est dite décroissante lorsque \par\begin{center}$\forall n\in\N, A_{n+1}\subset A_{n}$\end{center} \par ie : pour tout $n\in\N$, la réalisation de $A_{n+1}$ implique celle de $A_{n}$
\item La suite $(A_n)$ est une suite d'événement deux à deux incompatibles (disjoints) lorsque : \par\begin{center}$\forall i,j\in\N, i\neq j\Rightarrow A_i\cap A_j=\emptyset$\end{center} \par ie : il est impossible que deux événements d'indices différents de la suite soient réalisés simultanément
\end{itemize}}

\subsection{Probabilités}
\Def{}{On appelle probabilité sur l'espace probabilisable $(\Omega,\mathcal{A})$ toute application $P$ définie sur $\mathcal{A}$ vérifiant :\begin{enumerate}
\item $\forall A\in\mathcal{A}, P(A)\in [0,1]$
\item $P(\Omega)=1$
\item Pour tout famille dénombrable $(A_i)_{i\in I}$ d'événements deux-à-deux incompatibles : \par\begin{center}$P\left(\bigcup_{i\in I}A_i\right)=\sum\limits_{i\in I}P(A_i)$ ($\sigma$-additivité)\end{center}
\end{enumerate}}
Dans le cas d'un univers fini, la définition précédente est compatible avec celle donnée en sup, on étend les propriétés suivantes au cas d'un univers infini :
\Prop{}{Soit $(\Omega,\mathcal{A})$ un espace probabilisé, et soit $A$ et $B$ deux événements. On a :\begin{itemize}
\item $P(\overline{A})=1-P(A)$
\item $P(\emptyset)=0$ (cas particulier du résultat précédent)
\item $P(A\backslash B)=P(A)-P(A\cap B)$
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ (formule de Poincaré)
\item P est croissante : Si $A\subset B$ alors $P(A)\leq P(B)$
\end{itemize}}
\Pre{Toutes ces démonstrations fonctionnent exactement de la même manière sur des univers finis.
\par $P(\Omega)=P(A\cup\bar{A})=P(A)+P(\bar{A})=1$
\par D'où $P(\bar{A})=1-P(A)$
\par Donc, comme $\bar{\Omega}=\emptyset$, on a $P(\emptyset)=0$
\par $A\cup B=(A\backslash B)\cup(A\cap B)\cup (B\backslash A)$, donc $P(A\cup B) = P(A)+P(B)-P(A\cap B)$
\par Si $A\subset B$, comme $B=A\cup(B\backslash A)$ alors $P(B) = P(A)+P(B\backslash A)\geq P(A)$}
\begin{Rem}\begin{enumerate}
\item Une probabilité est positive. Toutes le sommes qui interviennent s'inscrivent dans le cadre des familles sommables positives. Elles peuvent donc se calculer en identifiant une bijection entre l'ensemble sur lequel on somme et $\N$ ou par théorème de sommation par paquets positifs.
\item Pour $n\in\N$, en appliquant la $\sigma$-additivité à la suite $(A_n)_{n\in\N}$ dont tous les termes sont vides à partir du rang $n+1$, on retrouve la $\sigma$-additivité finie de sup :
\par Pour une famille $(A_1,...,A_n)$ d'événements deux à deux incompatibles : $P\left(\bigcup_{k=1}^nA_k\right) =\sum\limits_{k=1}^nP(A_k)$
\item Dans le cas d'une suite d'événements deux à deux incompatibles $(A_i)_{i\in I}$, on a :
\par \begin{center} $\forall i\in I, P(A_i)\geq 0$ et $\bigcup_{i\in I}A_i\subset\Omega$ donc $\sum\limits_{i\in I}P(A_i)\leq P(\Omega)\leq 1$\end{center}
\par Ce qui assure la sommabilité de la famille $(P(A_i))_{i\in I}$ dans la définition d'une probabilité donnée ci-dessus.
\end{enumerate}\end{Rem}
\Thr{}{Soit $(\Omega, \mathcal{A},P)$ un espace probabilisé.\begin{enumerate}
\item Pour toute suite croissante $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ : \par\begin{center}$P\left(\bigcup_{n=0}^{+\infty}A_n\right)=\lim\limits_{n\to+\infty}P(A_n)$ (Continuité croissante)\end{center}
\item Pour toute suite décroissante $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ : \par\begin{center}$P\left(\bigcap_{n=0}^{+\infty}A_n\right)=\lim\limits_{n\to+\infty}P(A_n)$(Continuité décroissante)\end{center}
\item Pour toute suite $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ :\par\begin{center}$P\left(\bigcup_{n=0}^{+\infty}A_n\right)\leq\sum\limits_{n=0}^{+\infty}P(A_n)$\end{center}
\end{enumerate}}
\Pre{Pour la continuité croissante : Si $(A_n)$ croissante, alors pour tout $n\in\N, A_n\subset A_{n+1}$
\par Pour un $n\in\N$ fixé, on a $P\left(\bigcup_{k=0}^nA_k\right)=P(A_n)$
\par Posons $C_0=A_0$. Pour $n\in\N^*$, posons $C_n=A_n\backslash A_{n-1}$
\par Pour tout $i,j\in\N$, on a : $i\neq j\Rightarrow C_i\cap C_j=\emptyset$
\par On va montrer que $\bigcup_{n\in\N}C_n =\bigcup_{n\in\N}A_n$
\par $\forall n\in\N, C_n\subset A_n$ et donc $\bigcup_{n\in\N}C_n\subset \bigcup_{n\in\N}A_n$
\par Réciproquement, soit $x\in\bigcup_{n\in\N}A_n$, on a alors $\{n\in\N\vert x\in A_n\}\neq\emptyset$
\par On prend $n_0 = \min\{n\in\N\vert x\in A_n\}$
\par Si $n_0=0$ alors $x\in A_0$ donc $x\in C_0$
\par Sinon, $x\in A_{n_0}$ et $x\notin A_{n_0-1}$ donc $x\in C_{n_0}$
\par Donc $\bigcup_{n\in\N}A_n\subset \bigcup_{n\in\N}C_n$
\par $P\left(\bigcup_{n\in\N}C_n\right) =\sum\limits_{n=0}^{+\infty}P(C_n) = P(C_0)+\sum\limits_{n=1}^{+\infty}P(A_n)-P(A_{n-1})=\lim\limits_{n\to+\infty}P(A_n)$
\par D'où le résultat.
\par Pour la continuité décroissante : On peut passer au complémentaire de la suite, et on fait sa réunion en se servant du théorème croissant.
\par $P\left(\overline{\bigcap_{n=0}^{+\infty}A_n}\right)= P\left(\bigcup_{n=0}^{+\infty}\overline{A_n}\right) =1-\lim\limits_{n\to+\infty}P(A_n) = 1-P\left(\bigcap_{n=0}^{+\infty}A_n\right)$
\par D'où le résultat.
\par Pour la sous-additivité : $(A_n)$ une suite quelconque d'éléments de la tribu $\mathcal{A}$
\par On pose $C_n=\bigcup_{k=0}^n$ suite croissante
\par Donc $P\left(\bigcup_{n\in\N}C_n\right) =\lim\limits_{n\to+\infty}P(C_n)$
\par Pour $n\in\N$, $P(C_n)=P(A_0)+\sum\limits_{k=0}^nP(C_k\backslash C_{k-1})\leq \sum\limits_{k=0}^nP(A_k)$
\par Donc par passage à la limite monotone : $\lim\limits_{n\to+\infty}P(C_n)\leq \sum\limits_{n\in\N}P(A_n)$}
\begin{Rem}
Pour appliquer les théorèmes précédents à une suite $(A_n)_{n\in\N}$ non nécessairement monotone, on peut considérer :\begin{enumerate}
\item La suite $\left(\bigcup_{k=0}^nA_k\right)_{n\in\N}$ qui est croissante.
\item La suite $\left(\bigcap_{k=0}^nA_k\right)_{n\in\N}$ qui est décroissante.
\end{enumerate}
On obtient alors :\begin{enumerate}
\item $P\left(\bigcup_{n=0}^{+\infty}A_n\right)=\lim\limits_{n\to+\infty}P\left(\bigcup_{k=0}^nA_k\right)$
\item $P\left(\bigcap_{n=0}^{+\infty}A_k\right) = \lim\limits_{n\to+\infty}P\left(\bigcap_{k=0}^nA_k\right)$
\end{enumerate}
\end{Rem}
\Def{}{Soit $(\Omega,\mathcal{A}, P)$ un espace probabilisé et $A$ un événement.\begin{itemize}
\item Si $A\neq\emptyset$ et $P(A)=0$, on dit que l'événement $A$ est négligeable ou quasi-impossible.
\item Si $A\neq\Omega$ et $P(A)=1$, on dit que l'événement $A$ est presque sûr ou quasi-certain.
\end{itemize}}

\subsection{Univers finis, équiprobabilité, dénombrement}
\Prop{}{On suppose qu'il existe $p\in\R_+^*$ tel que \par\begin{center}$\forall \omega\in \Omega, P(\{\omega\})=p$\end{center}
\par Alors :\begin{itemize}
\item $\Omega$ est un ensemble fini et \par\begin{center}$p=\frac{1}{\mathrm{Card}(\Omega)}$\end{center}
\item Pour tout événement $A$, \par\begin{center}$P(A)=\frac{\mathrm{Card}(A)}{\mathrm{Card}(\Omega)}$\end{center}
\end{itemize}}
\begin{Rem}
Cette formule est plus connue sous la forme : \par\begin{center}$P(A)=\frac{\text{nombre de cas favorables}}{\text{nombre de cas possibles}}$\end{center}
\par On dit qu'on est en situation d'équiprobabilité.
\par Il s'agit d'une des rares situatiosn où une description de $\Omega$ est importante puisqu'il faudra en dénombrer les éléments.
\end{Rem}
On se fixe un ensemble $E$ de cardinal $n\in\N^*$.
\par On veut compter des objets fabriqués à partir d'éléments de $E$, par exemple des prélèvements dans une urne. Il faut modéliser ces objets par des objets mathématiques adéquats. Les deux objets mathématiques rencontrés principalement en dénombrement sont les parties (ou coumbinaisons) et les listes (ou uplets, suites finies, familles).
\subsubsection{Compter des parties}
Une partie de cardinal $k\in\N$ de $E$ est une $k$-combinaison. On peut voir une $k$-combinaison comme un prélèvement simultané de $k$ éléments de $E$ ; donc sans tenir compte ni d'ordre de tirage, ni de répétition.
\Prop{Nombre de parties}{Soit $E$ un ensemble à $n$ éléments\begin{enumerate}
\item Le nombre de $k$-combinaisons de $E$ est \par\begin{center} $\binom{n}{k}=\frac{1}{k!}n(n-1)...(n-k+1)=\left\{\begin{array}{rl} \frac{n!}{k!(n-k)!} & \text{si $k\leq n$} \\ 0 & \text{sinon}\end{array}\right.$\end{center}
\item Le nombre de parties de $E$ est \par\begin{center}$\sum\limits_{k=0}^n\binom{n}{k}=2^n$\end{center}
\end{enumerate}}
\subsubsection{Compter des listes}
Soit $k\in\N^*$. L'ensemble des $k$-listes d'éléments de $E$ est le produit cartésien $E^k$. On distingue deux cas particuliers de listes :\begin{itemize}
\item Une $k$-liste sans répétition (ou $k$-arrangement) est un élément $(x_1,...,x_k)\in E^k$ où $x_i\neq x_j$ si $i\neq j$. On rencontre des $k$-listes sans répétition quand par exeple on modélise des tirages successifs sans remise.
\item Une permutation de $E$ est une $n$-liste sans répétition de l'ensemble $E$ (de cardinal $n$)
\par On peut voir aussi une permutation de $E$ comme une bijection de $\llbracket 1,n\rrbracket$ dans $E$ ou une façon de réordonner les éléments de $E$.
\end{itemize}
\Prop{Nombre de listes}{Soit $E$ un ensemble à $n$ éléments.\begin{enumerate}
\item Soient $E_1,..., E_k$ $k$ ensembles de cardinaux respectifs $n_1,...,n_k\in\N^*$\par\begin{center}$\mathrm{Card}(E_1\times E_2\times...\times E_k)=n_1n_2...n_k$\end{center}
\item Le nombre de $k$-listes sans répétitions d'éléments de $E$ est $A_n^k = n(n-1)...(n-k+1)$
\item Le nombre de permutations d'éléments de $E$ est $n!$
\end{enumerate}}

\subsection{Probabilité conditionnelle, indépendance}
Les définitions et les théorèmes concernant les probabilités conditionnelles dans un univers infini sont identiques à celles d'un univers fini étudiés en sup.
\Def{}{Soit $(\Omega, \mathcal{A},P)$ un espace probabilisé et $A$ un événement.
\par On peut définir la probabilité conditionnelle de $A$ sachant $B$ notée $P(A/B)$ ou $P_B(A)$ :\begin{itemize}
\item Si $B$ un événement de probabilité non-nulle : \par\begin{center}$P(A/B)=P_B(A)=\frac{P(A\cap B)}{P(B)}$\end{center}
\item Si $P(B)=0$ \par\begin{center} $P_B(A)=0$\end{center}
\end{itemize}}
\Prop{}{Soit $(\Omega, \mathcal{A},P)$ un espace probabilisé. Soit $B$ un événement de probabilité non-nulle.
\par $P_B$ est une probabilité sur $(\Omega, \mathcal{A})$ appelée probabilité conditionnelle sachant $B$.}
\Pre{$P_B$ est définie sur $\mathcal{A}$ et vérifie :
\par Soit $A\in\mathcal{A}$, on a $A\cap B\subset B$ et $P$ croissante donc $0\leq P(A\cap B)\leq P(B)$
\par Ce qui donne $P_B(A)=\frac{P(A\cap B)}{P(B)}\in[0,1]$
\par On a $P_B(\Omega) =\frac{P(\Omega\cap B)}{P(B)}=\frac{P(B)}{P(B)}=1$
\par Soient $A_1, A_2$ deux événements incompatibles :
\par $P_B(A_1\cup A_2)=\frac{P((A_1\cup A_2)\cap B)}{P(B)}=\frac{P((A_1\cap B)\cup (A_2\cap B))}{P(B)}$
\par Or $(A_1\cap B)\cap(A_2\cap B) = (A_1\cap A_2)\cap B=\emptyset\cap B=\emptyset$
\par Donc par additivité de $P$ :
\par $P_B(A_1\cup A_2)=\frac{P(A_1\cap B)+P(A_2\cap B)}{P(B)} = \frac{P(A_1\cap B)}{P(B)}+\frac{P(A_2\cap B)}{P(B)}=P_B(A_1)+P_B(A_2)$
\par La $\sigma$-additivité se déduit de cette propriété par récurrence.
\par Et donc on a bien que $P_B$ est une probabilité sur $(\Omega, \mathcal{A})$}
\begin{Rem}
Considérer la probabilité d'un événement sachant $B$, c'est considérer que $B$ est réalisé, c'est à dire limiter l'univers à $B$.
\end{Rem}
\Prop{}{Soient $A$ et $B$ deux événements d'un espace probabilisé $(\Omega, \mathcal{A}, P)$. Alors si $P(A)\neq 0$ et $P(B)\neq 0$ :
\par \begin{center} $P(A\cap B)=P(A)P_A(B)=P(B)P_B(A)$\end{center}}
\Thr{Formule des probabilités composées}{Soit $(A_n)_{n\in\N}$ une famille d'événements telle que pour tout entier $n$, $P\left(\bigcap_{i=1}^{n-1}A_i\right)\neq 0$. Alors, pour tout entier $n$ :
\par\begin{center} $P\left(\bigcap_{i=1}^nA_i\right)=P(A_1)P_{A_1}(A_2)...P_{A_1\cap A_2\cap...\cap A_{n-1}}(A_n)$\end{center}
\par\begin{center} $P\left(\bigcap_{n\in\N}A_n\right) = \lim\limits_{n\to+\infty}P(A_1)P_{A_1}(A_2)...P_{A_1\cap...\cap A_{n-1}}(A_n)$\end{center}}
\Pre{Si $P(A_1\cap...\cap A_{n-1})=0$, alors la formule dit que $0=0$, ce qui est vrai.
\par Supposons que $P(A_1\cap...\cap A_{n-1})\neq 0$
\par Remarquons que $A_1\cap A_2\cap...\cap A_{n-1}\subset A_1\cap A_2\cap...\cap A_{n-2}\subset...\subset A_1$
\par Donc par croissance de $P$, les événements qui conditionnent sont tous de probabilités non-nulles et on peut appliquer la formule usuelle d'une probabilité conditionnelle.
\par Procédons par récurrence sur $n$.
\par Si $n=1$: la formule s'écrit $P(A_1)=P(A_1)$, ce qui est vrai.
\par Si la formule est vraie pour $n$ événements : \par Soient $A_1,...,A_{n+1}$ $n+1$ événements. En appliquant l'hypothèse de récurrence aux $n$ événements $A_1\cap A_2, A_3,...,A_{n+1}$, on obtient :
\par $P(A_1\cap A_2\cap...\cap A_n\cap A_{n+1}) = P(A_1\cap A_2)P_{A_1\cap A_2}(A_3)....P_{A_1\cap A_2\cap...\cap A_n}(A_{n+1})$
\par Mais $P(A_1\cap A_2)=P(A_1)P_{A_1}(A_2)$
\par D'où la récurrence.}
Les définitions de l'indépendance et de l'indépendance mutuelle sont identiques à celles du cas d'un univers fini étudiées en sup, on y ajoute simplement le cas d'une famille infinie d'événements.
\Def{}{Soit $(\Omega,\mathcal{A},P)$ un espace probabilisé. Soient $A$ et $B$ deux événements. On dit que $A$ et $B$ sont indépendants lorsque :
\par \begin{center}$P(A\cap B)=P(A)P(B)$\end{center}}
\begin{Rem}\begin{itemize}
\item Si $P(A)=0$ ou $P(B)=0$, $A$ et $B$ sont indépendants.
\item Il s'agit de ne pas confondre les notions d'événements indépendants et incompatibles. Si $A$ et $B$ sont de probabilités non-nulles et disjoints, alors ils ne sont pas indépendants, puisque $P(A)P(B)$ est non-nul alors que $P(A\cap B)$ l'est (intuitivement, $A$ et $\overline{A}$ ne sont pas indépendants.)
\item Si $P(B)\neq 0$, $A$ et $B$ sont indépendants si, et seulement si, $P_B(A)=P(A)$ \par ie : la réalisation de $A$ est indépendante de la réalisation de $B$
\item La notion formelle d'indépendance recouvre donc la notion intuitive, et dans de nombreux exercices, l'indépendance des événements proposés ira de soi, elle fera a priori parite des hypothèses (même si certains énoncés l'oublient, il faut alors le préciser). Par exemple, lorsque l'on lance une pièce de monnaie équilibrée $5$ fois de suite, l'indépendance des événements "il y a deux fois pile exactement parmi les trois premiers lancers" et "il y a une fois face exactement parmi les deux derniers lancers" est parfaitement intuitive, on peut la vérifier facilement par le calcul
\item La notion d'indépendance formelle est plus large que la notion intuitive. Dans certains cas, l'indépendance formelle peut sembler paradoxale, seule la vérification de $P(A)P(B)=P(A\cap B)$ peut prouver l'indépendance des événements $A$ et $B$.
\end{itemize}
\end{Rem}
\Def{}{Soit $(\Omega, \mathcal{A}, P)$ un espace probabilsié. Soit $(A_i)_{i\in I}$ une famille au plus dénombrable d'événements\begin{itemize}
\item Les événements $(A_i)_{i\in I}$ sont indépendants deux à deux si :\par\begin{center}$\forall i,j\in I, i\neq j\Rightarrow P(A_i\cap A_j)=P(A_i)P(A_j)$\end{center}
\item Les événements $(A_i)_{i\in I}$ soont indépendants ou mutuellement indépendants si pour toute famille finie $J\subset I$ on a :\par\begin{center}$P\left(\bigcap_{i\in J}A_i\right)=\prod\limits_{i\in J}P(A_i)$\end{center}
\end{itemize}}
\begin{Rem}\begin{itemize}
\item La notion de mutuelle indépendance est plus forte que la notion d'indépendance deux à deux. Des événements mutuellement indépendants sont deux à deux indépendants, mais la réciproque est fausse.
\item Si $A$ et $B$ sont indépendants alors $A$ et $\overline{B}$ le sont aussi.
\item Généralisation : Si $(A_i)_{i\in I}$ est une famille d'événements mutuellement indépendants alors les événements de toute famille $(B_i)_{i\in I}$ telle que pour tout $i\in I$, $B_i\in\{A_i, \overline{A_i}\}$ sont mutuellement indépendants.
\end{itemize}
\end{Rem}
\Pre{Si $A,B$ indépendants :
\par $A=A\cap(B\cup\overline{B})=(A\cap B)\cup (A\cap\overline{B})$, et l'union est disjointe
\par $P(A\cap\overline{B})=P(A)-P(A\cap B)$
\par $P(A)-P(A)P(B)=P(A)(1-P(B))=P(A)P(\overline{B})$
\par $I$ dénombrable, $(A_i)_{i\in I}$ famille d'événements mutuellement indépendants
\par Alors la famille $(B_i)_{i\in I}$ qui vaut $A_i$ ou $\overline{A_i}$ est faites d'événements mutuellement indépendants. Le résultat se prouve par récurrence sur le nombre de complémentaires.}

\subsubsection{Système complet d'événements, quasi-complet d'événements}
La formule des probabilités composées permet de gérer les enchaînements de phénomènes aléatoires. On s'intéresse maintenant à la modélisation d'un phénomène aléatoire qui peut être la conséquence de plusieurs causes.
\par La notion de système complet d'événements et la formule des probabilités totales ont été vues en sup dans le cas d'un nombre fini d'événements. Généralisons à des systèmes complets d'événements infinis.
\Def{}{Un système complet d'événements est une famille $(A_i)_{i\in I}$ au plus dénombrable d'événements tels que :\begin{itemize}
\item les événements sont deux à deux incompatibles ($\forall i,j\in I, i\neq j\Rightarrow A_i\cap A_j=\emptyset$)
\item leur union est l'univers tout entier ($\bigcup_{i\in I}A_i=\Omega$)
\end{itemize}}
\begin{Rem}
Autrement dit, $(A_i)_{i\in I}$ est un système complet d'événement si, et seulement si, à chaque réalisation de l'expérience aléatoire, un et un seul des événements $A_i$ est réalisé.
\par De cette définition, il sort qu'un événement et son contraire forment un système complet.
\end{Rem}
\Thr{}{Soit $(\Omega, A, P)$ un espace probabilisé. Pour tout système complet d'événements $(A_i)_{i\in I}$, on a :
\par \begin{center}$\sum\limits_{i\in I}P(A_i)=1$\end{center}}
\Def{}{Un système quasi-complet d'événements est uen famille $(A_i)_{i\in I}$ au plus dénombrable d'événements tels que :\begin{itemize}
\item les événements sont incompatibles deux à deux ($\forall i,j\in I, i\neq j\Rightarrow A_i\cap A_j=\emptyset$)
\item leur union est presque sûre : $P\left(\bigcup_{i\in I}A_i\right)=1$
\end{itemize}}

\subsubsection{Formule des probabilités totales} Cette formule permet de gérer les disjonctions de cas (les causes $(A_i)$) qui aboutissent à la réalisation d'un événement (la conséquence $B$)
\Thr{Formule des probabilités totales}{Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé. Soit $(A_i)_{i\in I}$ un système complet ou quasi-complet d'événements.
\par Pour tout événement $B$ :
\par\begin{center}$P(B)=\sum\limits_{i\in I}P(A_i\cap B)=\sum\limits_{i\in I}P(A_i)P_{A_i}(B)$\end{center}}
\Pre{La preuve est contenue dans l'énoncé ou presque
\par Pour $B\in\mathcal{A}$, on a :
\par $B=B\cap\Omega=B\cap\left(\bigcup_{i\in I}A_i\right)=\bigcup_{i\in I}B\cap A_i$
\par Et donc $P(B)=\sum\limits_{i\in I}P(B\cap A_i)$
\par Pour la dernière égalité, on rappelle juste la convention que si $P(A_i)=0$, alors $P_{A_i}(B)=0$}
\begin{Rem}
La formule reste valable pour un système complet d'événements fini, le cas le plus fréquent d'utilisation étant $(A, \overline{A})$
\end{Rem}

\subsubsection{Formule de Bayes} Sachant que la conséquence $B$ est réalisée, on veut maintenant connaître la probabilité d'une des causes.
\Thr{Formule d'inversion de Bayes}{Soit $(A_i)_{i\in I}$ un système complet ou quasi-complet d'événements d'un espace probabilisé $(\Omega, \mathcal{A},P)$. Soit $B$ un événement de probabilité non-nulle, soit $i_0\in I$.
\par Alors \par\begin{center}$P_B(A_{i_0})=\frac{P(B\cap A_{i_0})}{P(B)}=\frac{P(A_{i_0})P_{A_{i_0}}(B)}{\sum\limits_{i\in I}P(A_i)P_{A_i}(B)}$\end{center}}
\Pre{Pour un événement $B$ quelconque de probabilité non-nulle, et la formule fonctionne immédiatement en utilisant les propriétés vues plus tôt sur les probabilités conditionnelles : $P(A\cap B)=P(A)P_A(B)=P(B)P_B(A)$ et la formule des probabilités totales au dénominateur.}

\section{Variables aléatoires discrètes}
\subsection{Définition d'une variable aléatoire discrète}
Deux exemples de sup : on lance simultanément deux dés discernables et on choisit comme univers $\llbracket 1,6\rrbracket^2$, que l'on munit de la propbabilité uniforme. Notons $X$ la somme des valeurs des dés et $Y$ le maximum des deux valeurs.\begin{itemize}
\item $X$ peut prendre toutes les valeurs entières entre $2$ et $12$, $Y$ entre $1$ et $6$.
\item On peut voir $X$ et $Y$ comme des fonctions de $\Omega$ dans $\R$ : \par\begin{center}$X:(\omega_1,\omega_2)\mapsto \omega_1+\omega_2$ et $Y:(\omega_1,\omega_2)\mapsto \max(\omega_1,\omega_2)$\end{center}
\par On peut alors écrire $X(\Omega) = \llbracket 2,12\rrbracket$ et $Y(\Omega)=\llbracket 1,6\rrbracket$
\item Notons $A$ l'événement "la somme des 2 dés vaut $5$" : $A=\{(1,4), (2,3), (3,2), (4,1)\}$ et donc \par\begin{center}$P(A)=P(X=5)=\frac{4}{36}=\frac{1}{9}$\end{center}\par Dans l'égalité précédente, on a noté $(X=5)$ l'événement $A$. Il convient de voir $(X=5)$ comme une abbréviation pratique de $X^{-1}(\{5\})$.
\item Notons $B$ l'événement "la somme des 2 dés est inférieure à $4$ : $B=\{(1,1), (2,1), (3,1), (2,2), (1,3)\}$ et donc \par\begin{center}$P(B)=P(X\leq 4)=\frac{6}{36}=\frac{1}{6}$\end{center} \par Dans l'égalité précédente, on a noté $(X\leq 4)$ l'événement $B$. Il convient de voir $(X\leq 4)$ comme une abbréviation pratique de $X^{-1}(]-\infty, 4[)$
\end{itemize}
\Def{}{Soit $E$ un ensemble et $(\Omega, \mathcal{A})$ un espace probabilisable.
\par Une application $X:\to E$ est une variable aléatoire discrète si :\begin{itemize}
\item L'ensemble $X(\Omega)$ des valeurs prises par $X$ est au plus dénombrable.
\item Pour tout $x\in X(\Omega)$, l'ensemble $X^{-1}(\{x\})$, noté $(X=x)$ ou $[X=x]$, est un événement (un élément de $\mathcal{A}$)
\end{itemize}
Lorsque $E=\R$, la variable $X$ est dite réelle.}
Si $U$ est une partie de $E$, on note :
\par\begin{center}$(X\in U)=\{\omega\in\Omega\vert X(\omega)\in U\}=X^{-1}(U)$\end{center}
\par On note aussi $[X\in U]$ ou $\{X\in U\}$
\Thr{}{Si $X$ est une variable aléatoire discrète sur l'espace probabilisable $(\Omega, \mathcal{A})$, alors la suite $((X=x))_{x\in X(\Omega)}$ est un système complet d'événements de $\Omega$}
\Pre{Pour $U\in X(\Omega)$, $X\in U = \bigcup_{x\in U}(X=x)$. \par D'où le théorème.}

\subsection{Loi d'une variable aléatoire discrète}
\Def{}{Soit $X$ une variable aléatoire discrète sur un espace probabilisé, à valeurs dans un ensemble $E$
\par La loi $P_X$ de $X$ est la donnée de :\begin{itemize}
\item l'ensemble des valeurs prises par $X$ appelé univers image : $X(\Omega)$
\item les probabilités élémentaires : $p_x = P(X=x)$ pour tout $x\in X(\Omega)$
\end{itemize}}
\begin{Rem}
On note $X\sim Y$ lorsque les variables $X$ et $Y$ suivent la même loi ($P_X=P_Y$)
\end{Rem}
\Thr{}{Si $X$ une variable aléatoire discrète à valeurs dans un ensemble $E$, \begin{itemize}
\item la famille $(P(X=x))_{x\in E}$ est sommable de somme $1$ : \par\begin{center}$\sum\limits_{x\in E}P(X=x)=1$\end{center}
\item par $\sigma$-additivité, pour toute partie $U$ de $X(\Omega)$, \par\begin{center}$P(X\in U)=P\left(\bigcup_{x\in U}(X=x)\right)=\sum\limits_{x\in U}P(X=x)$ \end{center}
\item $(X(\Omega), \mathcal{P}(X(\Omega)), P_X)$ est un espace probabilisé.
\end{itemize}
Réciproquement, si $(p_x)_{x\in E}$ est une famille sommable de réels positifs de somme $1$ alors il existe une variable aléatoir discrète $X$ telle que pour tout $x\in E$, $P(X=x)=p_x$}
\Pre{$\Omega = \bigcup_{x\in X(\Omega)}(X=x)$ dénombrable, et alors $P(\Omega)=1=\sum\limits_{x\in X(\Omega)}P(X=x)$
\par Si $(U_n)_{n\in\N}\in \mathcal{P}(X(\Omega))^\N$ disjoints deux à deux :
\par $$ P_X(\bigcup_{n\in\N}U_n)=P(X^{-1}(\bigcup_{n\in\N}U_n)) $$
\par $$ = P(\bigcup_{n\in\N}(X^{-1}(U_n))) $$
\par $$ = \sum\limits_{n\in\N} P(X^{-1}(U_n))$$
\par $$ = \sum\limits_{n\in\N} P_X(U_n)$$
\par Pour vérifier que $(X(\Omega), \mathcal{P}(X(\Omega)), P_X)$ est un espace probabilisé, on se sert du fait que $X(\Omega)$ est dénombrable.
}


\Prop{}{Soient $X$ une variable aléatoire discrète à valeurs dans un ensemble $E$ et $f$ une fonction de $E$ dans un ensemble $F$ définie sur $X(\Omega)$
\par La fonction $Y=f\circ X :\left\{\begin{array}{rcl}\Omega & \to & F \\ \omega & \mapsto & f(X(\omega))\end{array}\right.$}

\Prop{}{Soit $X$ une variable aléatoire discrète et $Y = f(X)$ où $f:X(\Omega)\to F$
\par On a :\par\begin{center}$\forall y\in Y(\Omega), P(Y=y) = \sum\limits_{x\in X(\Omega), f(x)=y}P(X=x)$\end{center}}

\subsection{Indépendance et variables aléatoires discrètes}
\Def{}{$X$ et $Y$ sont deux variables aléatoires discrètes indépendantes lorsque pour toutes parties $A$ et $B$ de $X(\Omega)$ et $Y(\Omega)$ respectivement :
\par\begin{center}$P((X\in A)\cap (Y\in B))=P(X\in A)P(Y\in B)$\end{center}
\par On note $X\ind Y$}
\Thr{}{Deux variables aléatoires discrètes $X$ et $Y$ sont dites indépendantes si, et seulement si,
\par\begin{center}$\forall (x,y)\in X(\Omega)\times Y(\Omega), P((X=x)\cap (Y=y))=P(X=x)P(Y=y)$\end{center}
\par ie : $\forall (x,y)\in X(\Omega)\times Y(\Omega)$, les événements $(X=x)$ et $(Y=y)$ sont indépendants.}
\Pre{Ce théorème est admis.
\par Démontrons-le quand même.
\par L'indépendance implique la propriété.
\par Supposons maintenant : $\forall x\in X(\Omega), \forall y\in Y(\Omega), P(X=x\cap Y=y)=P(X=x)P(Y=y)$
\par Soit $A\subset\mathcal{P}(X(\Omega))$ et $B\subset \mathcal{P}(Y(\Omega))$
\par Alors $P(X\in A\cap Y\in B) = P\left((\bigcup_{a\in A}(X=a))\cap (\bigcup_{b\in B}(Y=b))\right)$
\par $ = P\left(\bigcup_{a\in A, b\in B}(X=a)\cap(Y=b) \right) = \sum\limits_{a\in A, b\in B}P((X=a)\cap (Y=b)) = \sum\limits_{a\in A, b\in B}P(X=a)P(Y=b)$
\par $ = \left(\sum\limits_{a\in A}P(X=a)\right)\left(\sum\limits_{b\in B}P(Y=b)\right)$
\par $ =P(X\in A)P(Y\in B)$}
\Def{}{Les variables aléatoires discrètes $X_1,X_2,...,X_n$ sont mutuellement indépendantes lorsque pour toutes parties $A_1,A_2,...,A_n$ de $X_1(\Omega),X_2(\Omega),...,X_n(\Omega)$ :
\par\begin{center}$P\left(\bigcap_{i=1}^n(X_i\in A_i)\right)=\prod\limits_{i=1}^nP(X_i\in A)$ \end{center}}
\Thr{}{Les variables aléatoires discrètes $X_1,X_2,...,X_n$ sont (mutuellement) indépendantes si, et seulement si, pour tout $n$-uplet $(x_1,x_2,...,x_n)$ de $X_1(\Omega)\times X_2(\Omega)\times...\times X_n(\Omega)$ :
\par\begin{center}$P\left(\bigcap_{i=1}^n(X_i=x_i)\right)=\prod\limits_{i=1}^nP(X_i=x_i)$\end{center}}
\Pre{Ce théorème est aussi admis.
\par Si on ne l'admettait pas, on devrait faire une récurrence avec la propriété vérifiée précédemment.}
\Def{}{Une suite de variables aléatoires discrètes $(X_n)_{n\in\N}$ est une suite de variables aléatoires discrètes (mutuellement) indépendantes lorsque pour toute partie finie $I = \{i_1,...,i_n\}$ de $\N$, les variables $X_{i_1}, X_{i_2},..., X_{i_n}$ sont (mutuellement) indépendantes.}
\Prop{}{Soient $X$ et $Y$ deux variables aléatoires discrètes définies sur $\Omega$ à valeurs dans $E$.
\par Soient $f$ et $g$ deux applications de $E$ dans $F$ définies respectivement sur $X(\Omega)$ et $Y(\Omega)$
\par Si $X$ et $Y$ sont indépendantes alors $f(X)$ et $g(Y)$ le sont aussi.}
\Prop{}{Soient $X_1,..., X_n$ $n$ variables aléatoires discrètes définies sur $\Omega$ à valeurs dans $E$.
\par Soient $f_1,...,f_n$ $n$ applications de $E$ dans $F$.
\par Si $X_1,...,X_n$ sont (mutuellement) indépendantes alors $f_1(X_1),..., f_n(X_n)$ le sont aussi.}
\Pre{On admet le premier résultat. Le second en découle par récurrence.}
\Prop{Lemme des coalitions}{Soient $X_1,X_2,...,X_n$ $n$ variables aléatoires discrètes définies sur $\Omega$ à valeurs dans $E$.
\par Soient $f$ et $g$ deux applications respectivement de $E^m$ dans $F$ et de $E^{n-m}$ dans $F$.
\par Si $X_1,X_2,..., X_n$ sont (mutuellement) indépendantes alors $f(X_1,..., X_m)$ et $g(X_{m+1}, X_n)$ le sont aussi.
\par Ce théorème s'étend à plus de deux coalitions.}
\Pre{Admis}


\subsection{Espérance d'une variable aléatoire discrète réelle ou complexe}
Rappel de sup : dans le cas où $\Omega$ est fini et $\mathcal{A}=\mathcal{P}(\Omega)$, la moyenne des valeurs prises par $X$ sur $\Omega$ est :
\par\begin{center}$m=\frac{\sum\limits_{\omega\in\Omega}X(\omega)}{\mathrm{Card}(\Omega)}$\end{center}
\begin{itemize}
\item Si $P$ est la propabilité uniforme sur $\Omega$, $\forall\omega\in\Omega, P(\{\omega\})=\frac{1}{\mathrm{Card}(\Omega)}$ et $m = \sum\limits_{\omega\in\Omega}X(\omega)P(\{\omega\})$
\item Si $P$ est une probabilité quelconque, $\sum\limits_{\omega\in\Omega}X(\omega)P(\{\omega\})$ est la moyenne des valeurs de $X(\omega)$ pondérée par la probabilité de $\omega$. Cette moyenne est appelée espérance de $X$, notée $E(X)$
\end{itemize}
En regroupant les valeurs de $\omega$ pour lesquelles $X$ prend une valeur fixée, on obtient une autre expression de $E(X)$ :
\par $E(X) =\sum\limits_{\omega\in\Omega}X(\omega)P(\{\omega\}) = \sum\limits_{x\in X(\Omega)} x\sum\limits_{\omega\in X^{-1}(\{x\})}P(\{\omega\}) = \sum\limits_{x\in X(\Omega)}xP(X^{-1}(X))=\sum\limits_{x\in X(\Omega)}xP(X=x)$
C'est cette dernière expression qui est utilisée pour définir l'espérance dans le cas général.
\Def{}{Soit $X$ une variable aléatoire discrète à valeurs dans $[0, +\infty]$.
\par\begin{center}$E(X)=\sum\limits_{x\in X(\Omega)}xP(X=x)$\end{center}
\par Avec la convention $xP(X=x)=0$ lorsque $X=+\infty$ et $P(X=+\infty)=0$}
\Def{}{Soit $X$ un evariable aléatoire discrète à valeurs réelles ou complexes.
\par On dit que $X$ admet une espérance (ou est d'espérance finie) lorsque la famillle $(xP(X=x))_{x\in X(\Omega)}$ est sommable. Dans ce cas :
\par\begin{center} $E(X)=\sum\limits_{x\in X(\Omega)}xP(X=x)$\end{center}
\par $X$ est dite centrée lorsque $E(X)=0$}
\begin{Rem}
La définition de l'espérance s'étend sans difficulté aux variables aléatoires à valeurs dans un espace vectoriel de dimension finie, en passant par les variables aléatoires coordonnées. L'expression
\par \begin{center} $E(X)=\sum\limits_{x\in X(\Omega)}P(X=x)x$\end{center}
\par reste valable (où $x$ est un vecteur, $P(X=x)$ un scalaire)
\end{Rem}
\Prop{}{Soit $X$ une variable aléatoire discrète à valeurs dans $\N\cup\{+\infty\}$. On a :
\par $$E(X)=\sum\limits_{n\in\N}P(X\geq n)$$}
\Pre{$$E(X)= \sum\limits_{n\in\N}nP(X=n)$$
\par $$ =\sum\limits_{n\in\N}\sum\limits_{k=0}^{n-1}P(X=n)$$
\par $$ =\sum\limits_{0\leq k<n, (k,n)\in\N^2}P(X=n)$$
\par $$=\sum\limits_{k\in\N}\sum\limits_{n<k}P(X=n)$$
\par $$=\sum\limits_{k\in\N}P(X>k)$$
\par $$=\sum\limits_{k\in\N^*}P(X\geq k)$$}
Calculer l'espérance d'une variable aléatoire réelle à partir de sa définition nécessite donc de connaître sa loi, ce qui peut être délicat lorsque la variable est le la forme $f(X)$
Le théorème suivant permet de calculer l'espérance de $f(X)$ à partir de la loi de $X$ :
\Thr{Transfert}{Soit $X$ une variable aléatoire discrète à valeurs réelles ou complexes et $f:X(\Omega)\to\C$
\par La variable aléatoire réelle $f(X)$ est d'espérance finie si, et seulement si, la famille $(f(x)P(X=x))_{x\in X(\Omega)}$ est sommable.
\par Dans ce cas :
\par\begin{center}$E(f(X))=\sum\limits_{x\in X(\Omega)}f(x)P(X=x)$\end{center}}
\begin{Rem}
On admet aussi que la formule de transfert s'applique aux couples, au $n$-uplets de variables aléatoires.
\par Par exemple pour deux variables aléatoires discrètes $X$ et $Y$ à valeurs réelles ou complexes et la fonction $f:(x,y)\mapsto xy$ :\begin{itemize}
\item $XY$ est d'expérance finie si, et seulement si, la famille $(f(x,y)P((X,Y)=(x,y)))_{(x,y)\in X(\Omega)\times Y(\Omega)}$ est sommable.
\item Dans le cas sommable : \par\begin{center}$E(XY)=\sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}f(x,y)P((X,Y)=(x,y))=\sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}xyP(X=x, Y=y)$\end{center}
\end{itemize}
\end{Rem}
\Thr{Propriétés de l'espérance}{Soient $X,Y$ deux variables aléatoires discrètes à valeurs réelles ou complexes et d'espérance finie.\begin{itemize}
\item $\forall \lambda, \mu\in C, \lambda X+\mu Y$ est d'espérance finie et \par\begin{center}$E(\lambda X+\mu Y)=\lambda E(X)+\mu E(Y)$ (linéarité)\end{center} \par On peut généraliser cette propriété avec $n$ variables aléatoires réelles discrètes d'espérance finie, qu'elles soient indépendantes ou non.
\par Dans la suite, $X$ et $Y$ sont à valeurs réelles.
\item Si $X\geq 0$ alors $E(X)\geq 0$ (Positivité)
\item Si $X\geq 0$ et $E(X)=0$ alors $(X=0)$ est presque sûr (stricte positivité)
\item Si $X\leq Y$ alors $E(X)\leq E(Y)$ (croissance)
\end{itemize}}
\Pre{Linéarité de l'espérance : Si $X$ et $Y$ admettent une espérance :
\par $X+Y$ a une espérance si, et seulement si, $\sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}\vert x+y\vert P(X=x, Y=y)<+\infty$
\par $\sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)} \vert x+y\vert P(X=x, Y=y) \leq \sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}(\vert x\vert + \vert y\vert)P(X=x, Y=y)$
\par $$\leq \sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}\vert x\vert P(X=x) + \sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}\vert y\vert P(Y=y)$$
\par $$\leq \sum\limits_{x\in X(\Omega)}\vert x\vert\sum\limits_{x\in X(\Omega)}P(X=x, Y=y) + \sum\limits_{y\in Y(\Omega)}P(X=x, Y=y)$$
\par Or $(X=x) = \bigcup_{y\in Y(\Omega)} (X=x)\cap (Y=y)$ et réciproquement.
\par D'où la majoration du terme précédent :
\par $$\leq \sum_{x\in X(\Omega)}\vert x\vert P(X=x)+\sum\limits_{y\in Y(\Omega)}\vert y\vert P(Y=y) < + \infty$$
\par Avec la sommabilité, en se débarrassant de la valeur absolue du début, on remplace les inégalités larges par des égalités, et on a que $E(X+Y)=E(X)+E(Y)$.
\par Se fait de même pour la multiplication par un scalaire.
\par Pour la stricte positivité :
\par $E(X)=0\Rightarrow (X=0)$ presque sûrement
\par $E(X) = \sum\limits_{x\in X(\Omega)}xP(X=x) = 0$
\par Comme $xP(X=x)\geq 0$, alors $\forall x\in X(\Omega), x=0$ ou $P(X=x)=0$
\par Donc $P(X\neq 0) = \sum\limits_{x\in X(\Omega), \neq 0}xP(X=x)=0$
}

\begin{Rem}
Dans le cadre du théorème précédent : \begin{itemize}
\item On a en particulier que $\forall \lambda,\mu \in\C,\lambda X+\mu$ admet une espérance et $E(\lambda X + \mu)=\lambda E(X)+\mu$
\item Pour $X$ réelle et $a,b\in \R$, si $a\leq X\leq b$ alors $a\leq E(X)\leq b$
\end{itemize}
\end{Rem}
\Thr{Critère de majoration positif}{Soit $X$ une variable aléatoire discrète à valeurs réelles ou complexes. Soit $Y$ une variable aléatoire discrète à valeurs réelles.
\par Si on a :\begin{itemize}
\item $Y$ est d'espérance finie
\item $\vert X\vert\leq Y$
\end{itemize}
Alors $X$ est d'espérance finie.}
\Thr{Espérance d'un produit de variables indépendantes}{Soit $X,Y$ deux variables aléatoires discrètes à valeurs réelles ou complexes.
\par Si on a :\begin{itemize}
\item $X$ et $Y$ d'espérance finie
\par ($X$ et $Y$ admettent une espérance serait plus adapté)
\item $X$ et $Y$ indépendantes
\end{itemize}
Alors $XY$ est d'espérance finie et
\par $$E(XY)=E(X)E(Y)$$}
\Pre{$E(\vert XY\vert) = \sum\limits_{(x,y)\in X(\Omega)\times Y(\Omega)}\vert xy\vert P(X=x, Y=y)$
\par $ = \sum\limits_{x,y}\vert xy\vert P(x=x)P(Y=y) = \sum\limits_{x\in X(\Omega)}\vert x\vert P(X=x) \sum\limits_{y\in Y(\Omega)}\vert y\vert P(Y=y)$
\par $=E(\vert X\vert)E(\vert Y\vert)<+\infty$ par hypothèse
\par On a donc que $(xyP(X=x)P(Y=y))_{(x,y)\in X(\Omega)\times Y(\Omega)}$ est sommable donc par sommation par paquets :
\par $E(XY)=\sum\limits_{x\in X(\Omega)}xP(X=x)\sum\limits_{y\in Y(\Omega)}yP(Y=y) = E(X)E(Y)$}

\Thr{Espérance d'un produit de n variables indépendantes}{Soit $X_1,...,X_n$ des variables aléatoires discrètes à valeurs réelles ou complexes.
\par Si on a :\begin{itemize}
\item $X_1,..., X_n$ d'espérance finie
\item $X_1,..., X_n$ (mutuellement) indépendantes
\end{itemize}
Alors la variable $X_1...X_n$ est d'espérance finie et \par\begin{center}$E\left(\prod\limits_{i=1}^nX_i\right)=\prod\limits_{i=1}^nE(X_i)$\end{center}}
Pre{$H_n$ : $\forall X_1,..., X_n$ variables aléatoires complexes mutuellement indépendantes et posssédant une espérance, alors $X_1...X_n$ possède une espérance et $E(X_1...X_n)=E(X_1)...E(X_n)$
\par On a vu $H_2$ dans la preuve précédente
\par On suppose $H_n$, prenons $X_1,..., X_{n+1}$ complexes mutuellement indépendantes
\par Par $H_n$, $(X_2...X_{n+1})$ admet une espérance et $E(X_2...X_{n+1})=E(X_2)...E(X_{n+1})$
\par Par lemme des coalitions, $X_1$ et $X_2...X_{n+1}$ sont indépendantes
\par Par $H_2$, $X_1(X_2...X_{n+1})$ admet une espérance et :
\par $$E(X_1(X_2...X_{n+1}))=E(X_1)E(X_2...X_{n+1}) = \prod\limits_{i=1}^nE(X_i)$$}
\begin{Rem}
La réciproque de cette proposition est fausse, l'égalité $E(XY)=E(X)E(Y)$ ne suffit pas à garantir l'indépendance des variables $X$ et $Y$.
\par Par exemple, $X$ la variable aléatoire de loi uniforme sur $\{-1,0,1\}$ et $Y=X^2$
\par On peut vérifier que $E(XY)=E(X)E(Y)$ mais $P((X,Y)=(0,0))=\frac{1}{3}$ tandis que $P(X=0)P(Y=0) = \frac{1}{9}$
\end{Rem}

\subsection{Variance d'une variable aléatoire discrète réelle, écart type}
\Def{}{$X$ une variable aléatoire complexe, on dit que $X$ admet un moment d'ordre $k$ pour $k\in\N^*$ si $E(X^k)$ existe}
\Thr{}{Si $X$ admet un moment d'ordre $k+1$ alors $X$ admet un moment d'ordre $k$.}
\Pre{On suppose que $X$ admet un moment d'ordre $k+1$ (ie $E(X^{k+1}) < +\infty$)
\par Alors $E(\vert X^k\vert) = \sum\limits_{x\in X(\Omega)}\vert x\vert^kP(X=x)$
\par $$ E(\vert x\vert^k)\leq \sum\limits_{x\in X(\Omega), \vert x\vert \geq 1} \vert x\vert^kP(X=x) +\sum\limits_{x\in X(\Omega), \vert x\vert<1} \vert x\vert^kP(X=x)$$
\par $$ \leq E(\vert X\vert^{k+1})+1$$}

\Prop{}{Soit $X$ une variable aléatoire discrète à valeurs réelles.
\par Si $X^2$ est d'espérance finie alors, pour tout polynôme $f$ de degré au plus 2, la variable aléatoire $f(X)$ est d'espérance finie.
\par Par exemple, $X$, $X(X-1)$ sont d'espérance finie.}
\Pre{On suppose que $X^2$ est d'espérance finie.
\par Par linéarité, la variable $Y=\frac{1}{2}(X^2-1)$ est aussi d'espérance finie.
\par De plus, on a $\vert X\vert\leq Y$, donc par critère de majoration positif, $X$ est d'espérance finie.
\par On pose $f:x\mapsto a_0+a_1x+a_2x^2$ où $a_0,a_1,a_2\in\R$
\par D'après ce qui précède, $X^2$, $X$ et la variable constante égale à $1$ sont d'espérance finie, donc par linéarité $f(X)=(a_0+a_1X+a_2X^2)$ est d'espérance finie.}
\Def{Variance et ecart-type}{Soit $X$ une variable aléatoire discrète à valeurs réelles.
\par Si $X^2$ est d'espérance finie, on définit la variance de $X$ par \par\begin{center}$V(X)=E((X-E(X))^2)$\end{center}
\par et son écart-type par \par\begin{center}$\sigma(X)=\sqrt{V(X)}$\end{center}}
\begin{Rem}
La définition de la variance a un sens, car si $X^2$ est d'espérance finie, $m=E(X)$ existe et $(X-m)^2= X^2-2mX + m^2$ qui est un polynôme du second degré en $X$ d'espérance finie.
\par La définition de l'écart-type a aussi un sens, car par positivité de l'espérance, la variance est un réel positif.
\end{Rem}
\Prop{de la variance}{Soit $X$ une variable aléatoire discrète à valeurs réeles.
\par On suppose que $X^2$ est d'espérance finie.\begin{enumerate}
\item Köning-Huygens : formule pratique pour la variance :\par\begin{center}$V(X)=E(X^2)-E(X)^2$\end{center}
\item Soient $a,b\in\R$ \par\begin{center}$V(aX+B)=a^2V(X)$\end{center} \par En particulier, la variance est invariante par translation
\item $V(X)$ est nulle si, et seulement si, $P(X=E(X))=1$ \par ie : $X$ est presque sûrement constante.
\end{enumerate}}
\Pre{Pour le premier point :
\par On a $V(X)=E((X-E(X))^2)=E(X^2-2E(X)X+E(X)^2)= E(X^2)-2E(X)E(X)+E(X)^2 = E(X^2)-E(X)^2$
\par Pour le deuxième point :
\par On a $V(aX+b) = E((aX+b-E(aX+b))^2)=E((aX+b-aE(X)-b)^2)=E(a^2(X-E(X))^2)=a^2V(X)$}
\Pre{Pour le troisième point : si $V(X)=0$
\par On pose $m=E(X)$, d'après le théorème de transfert : $V(X)=E((X-E(X))^2)=E((X-m)^2)=\sum\limits_{n=0}^{+\infty}(x_n-m)^2P(X=x_n)$
\par Donc $\forall n\in\N, 0\leq (x_n-m)^2P(X=x_n)\leq\sum\limits_{n=0}^{+\infty}(x_n-m)^2P(X=x_n)=0$
\par Et donc $\forall n\in\N, (x_n-m)^2P(X=x_n)=0$
\par Or il existe $n_0$ tel que $P(X=x_{n_0})\neq 0$ (sinon la somme des probabilités ne vaudrait pas $1$)
\par Donc pour un tel $n_0$, $x_{n_0}=m$, car sinon $(x_{n_0}-m)^2P(X=x_{n_0})\neq 0$
\par Comme les $x_n$ sont deux à deux distincts : $\forall n\in\N\backslash\{n_0\}, x_n\neq m$
\par D'où $\forall n\in\N\backslash\{n_0\}, P(X=x_n)=0$
\par Comme $((X=x_n))_{n\in\N}$ est un système complet d'événements : $P(X=x_{n_0})=1-\sum\limits_{n\in\N, n\neq n_0}P(X=x_n)=1$
\par D'où $P(X=x_{n_0})=P(X=m)=P(X=E(X))=1$
\par Réciproquement, on suppose que $P(X=E(X))=1$
\par Comme $P(X=E(X))\neq 0$, $E(X)\in X(\Omega)$, et il existe alors $n_0\in\N$ tel que $x_{n_0}=E(X)$
\par On a que pour $n\in\N\backslash\{n_0\}, (X=x_n)\subset\overline{(X=x_{n_0})}$, et par croissance de $P$ : \par $0\leq P(X=x_n)\leq P(\overline{(X=x_{n_0})})=1-P(X=x_{n_0})=1-1=0$
\par Donc $\forall n\in\N\backslash\{n_0\}, P(X=x_n)=0$
\par On en déduit par théorème de transfert : \par $V(X)=E((X-E(X))^2) = \sum\limits_{n=0}^{+\infty}(x_n-E(X))^2P(X=x_n)=0$}
\Def{}{Soit $X$ une variable aléatoire discrète à valeurs réelles telle que $X^2$ est d'espérance finie et telle que $\sigma(X)>0$
\par La variable $\frac{X}{\sigma(X)}$ a un écart-type égal à $1$. Elle est appelée réduite de $X$.
\par La viariable $\frac{X-E(X)}{\sigma(X)}$ a une espérance nulle et un écart-type égal à $1$. Elle est appelée variable centrée réduite associée à $X$.}

\subsection{Covariance, coefficent de corrélation linéaire}
Dans ce paragraphe, $X$ et $Y$ sont deux variables aléatoires discrètes à valeurs réelles.
\Prop{Inégalité de Cauchy-Schwarz}{Si $X^2$ et $Y^2$ sont d'espérance finie, alors $XY$ l'est aussi et \par\begin{center}$E(XY)^2\leq E(X^2)E(Y^2)$\end{center}
\par Avec égalité si, et seulement si, $P(X=0)=1$ ou il existe $\lambda\in\R$ tel que $P(Y=\lambda X)=1$}
\Prop{}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors, pour toute fonction $f:\R^2\to \R$ polynomiale de degré au plus $2$, la variable aléatoire $f(X,Y)$ est d'espérance finie.}
\Def{}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors on peut définir la covariance de $X$ et $Y$ par :\par\begin{center}$\mathrm{cov}(X,Y)=E((X-E(X))(Y-E(Y)))$\end{center}}
\Pre{Il suffit de démontrer que $\sum\vert xy\vert P(X=x, Y=y)$ est d'espérance finie.
\par $\forall x\in X(\Omega), \forall y\in Y(\Omega), \vert xy\vert \leq\frac{1}{2}\left(\vert x\vert^2 + \vert y\vert^2\right)$
\par Donc $E(\vert XY\vert)<+\infty$ donc $E(XY)$ admet une espérance
\par De plus $X$ admet une variance donc une espérance. De même pour $Y$.
\par Donc $(X-E(X))(Y-E(Y))$ admet une espérance et :
\par $$ E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y)-E(X)E(Y)+ E(X)E(Y)$$
\par $$ \mathrm{cov}(X,Y)=E(XY) - E(X)E(Y)$$}
\begin{Rem}
La définition de la covariance a un sens, car si $X^2$ et $Y^2$ sont d'espérance finie, $m=E(X)$ et $m'=E(Y)$ existent et $(X-m)(Y-m')=XY-m'X-mY+mm'$ qui est un polynôme du second degré en $X$ et $Y$, et donc d'espérance finie.
\par La covariance est l'espérance du produit des variables centrées.
\end{Rem}

\begin{Rem}
La covariance est une forme bilinéaire symétrique positive pour l'espace vectoriel des variables aléatoires réelles.
\par On dispose donc de l'inégalité de Cauchy-Schwarz.
\par Si $E$ est un $\R$-ev et $\varphi$ une forme bilinéaire symétrique sur $E$, alors :
\par $$\forall x,y\in E, \vert \varphi(x,y)\vert\leq \sqrt{\varphi(x,x)}\sqrt{\varphi(y,y)}$$
\par On fixe $(x,y)\in E$ et on considère : $P:\left\{\begin{array}{rcl} \R & \to & \R \\ t & \mapsto & \varphi(x+ty, x+ty)\end{array}\right.$
\par Par bilinéarité : $P(t) = \varphi(x,x) + 2t\varphi(x,y)+t^2\varphi(y,y)$
\par $P$ est une fonction polynomiale positive.
\par Si $\varphi(y,y)> 0$ : $P$ est polynomiale de degré 2, positive si son discriminant est négatif.
\par Or $P$ est positive par positivité de $\varphi$
\par Donc $\varphi(x,y)^2 - \varphi(x,x)\varphi(y,y)\leq 0$
\par Si $\varphi(y,y)=0$ : $P$ est affine. Comme $\varphi$ est positive, alors $P$ aussi. Donc $P$ est constante (affine et positive, elle vaut $\varphi(x,x)$ qui est alors positive et $\varphi(x,y)=0$).
\par Donc $\varphi(x,y)\leq \sqrt{\varphi(x,x)}\sqrt{\varphi(y,y)}$
\end{Rem}
\Def{}{Si $X$ et $Y$ possèdent des variances non-nulles, on peut définir la corrélation de $X$ et $Y$ :
\par $$\mathrm{corr}(X,Y) = \frac{\mathrm{cov}(X,Y)}{\sigma(X)\sigma(Y)}$$
\par D'après Cauchy-Schwarz, on a : $-1\leq\mathrm{corr}(X,Y)\leq 1$}

\Prop{de la covariance}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors :\begin{itemize}
\item \begin{center}$\mathrm{cov}(X,X)=V(X)$\end{center}
\item Köning-Huygens : formule pratique pour la covariance :\par\begin{center}$\mathrm{cov}(X,Y)=E(XY)-E(X)E(Y)$\end{center}
\item Si $X$ et $Y$ sont indépendantes alors \par\begin{center}$\mathrm{cov}(X,Y)=0$\end{center}
\item La covariance est une forme linéaire positive, symétrique et bilinéaire (c'est "presque" un produit scalaire)
\end{itemize}}
\Prop{Variance d'une somme}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors $(X+Y)^2$ aussi avec
\par\begin{center}$V(X+Y)=V(X)+2\mathrm{cov}(X,Y)+V(Y)$\end{center}
\par Si de plus les variables $X$ et $Y$ sont indépendantes :
\par\begin{center}$V(X+Y)=V(X)+V(Y)$\end{center}}
\Prop{Variance d'une somme}{Si $X_1,...,X_n$ sont $n$ variables telles que $X_1^2,..., X_n^2$ sont d'espérance finie alors $(X_1+...+X_n)^2$ l'est aussi avec :
\par\begin{center}$V\left(\sum\limits_{k=1}^nX_k\right)=\sum\limits_{k=1}^nV(X_k)+2\sum\limits_{1\leq i<j\leq n}\mathrm{cov}(X_i, X_j)$\end{center}
\par Si de plus les variables $X_1,...,X_n$ sont indépendantes \textbf{deux à deux} alors :
\par\begin{center}$V\left(\sum\limits_{k=1}^nX_k\right)=\sum\limits_{k=1}^nV(X_k)$\end{center}}
\Pre{$V(\sum\limits_{i=1}^n X_i) = \mathrm{cov}(\sum\limits_{i=1}^n X_i, \sum\limits_{j=1}^nX_j)$
\par $ = \sum\limits_{i=1}^n\mathrm{cov}(X_i, \sum\limits_{j=1}^nX_j) = \sum\limits_{i\leq n,j\leq n} \mathrm{cov}(X_i, X_j)$
\par $ = \sum\limits_{i=1}^nV(X_i) +2\sum\limits_{1\leq i<j\leq n}\mathrm{cov}(X_i, X_j)$}
La loi binomiale $\mathcal{B}(n,p)$ est lme nombre de succès pour une expérience aléatoire de probabilité de succès $p$ répétée $n$ fois, où les expériences sont indépendantes.
\par Si on appelle $X_i$ la variable aléatoire indicatrice du succès à la $i$-ème tentative, si $X\leadsto \mathcal{B}(n,p)$ alors $X=\sum\limits_{i=1}^n X_i$
\par Les $X_i$ sont indépendantes, $X_i\leadsto \mathcal{B}(1,p)$, $E(X_i) = p$, $E(X_i^2)=1$ et $V(X_i) = p - p^2 = p(1-p)$
\par $E(X) = np$ et $V(X)=np(1-p)$
\par La loi hypergéométrique $\mathcal{H}(N, n, p)$ : elle modélise le tirage sans remise de $n$ boules dans une urne contenant $N$ boules, donc $Np$ noires.
\par $X$ est le nombre de boules noires tirées. L'univers de $X$ est $\llbracket 0, n\rrbracket$.
\par Pour $k\in X(\Omega)$ : $P(X=k) = \frac{\binom{Np}{k}\binom{N(1-p)}{n-k}}{\binom{N}{n}}$
\par Notons $X_i$ la variable indicatrice du succès au rang $i$, $X_i\leadsto \mathcal{B}(1,p)$
\par On a $X=\sum\limits_{i=1}^n X_i$, et donc $E(X) = np$
\par Pour $i,j$, on a que : $P(X_i=1, X_j=1) = \frac{Np(Np-1)\binom{n-2}{n-2}(n-2)!}{\binom{N}{n}n!} = \frac{p(Np-1)}{N-1}$
\par D'où $\mathrm{cov}(X_i, X_j) = E(X_iX_j) - E(X_i)E(X_j) = \frac{p(Np-1)}{N-1} - p^2 = \frac{p(p-1)}{N-1}$
\par $V(X) = np(1-p) + n(n-1)\frac{p(p-1)}{N-1} = np(1-p)\left(1-\frac{n-1}{N-1}\right)$


\section{Lois usuelles}
\subsection{Lois usuelles finies}
Dans toute la suite on considère $n\in\N^*, p\in[0,1]$ et $q=1-p$
\Def{}{Une variable aléatoire discrète $X$ suit une loi de Bernoulli de paramètre $p$ si
\par\begin{center}$X(\Omega)=\{0,1\}$ et $P(X=1)=p$\end{center}
\par On note $X\sim\mathcal{B}(p)$}
\Prop{}{Si $X\sim\mathcal{B}(p)$ alors
\par\begin{center}$E(X)=P$ et $V(X)=pq$\end{center}
En particulier, le paramètre d'une loi de Bernouilli est son espérance.}
\Pre{On a $E(X) = 0 \times P(X=0) + 1\times P(X=1) = p$
\par Mais si $X(\Omega)\subset\{0,1\}$ alors $X^2=X$. Donc :
\par $V(X)=E(X^2)-E(X)^2= p - p^2 = p(1-p)=pq$}

\Def{}{Une variable aléatoire discrète $X$ suite une loi uniforme sur l'ensemble fini non-vide $K\subset \R$ si
\par\begin{center}$X(\Omega)=K$ et $\forall k\in K, P(X=k) = \frac{1}{\mathrm{Card}(K)}$\end{center}
On note $X\sim \mathcal{U}(K)$}
\Prop{}{Si $X\sim\mathcal{U}(\llbracket 1,n\rrbracket)$ alors
\par\begin{center}$E(X)=\frac{n+1}{2}$ et $V(X) = \frac{n^2-1}{12}$\end{center}}
\Pre{Pour l'espérance : $E(X) = \sum\limits_{i=1}^n n\frac{1}{n} = \frac{1}{n}\sum\limits_{i=1}^ni = \frac{1}{n}\frac{n(n+1)}{2} = \frac{n+1}{2}$
\par $E(X^2) = \sum\limits_{i=1}^ni^2\frac{1}{n} = \frac{1}{n} \frac{n(n+1)(2n+1)}{6} = \frac{(n+1)(2n+1)}{6}$
\par D'où $V(X) = \frac{(n+1)(2n+1)}{6}-\left(\frac{n+1}{2}\right)^2 = \frac{4n^2 + 2n + 4n + 2 - 3n^2 - 6n - 3}{12} = \frac{n^2-1}{12}$}


\Def{}{Une variable aléatoire discrète $X$ suit une loi binomiale des paramètres $n$ et $p$ si $X(\Omega)=\llbracket 0,n\rrbracket$ et
\par\begin{center}$\forall k\in \llbracket 0,n\rrbracket, P(X=k)=\binom{n}{k}p^kq^{n-k}$\end{center}
\par $X\sim \mathcal{B}(n,p)$}
\Prop{Modèle loi binomiale}{Si $X$ est le nombre de succès lors de la répétition de $n$ épreuves de Bernouilli indépendantes de même paramètre $p$ alors $X$ suit la loi $\mathcal{B}(n,p)$
\par Formellement :
\par On pose pour $k\in\llbracket 1,n\rrbracket$ $X_k$ la variable aléatoire discrète qui vaut $1$ si la $k$-ième épreuve est un euscès et $0$ sinon. Si :\begin{itemize}
\item pour tout $k\in\llbracket 1,n\rrbracket, X_k\sim\mathcal{B}(p)$
\item $X_1,X_2,..., X_n$ sont mutuellement indépendantes
\item $X=X_1+X_2+...+X_n$
\end{itemize}
Alors \par\begin{center}$X\sim \mathcal{B}(n,p)$\end{center}
\par En outre \par\begin{center}$E(X)=np$ et $V(x)=npq$\end{center}}
\Pre{$X$ est une somme de $n$ termes qui valent $0$ ou $1$, donc $X(\Omega)\subset \llbracket 0,n\rrbracket$
\par Soit $k\in\llbracket 0,n\rrbracket$
\par On a l'union d'événements disjoints deux à deux : $(X=k) =\bigcup_{I\subset \llbracket 1,n\rrbracket, \mathrm{Card}(I)=k} \bigcap_{i\in I}(X_i=1)\cap \bigcap_{i\in\overline{I}}(X_i=0)$
\par Donc, par additivité : $P(X=k) = \sum\limits_{I\subset \llbracket 1,n\rrbracket, \mathrm{Card}(I)=k}P\left(\bigcap_{i\in I}(X_i=1)\cap \bigcap_{i\in\overline{I}}(X_i=0)\right)$
\par Donc $P(X=k)=p^kq^{n-k} \sum\limits_{I\subset \llbracket 1,n\rrbracket, \mathrm{Card}(I)=k}1 = \binom{n}{k}p^kq^{n-k}$
\par Par linéarité de l'espérance on a donc $E(X) = E(X_1)+E(X_2)+...+E(X_n) = np$
\par Par indépendance mutuelle donc deux à deux des $X_i$ : $V(X) = V(X_1)+V(X_2)+...+V(X_n)=npq$}

\subsection{Lois usuelles infinies}
\Def{}{On suppose que $p\in]0,1[$
\par Une variable aléatoire discrète $X$ suit une loi géométrique de paramètre $p$ si $X(\Omega)=\N^*$ et
\par\begin{center}$\forall n\in\N^*, P(X=n)=pq^{n-1}$\end{center}
\par On note $X\sim\mathcal{G}(p)$}
\Prop{}{Si $X$ suite une loi géométrique de paramètre $p$ alors pour tout $k\in\N^*$
\par\begin{center}$P(X>k)=(1-p)^k$\end{center}}
\Prop{Espérance et variance d'une loi géométrique}{Si la variable aléatoire discrète $X$ suite une loi $\mathcal{G}(p)$ alors $X$ possède une espérance et une variance qui valent
\par\begin{center}$E(X)=\frac{1}{p}$ et $V(X)=\frac{q}{p^2}$\end{center}}
\Pre{Espérance : $p\sum\limits_{i=0}^{+\infty} npq^{n-1} = \left(\frac{p}{1-q}\right)'=\frac{1}{p}$
\par Variance : $ V(X) = E(X^2 - X)+E(X) (E(X))^2 = \frac{2(1-p)}{p^2}+\frac{1}{p} -\frac{1}{p^2} = \frac{1-p}{p^2} = \frac{q}{p^2}$}
\Prop{Modèle loi géométrique}{Si $X$ est le rand u premier succès dans une suite illimitée d'épreuves de Bernouilli indépendantes de même paramètre $p$ alors $X$ suit la loi $\mathcal{G}(p)$
\par Formellement :
\par On pose pour tout $k\in\N^*$ $X_k$ la variable aléatoire discrète qui vaut $1$ si la $k$-ième épreuve est un succés et $0$ sinon.
\par Si :\begin{itemize}
\item pour tout $k\in\N^*n, X_k\sim\mathcal{B}(p)$
\item la suite $(X_n)$ est une suite de variables mutuellement indépendantes
\item $X = \min\{n\in\N^*, X_n=1\}$
\end{itemize}
Alors $X\sim\mathcal{G}(p)$}
\begin{Exe}
L'exemple le plus classique est le teps d'attente d'un premier pile lors des lancers successifs d'une pièce qui donne pile avec probabilité $p$.
\end{Exe}

\Def{Loi de Poisson}{Une variable aléatoire discrète $X$ suit une loi de Poisson de paramètre $\lambda\in\R_+^*$ si $X(\Omega)=\N$ et
\par\begin{center}$\forall n\in\N, P(X=n)=e^{-\lambda}\frac{\lambda^n}{n!}$\end{center}
\par On note $X\sim \mathcal{P}(\lambda)$}
\Prop{Espérance et variance d'une loi de Poisson}{Si la variable aléatoire discrète $X$ suit une loi $\mathcal{P}(\lambda)$ alors $X$ possède une espérance et une variance et
\par\begin{center}$E(X)=V(X)=\lambda$\end{center}}
\Pre{$E(X) =\sum\limits_{n\in\N}ne^{-\lambda}\frac{\lambda^n}{n!} =e^{-\lambda}\sum\limits_{n\in\N^*}\frac{\lambda^{n}}{(n-1)!} = \lambda e^{-\lambda}\sum\limits_{k\in\N}\frac{\lambda^n}{n!}=\lambda$
\par $E(X^2-X) = \sum\limits_{n\in\N}n(n-1)e^{-\lambda}\frac{\lambda^n}{n!}=e^{-\lambda}\sum\limits_{k\geq 2}\frac{\lambda^{k}}{(k-2)!}=\lambda^2$
\par Donc $V(X) =  E(X^2 - X) + E(X) - E(X)^2 = \lambda^2 + \lambda - \lambda^2 =\lambda$}


\section{Fonctions génératrices}
Soit $X$ une variable aléatoire discrète à valeurs dans $\N$
\Def{}{On note $R_X$ le rayon de convergence de la série entière $\sum P(X=n)t^n$
\par La fonction génératrice d'une variable aléatoire $X$ à valeurs dans $\N$ est définie pour tout $t\in]-R_X,R_X[$ par
\par\begin{center}$G_X(t)=E(t^X)=\sum\limits_{n=0}^{+\infty}P(X=n)t^n$\end{center}}
\begin{Exe}
Fonction génératrice de $X\leadsto\mathcal{U}(\llbracket 1,n\rrbracket)$ :
\par $$ G_X:t\mapsto \frac{1}{n}\sum\limits_{k=1}^nt^k = \frac{1}{n}t\frac{1-t^n}{1-t}$$
\par Fonction génératrice de $X\leadsto \mathcal{B}(n,p)$ :
\par $$G_X:t\mapsto \sum\limits_{k=1}^n\binom{n}{k}p^k(1-p)^{n-k}t^k = (pt+1-p)^n$$
\par Fonction génératrice de $X\leadsto\mathcal{P}(\lambda)$ :
\par $$G_X:t\mapsto \sum\limits_{k=0}^{+\infty}e^{-\lambda}\frac{\lambda^nt^n}{n!} = e^{-\lambda + \lambda t}$$
\par Fonction génératrice de $X\leadsto \mathcal{G}(p)$ :
\par $$G_X:t\mapsto \sum\limits_{n=1}^{+\infty}(1-p)^{n-1}pt^n = \frac{pt}{(1-(1-p)t)}$$
\end{Exe}

\Prop{}{La série $\sum P(X=n)t^n$ converge pour $t=1$ donc le rayon de convergence $R_X$ de la série entière $\sum P(X=n)t^n$ est au moins égal à $1$}
\Pre{On a par définition que $\sum\limits_{k\in\N}P(X=k)=1$, donc le rayon est au moins $1$.}
\Prop{}{La série entière $P(X=n)t^n$ converge normalement sur $[-1,1]$ donc $G_X$ est continue sur $[-1,1]$}
\Pre{La convergence normale est assurée par la majoration de $\vert P(X=n)t^n\vert$ par $P(X=n)$ sur $[-1,1]$
\par La continuité découle de celle de $t\mapsto t^n$}
\Thr{}{Soit $X$ une variable aléatoire à valeurs dans $\N$.\begin{enumerate}
\item La loi de $X$ est entièrement caractérisée par la connaissance de sa fonction génératrice $G_X$
\par ie : $G_X:t\mapsto \sum\limits_{n=0}^{+\infty}a_nt^n$ est la fonction génératrice de $X$ si, et seulement si, $\forall n\in \N, P(X=n)=a_n$
\item $G_X(1)=1$
\item $X$ admet une espérance si, et seulement si, $G_X$ est dérivable en $1$, avec dans ce cas \par\begin{center}$E(x) =G_X'(1)$\end{center}
\item $X$ admet une variance si, et suelement si, $G_X$ est deux fois dérivable en $1$, avec dans ce cas :\par\begin{center}$E(X(X-1))=G_X''(1)$\end{center}
\par et donc \par\begin{center}$V(X)=G_X''(1)+G_X'(1)-(G_X'(1))^2$\end{center}
\end{enumerate}}
\Pre{La première propriété découle de l'unicité d'un DSE.
\par La deuxième propriété découle de la définition d'une probabilité.
\par Pour la troisième propriété :
\par Dans les hypothèses de l'énoncé, on suppose $E(X)<+\infty$ (ce qu'on peut écrire puisque $X$ est positive)
\par Alors la série $\sum nP(X=n)$ converge, et : $\forall t\in [-1,1], \vert nP(X=n)t^n\vert\leq nP(X=n)$
\par Donc :\begin{itemize}
\item $\forall n\in\N, t\mapsto P(X=n)t^n$ est $\mathcal{C}^1$
\item $\sum t\mapsto P(X=n)t^n$ converge simplement sur $[-1,1]$
\item $\sum t\mapsto nP(X=n)t^{n-1}$ converge normalement donc uniformément sur $[-1,1]$
\end{itemize}
\par Donc par théorème de dérivation des séries de fonctions, $G_X$ est dérivable sur $[-1,1]$ de dérivée $t\mapsto \sum\limits_{n=1}^{+\infty}nP(X=n)t^{n-1}$
\par Donc $G_X'(1)=E(X)$
\par Réciproquement : Procédons par contraposée, en supposant $E(X)=+\infty$ (possible comme $X$ positive)
\par $\forall t\in[0,1[, G_X'(t)=\sum\limits_{n=1}^{+\infty}nP(X=n)t^{n-1}$
\par $G_X'$ est croissante sur $[0,1[$, donc $G_X'$ a une limite finie ou infinie en $1^-$
\par Et comme $nP(X=n)t^{n-1}$ positive sur $[0,1[$ :
\par $$\forall N\in\N, G_X'(t)\geq \sum\limits_{n=1}^Nn(P(X=n))t^{n-1}$$
\par Passage à la limite quand $t\to 1^-$ :
\par $$\lim\limits_{t\to 1^-}G_X'(t) \geq \sum\limits_{n=1}^{+\infty}nP(X=n)$$
\par Or $\left(\sum\limits_{k=0}^N kP(X=k)\right)$ est la suite des soimmes partielles d'une série divergente de terme général positif.
\par Donc $\sum\limits_{n=1}^NnP(X=n)\to_{N\to+\infty}+\infty$
\par Donc $G_X'\to_{t\to 1^-}+\infty$
\par Donc $G_X$ est positive en $1$ et $G_X'\to_{t\to 1^-}+\infty$, et donc par théorème de prolongement $\mathcal{C}^1$, $G_X$ n'est pas dérivable en $1$.
\par La dernière propriété se prouve de même que la troisième, mais avec la dérivée seconde, en distancent que $V(X) = E(X^2-X) + E(X) - (E(X))^2 = G_X''(1)+G_X'(1)-G_X'(1)^1$.}
\begin{Rem}
$G_X=G_Y$ si, et seulement si, $X$ et $Y$ ont même loi.
\end{Rem}
\Thr{}{Si $X$ et $Y$ sont deux variables aléatoires indépendantes à valeurs dans $\N$ alors :\par\begin{center}$\forall t\in[-1,1], G_{X+Y}=G_XG_Y$\end{center}
\par Soit $n\in\N, n\geq 2$. Si $X_1,...,X_n$ sont des variables aléatoire réelles mutuellement indépendantes alors : \par\begin{center}$G_{X_1+...+X_n}=G_{X_1}...G_{X_n}$\end{center}}
\Pre{Soit $t\in[-1,1]$:
\par $G_{X+Y}(t)=\sum\limits_{n\in\N}P(X+Y=n)t^n =\sum\limits_{n\in\N}\sum\limits_{i+j=n}P(X=i,Y=j)t^n$
\par $= \sum\limits_{n\in\N}\sum\limits_{i+j=n}P(X=i)P(Y=j)t^n$ par indépendance
\par $=\left(\sum\limits_{i\in\N}P(X=i)t^i\right) \left(\sum\limits_{j\in\N}P(Y=j)t^j\right)$ par produit de Cauchy de deux séries absolument convergentes
\par $=G_X(t)G_Y(t)$
\par Pour l'extension du théorème, on se contente de faire une récurrence en se servant de la propriété avec deux variables.}


\section{Couples de variables aléatoires réelles}
\subsection{Loi conjointe, lois marginales}
\Def{}{Soient $X$ et $Y$ deux variables aléatoires discrètes réelles sur un même espace probabilisé $(\Omega, \mathcal{A},P)$.\begin{enumerate}
\item On appelle couple des variables $X$ et $Y$, et on note $Z=(X,Y)$ l'application \par\begin{center}$Z:\left\{\begin{array}{rcl}\Omega & \to & X(\Omega)\times Y(\Omega) \\ \omega & \mapsto & (X(\omega), Y(\omega)) \end{array}\right.$\end{center}
\item La loi du couple $Z=(X,Y)$ est appelée loi conjointe, elle est définir par :\begin{itemize}
    \item $Z(\Omega)=X(\Omega)\times Y(\Omega)$
    \item Les probabilités élémentaires $P(X=x, Y=y)=P((X=x)\cap(Y=y))$ pour tous $(x,y)\in X(\Omega)\times Y(\Omega)$
\end{itemize} On a bien sûr \par\begin{center}$\forall (x,y)\in X(\Omega)\times Y(\Omega), P(X=x,Y=y)\geq 0$ et $\sum\limits_{(x,y)\in X(\Omega)\times Y(\omega)}P(X=x,Y=y)=1$\end{center}
\item Les lois de $X$ et $Y$ sont appelées lois marginales du couple $Z=(X,Y)$
\par Si la loi conjointe du couple $Z=(X,Y)$ est connue, alors les lois marginales de $X$ et $Y$ le sont aussi :\begin{itemize}
    \item On détermine la loi de $X$ en appliquant la formule des probabilités totales avec le système complet d'événements $([Y=y])_{y\in Y(\Omega)}$ :
    \par\begin{center}$\forall x\in X(\Omega), P(X=x)=\sum\limits_{y\in Y(\Omega)}P(X=x, Y=y)$\end{center}
    \item On détermine la loi de $Y$ en appliquant la formule des probabilités totales avec le système complet d'événements $([X=x])_{x\in X(\Omega)}$ :
    \par\begin{center}$\forall y\in Y(\Omega), P(Y=y)=\sum\limits_{x\in X(\Omega)}P(X=x, Y=y)$\end{center}
\end{itemize} La réciproque est évidemment fausse, la connaissance des lois marginales de $X$ et $Y$ ne permet pas de déterminer la loi conjointe du couple $Z=(X,Y)$
\end{enumerate}}
\begin{Rem}
Dans le cas de variables aléatoires finies on peut noter : \par\begin{center}$X(\Omega)=\{x_1,x_2,...,x_r\}$ et $Y(\Omega)=\{y_1,y_2,...,y_s\}$\end{center}
\par Et notons pour alléger, si $i\in\llbracket 1,r\rrbracket$ et $j\in\llbracket 1,s\rrbracket$ :\par\begin{center} $p_i=P(X=x_i)$, $q_j=P(Y=y_j)$ et $p_{i,j}=P(X=x_i, Y=y_i)$\end{center}
\par On peut alors se représenter la loi du couple $(X,Y)$ et leurs lois marginales dans un tableau à double-entrée.
\end{Rem}
\begin{Rem}
On rappelle que $X$ et $Y$ sont indépendantes si, et seulement si,
\par\begin{center}$\forall (x,y)\in X(\Omega)\times Y(\Omega), P(X=x, Y=y)=P(X=x)P(Y=y)$\end{center}
\par Donc $X$ et $Y$ sont indépendantes si, et seulement si, tous les coefficients du tableau de la loi du couple sont le produit des probabilités de $X$ et $Y$ situées en marge du tableau.
\end{Rem}

\subsection{Lois conditionnelles d'un couple}
\Def{}{Soit $X$ une variable aléatoire réelle discrète sur $(\Omega,\mathcal{A}, P)$ et $A$ un événement de probabilité non-nulle. La loi conditionnelle de $X$ sachant $A$ est la donnée de :\begin{itemize}
\item $X(\Omega)$
\item $\forall x\in X(\Omega), P_A(X=x)$
\end{itemize}
Elle est notée $X_{/A}$}
\Def{}{Soit $Z=(X,Y)$ un couple de variables aléatoires réelles discrètes sur $(\Omega, \mathcal{A}, P)$
\par La loi de $Z$ est donnée par :\begin{enumerate}
\item Les lois de $X$ conditionnées par $Y$ sont les lois de $X$ conditionnées par les événements $[Y=y]$ pour tout $y\in Y(\Omega)$
\par Plus précisement, pour $y\in Y(\Omega)$ fixé tel que $P(Y=y)\neq 0$, la loi de $X$ sachant $[Y=y]$ est définie par :\begin{itemize}
    \item La donnée de $X(\Omega)$
    \item Les nombres $P_{Y=y}(X=x)=\frac{P(X=x,Y=y)}{P(Y=y)}$ pour tout $x\in X(\Omega)$
\end{itemize}
\item Les lois de $Y$ conditionnées par $X$ sont les lois de $Y$ conditionnées par les événements $[X=x]$ pour tout $x\in X(\Omega)$
\par Plus précisement, pour $x\in X(\Omega)$ fixé tel que $P(X=x)\neq 0$, la loi de $Y$ sachant $[X=x]$ est définie par :\begin{itemize}
    \item La donnée de $Y(\Omega)$
    \item Les nombres $P_{X=x}(Y=y)=\frac{P(X=x,Y=y)}{P(X=x)}$ pour tout $x\in X(\Omega)$
\end{itemize}
\end{enumerate}}
\begin{Rem}
Souvent, lorsque l'on étudie un couple de variables aléatoires réelles :\begin{enumerate}
\item On détermine la loi de $X$
\item Puis pour tout $x\in X(\Omega)$, on détermine la loi de $Y$ conditionnée par l'événement $X=x$
\item On déduit la loi conjointe du couple $Z=(X,Y)$ en utilisant que pour tout $x\in X(\Omega)$ et $y\in Y(\Omega)$ \par\begin{center}$P(X=x, Y=y)=P(X=x)P_{X=x}(Y=y)$\end{center}
\item Et enfin on détermine la loi de $Y$
\end{enumerate}
\end{Rem}


\section{Résultats probabilistes asymptotiques}
\subsection{Inégalités de Markov et Bienaymé-Tchebychev}
\Thr{Inégalité de Markov}{Soit $X$ une variable aléatoire réelle discrète positive d'espérance finie, on a:
\par\begin{center}$\forall a>0, P(X\geq a)\leq \frac{E(X)}{a}$\end{center}}
\Pre{Avec $a\in\R_+$ fixé :
\par $E(X) = \sum\limits_{x\in X(\Omega)}xP(X=x) \geq \sum\limits_{x\in X(\Omega),x\geq a}xP(X=x)$ (comme $X$ est positive)
\par $\geq a\sum\limits_{x\in X(\Omega), x\geq a} P(X=x)\geq aP(X\geq A)$
\par D'où l'inégalité $P(X\geq a)\leq \frac{E(X)}{a}$}


\Thr{Inégalité de Bieinaymé-Tchebychev}{Soit $X$ une variable aléatoire rélle discrète telle que $X^2$ est d'espérance finie, on a :
\par\begin{center}$\forall \varepsilon>0, P(\vert X-E(X)\vert\geq \varepsilon)\leq \frac{V(X)}{\varepsilon^2}$\end{center}
\par En passant à l'événement contraire :
\par\begin{center}$\forall\varepsilon>0, P(\vert X-E(X)\vert<\varepsilon)\geq 1 - \frac{V(X)}{\varepsilon^2}$\end{center}}
\Pre{Pour $\varepsilon>0$:
\par $E((X-E(X))^2) = \sum\limits_{x\in X(\Omega)}(x-E(X))^2P(X=x)$
\par $\geq \varepsilon^2\sum\limits_{x\in X(\Omega), \vert X-E(X)\vert\geq\varepsilon} P(X=x)$
\par $\geq\varepsilon^2P(\vert X - E(X)\vert\geq\varepsilon)$
\par D'où l'inégaité.}


\subsection{Loi faible des grands nombres}
\Thr{Loi faible des grands nombres}{Soit $(X_n)_{n\in\N^*}$ une suite de variables réelles indépendantes de même loi, de variance finie.
\par En notant $S_n=\sum\limits_{k=1}^nX_k, m=E(X_1)$ et $\sigma =\sigma(X_1)$, on a :\begin{enumerate}
\item \begin{center}$\forall \varepsilon>0, P\left(\left\vert\frac{S_n}{n}-m\right\vert\geq\varepsilon\right)\leq\frac{\sigma^2}{n\varepsilon^2}$\end{center} \par Cette inégalité doit êtr edémontrée à chaque utilisation d'après le programme
\item \begin{center}$\forall \varepsilon>0, P\left(\left\vert\frac{S_n}{n}-m\right\vert\geq\varepsilon\right)\to_{n\to+\infty} 0$\end{center}
\end{enumerate}}
\Pre{On a ici que $(frac{\S_n}{n}) = \frac{1}{n}\sum\limits_{i=1}^nE(X_i) = m$
\par Par indépendance, $V(S_n) = \frac{1}{n^2}\sum\limits_{i=1}^nV(X_i) = \frac{\sigma^2}{n}$

}
\begin{Rem}
La loi faible des grands nombres est utilisée lorsque l'on cherche à comparer la probabilité $p$ d'un événement $A$ et la fréquence d'apparition de cet événement quand l'on répète un grand nombre de fois l'expérience aléatoire associée à $A$
\par On suppose les expériences indépendantes.
\par Pour $k\in\N^*$, à la $k$-ième réalisation de l'expérience, on note $X_k$ la variable aléatoire réelle indicatrice de l'événement $A$. ($(X_k=1)$ si $A$ est réalisé lors de cette expérience, $(X_k=0)$ sinon)
\par Pour tout $k\in\N^*$, $X_k$ suit la loi de Bernouilli $\mathcal{B}(p)$, d'espérance $m=p$, d'écart-type $\sigma =\sqrt{p(1-p)}$
\par Pour tout $n\in\N^*$, $S_n=\sum\limits_{k=1}^nX_k$ est le nombre de fois où l'événement $A$ a été observé pendant les $n$ preières expériences et la fréquence d'apparition de $A$ lors ce ces $n$ expériences est :
\par\begin{center}$F_n = \frac{S_n}{n}$\end{center}
\par On a : \par\begin{center}$\forall \varepsilon>0, P(\vert F_n-p\vert\leq \varepsilon)=P\left(\left\vert\frac{S_n}{n}-m\right\vert\geq \varepsilon\right)\leq \frac{\sigma^2}{n\varepsilon^2}=\frac{p(1-p)}{n\varepsilon^2}$\end{center}
\par Et : \par\begin{center}$\forall \varepsilon>0, P(\vert F_n-p\vert\geq\varepsilon)\to_{n\to+\infty}0$\end{center}
\par D'où : \par\begin{center}$\forall \varepsilon>0, P(\vert F_n-p\vert <\varepsilon)\to_{n\to+\infty}1$\end{center}
\par L'événement "$F_n$ tend vers $p$ est quasi-certain.
\par Lorsque la valeur de $p$ ,'est pas connue, on majore souvent $\frac{p(1-p)}{n\varepsilon^2}$ par $\frac{1}{4n\varepsilon^2}$ en utilisant :
\par\begin{center}$\boxed{\forall p\in [0,1], 0\leq p(1-p)\leq \frac{1}{4}}$
\end{center}
\end{Rem}



\chapter{Espaces préhilbertiens}
Dans tout le cours, $E$ est un $\R$-ev.
\section{Préliminaires}
\Def{}{Soit $E$ un $\R$-ev, on dit qu'une application $\varphi:E\times E\to\R$ est :\begin{itemize}
\item une forme bilinéaire si : $\forall x\in E, y\mapsto \varphi(x,y)=\varphi(x, .)$ est linéaire et $\varphi(., x)$ est linéaire.
\item symétrique si : $$\forall x,y\in E, \varphi(x,y)=\varphi(y,x)$$
\item positive si : $$\forall x\in E, \varphi(x, x)\geq 0$$
\item définie si : $$\forall x\in E, \varphi(x, x)=0\Rightarrow x=0$$
\end{itemize}}
\Def{Produit scalaire}{Une forme bilinéaire symétrique définie positive est un produit scalaire.}
\begin{Rem}
L'expression matricielle du produit scalaire, avec $X, Y\in\mathcal{M}_{n,1}(\R)$ est $X^ATY$ avec $A$ la matrice d'une forme bilinéaire symétrique. (ses coefficients sont les $\varphi(e_i, e_j)$)
\par Comme $\varphi$ est symétrique, $\varphi(e_i, e_j)=\varphi(e_j, e_i)$ donc $A$ est une matrice symétrique.
\end{Rem}
\Prop{Identités polaires}{$\varphi$ une forme bilinéaire symétrique.
\par $$\forall x, y\in E, \varphi(x, y) =\frac{1}{2}(\varphi(x+y, x+y) - \varphi(x, x) - \varphi(y,y))$$
\par $$ = \frac{1}{4}(\varphi(x+y, x+y) - \varphi(x-y, x-y))$$
\par $\varphi$ est donc entièrement caractérisée par l'application $u\mapsto \varphi(u,u)$}
\Def{}{Un espace préhilbertioen réel est un espace vectoriel avec un produit scalaire.
\par Un espace euclidien est un espace préhilbertien réel de dimension finie.}
\begin{Rem}
On note $\langle.,.\rangle$ ou $(.\vert.)$ le produit scalaire dans un espace préhilbertien.
\par Un produit scalaire induit une norme d'expression $\Vert x\Vert= \sqrt{\langle x, x\rangle}$
\end{Rem}
\Thr{Cauchy-Schwarz}{$E$ un $\R$-v et $\varphi$ une forme bilinéaire symétrique positive
\par Alors $\forall x,y\in E, \vert\varphi(x,y)\vert\leq \varphi(x, x)\varphi(y,y)$
\par Dans un espace préhilbertien réel $E$, 
\par $$\forall x,y\in E, \vert\langle x,y\rangle \vert \leq \Vert x\Vert\Vert y\Vert$$
\par avec le cas d'égalité si, et seulement si, $x$ et $y$ sont colinéaires.}
\Pre{L'inégalité a déjà été prouvée dans le chapitre de proba.
\par Si $x$ et $y$ sont colinéaires : si $x=0$ alors $\vert\langle x,y\rangle\vert=0 = \Vert x\Vert \Vert y\Vert$
\par Sinon, on peut écrire $y=\alpha x$ pour $\alpha\in\R$, et donc :
\par $$\vert\langle x, y\rangle\vert = \vert\alpha\langle x, x\rangle\vert = \vert\alpha\vert\Vert x\Vert^2 = \Vert x\Vert\Vert\alpha x\Vert = \Vert x\Vert\Vert y\Vert$$
Réciproquement, on suppose que $\vert\langle x, y\rangle\vert = \Vert x\Vert\Vert y\Vert$
\par Considérons $P$ le polynôme qui à $t\in\R$ associe $\Vert x+ty\Vert^2 = \Vert x\Vert^2+2t\langle x,y\rangle+t^2\Vert y\Vert$
\par Ce polynôme est de degré $2$ et de discriminant nul par hypothèse.
\par Donc il existe un unique $t_0\in\R$ tel que $\Vert x+t_0y\Vert^2=0$
\par Par définition de la norme, $x=-t_0y$ et les deux vecteurs sont donc colinéaires.}
\Thr{Inégalit"é triangulaire}{Soient $x, y\in E$, alors :
\par $$\Vert x+y\Vert^2=\Vert x\Vert^2+\Vert y\Vert^2+2\langle x,y\rangle$$
\par $$(\Vert x\Vert+\Vert y\Vert)^2=\Vert x\Vert^2+\Vert y\Vert^2 + 2\Vert x\Vert\Vert y\Vert$$
\par Donc par Cauchy-Schwarz, $\vert\langle x, y\rangle\vert\leq \Vert x\Vert\Vert y\Vert$ et on en déduit l'inégalité triangulaire :
\par $$\Vert x+y\Vert\leq \Vert x\Vert+\Vert y\Vert$$
\par Avec cas d'égalité si $x$ et $y$ sont colinéaires et de même sens.}


\section{Orthogonaux}
\Def{Orthogonalité}{\begin{itemize}
\item pour $u,v\in E$, $u$ et $v$ sont orthogonaux, noté $u\perp v$, si $\langle u, v\rangle = 0$
\item deux sev $F$ et $G$ de $E$ sont orthogonaux si $\forall x\in F, \forall y\in G, \langle x, y\rangle = 0$
\item pour $A\subset E$, l'orthogonal de $A$ est l'ensemble $A^\perp = \{x\in E\vert \forall a\in a, \langle a, x\rangle=0\}$
\end{itemize}}
\Prop{propriétés des orthogonaux}{\begin{itemize}
\item Pour $A, B$ inclus dans $E$, $A\subset B\Rightarrow B^\perp \subset A^\perp$ 
\item $$\forall A\subset E, A^\perp = vect(A)^\perp$$
\item $A^\perp$ est un sev de $E$
\item $F, G$ sev de $E$ :
\par $$ F^\perp\cap G^\perp = (F+G)^\perp$$
\par $$F^\perp + G^\perp \subset (F\cap G)^\perp$$ (réciproque fausse)
\item $$F\subset (F^\perp)^\perp$$ 
\end{itemize}}
\Pre{$$F^\perp \cap G^\perp  = (F+G)^\perp$$
\par $F^\perp\cap G^\perp = (F\cup G)^\perp = vect(F\cup G)^\perp = (F+G)^\perp$
\par $$F^\perp+G^\perp \subset (F\cap G)^\perp$$
\par $F\cap G\subset F$
\par $F^\perp\subset (F\cap G)^\perp$
\par $F\cap G\subset G$
\par $G^\perp\subset(F\cap G)^\perp$
\par $$F\subset (F^\perp)^\perp$$
\par Si $x\in F$, alors $\forall y\in F^\perp, \langle x, y\rangle = 0$. Donc $x\in (F^\perp)^\perp$}

\begin{Rem}
En dimension finie, si $F$ est un sev de $E$ de base $(u_1, ..., u_p)$, alors :
\par $$\forall x\in E, x\in F^\perp \Leftrightarrow \left\{\begin{array}{l} \langle x, u_1\rangle =0\\ ... \\\langle x, u_p\rangle\end{array}\right.$$
\end{Rem}

\begin{Exe}
$E =\mathcal{C}([0,1],\R)$, muni du produit scalaire $\langle f,g\rangle = \int_0^1f(t)g(t)$
\par $F = \R[X]$ un sev de $E$
\par Si $f\in F^\perp$, on considère $(P_n)\in \R[X]^\N$ qui converge uniformément vers $f$ (qui existe par Stone-Weierstrass)
\par Comme $f$ est bornée contintinue sur le segment $[0,1]$, alors $(f\times P_n) \to f^2$
\par Donc $\in_0^1f(t)P_n(t)dt \to \int_0^1f^2(t)dt$ par convergence uniforme.
\par Or $f\in F^\perp$ donc $\forall n\in\N, \langle f, P_n\rangle = 0$
\par Donc $\int_0^1f^2 = 0$, $f^2$ est positive continue sur le segment $[0,1]$ donc par stricte positivité, $\forall x\in[0,1], f(x)=0$
\par Donc $f=0$, donc $F^\perp \subset \{0\}$
\par Donc $F^\perp=\{0\}$
\end{Exe}

\begin{Rem}
Si $F$ et $G$ sont deux sev de $E$
\par $F$ et $G$ sont orthogonaux si, et seulement si, $F\subset G^\perp$
\end{Rem}

\Def{Familles orthogonales}{Soit $(x_i)_{i\in I}$ une famille de vecteurs d'un espace préhilbertien $E$
\par $(x_i)$ est une famille orthogonale si :
\par $$\forall i,\in I, i\neq j \Rightarrow \langle x_i, x_j\rangle$$
\par $(x_i)$ est une famille orthonormale si :
\par $(x_i)_{i\in I}$ est orthogonale et $\forall i\in I, \Vert x_i\Vert = 1$}
\Prop{}{Toute famille orthogonale de vecteurs non nuls est une famille libre.}
\Pre{$(u_i)_{i\in I}$ une famille orthogonale.
\par Soit $J$ fini, $J\subset I$
\par Soit $(\lambda_j)_{j\in J}\in\R^J$
\par On suppose que $\sum\limits_{j\in J}\lambda_j u_j = 0$
\par Prenons $j_0\in J$
\par $\langle u_{j_0}, \sum\limits_{j\in J}\lambda_ju_j\rangle = 0$
\par Donc $\lambda_{j_0}\langle u_{j_0}, u_{j_0}\rangle  = 0$
\par Comme $u_{j_0}\neq 0$, donc $\lambda_{j_0}=0$}
\begin{Rem}
En particulier dans un espace de dimension $n$, une famille orthonormale de $n$ vecteurs est une base orthonormale (BON).
\par De l'intérêt dans un espace euclidien de travailler dans une BON :
\par Soit $E$ euclidien et $B=(e_1,..., e_n)$ une BON
\par alors avec $\forall x\in E, x  = \sum\limits_{i=1}^nx_i e_i$
\par et $\forall y\in E, y = \sum\limits_{i=1}^n y_ie_i$
\par $$\langle x, y\rangle = \sum\limits_{i, j\in\llbracket 1, n\rrbracket}x_iy_j\langle e_i, e_j\rangle$$
\par $$  = \sum\limits_{i=1}^nx_iy_i = X^TY$$
\par $\Vert x\Vert^2 = \sum\limits_{i=1}^nx_i^2$ est vrai si, et seulement si, on a une BON
\par $\forall i\in\llbracket 1, n\rrbracket, x_i = \langle x, e_i\rangle$ est vrai si, et seulement si, on a une BON
\end{Rem}
\Thr{}{Soit $E$ un espace euclidien et $F$ un sev de $E$
\par Alors $$F\oplus F^\perp = E$$
\par (ie : $F^\perp$ est un supplémentaire de $F$)}
\Pre{$\forall x\in E, x\in F\cap F^\perp \Rightarrow \langle x,x\rangle = 0$
\par Donc $x=0$
\par Donc $F\cap F^\perp = \{0\}$ (ce qui est valable même en dimension infinie)
\par On note $n$ la dimension de $E$ et $p$ la dimension de $F$. On se donne une base de $F$ $(u_1,..., u_p)$
\par On considère $\varphi : \left\{\begin{array}{rcl} E & \to & \R^p \\ x & \mapsto & (\langle x, u_1\rangle, ..., \langle x, u_p\rangle)\end{array}\right.$
\par Donc $\varphi\in\mathcal{L}(E, \R^p)$
\par On a que $F^\perp = \ker\varphi$
\par Considérons $\varphi_{|F}$ ie $\varphi_{|F}:\left\{\begin{array}{rcl} F & \to & R^p \\ x& \mapsto & \varphi(x)\end{array}\right.$
\par Alors $\ker\varphi_{|F} = \ker\varphi\cap F  =\{0\}$
\par Donc $\varphi_{|F}$ est une application injective de l'espace $F$ de dimension $p$ dans $\R^p$ de dimension $p$
\par donc $\varphi_{|F}$ est bijective donc surjective
\par Donc $\varphi$ est surjective
\par Donc $rg(\varphi) = p$
\par Donc par théorème du rang, $\dim\ker\varphi = n-p$
\par Donc $\dim F^\perp + \dim F = \dim E$ et $F+F^\perp$ est directe
\par Donc $F\oplus F^\perp = E$}
\Prop{Corollaire}{Tout espace euclidien possède une BON.}
\Pre{On le fait par récurrence forte sur la dimension :
\par $H_n$ : tout espace de dimension $n$ ou moins possède une BON.
\par Dans un espace de dimension $n+1$, en sachant $H_n$, on pourra prendre un espace de dimension $1$ et son orthogonal de dimension $n$, on réutilise l'hypothèse de récurrence pour avoir une BON de l'orthogonal et on norme le vecteur de base de la droite.}
\Thr{Extension}{Soit $E$ préhilbertien réel et $F$ un sev de $E$ de dimension finie. Alors :
\par $$F\oplus F^\perp = E$$}
\Pre{Dans le cadre de l'énoncé, on sait déjà que $F\cap F^\perp = \{0\}$
\par Soit $x\in E$, on considère $G_x = Vect(x)+F$
\par $G$ est euclidien. $F$ est un sev de $G_x$ et $x\in G_x$
\par Donc $G_x = F\oplus F^{\perp_{G_x}}$ (l'orthogonal dans $G_x$)
\par Donc $\exists u, v\in F\times F^{\perp_{G_x}}, x = u+v$
\par $\forall f\in F, \langle v, f\rangle = 0$
\par Donc $v\in F^\perp$
\par Donc $E\subset F+F^\perp$}
\Thr{Méthode de Schmidt}{Cette méthode permet de "redresser une boîte à chaussures écrasée", c'est à dire construire une base orthogonale.
\par Soit $E$ un espace préhilbertien, $(u_i)_{i\in I}$ famille libre de $E$ avec $I\subset \N$
\par Alors on peut construire par récurrence une famille morthogonale $(v_n)_{n\in I}$ qui vérifie :
\par $$\forall n\in I, Vect(u_0,..., u_n) =Vect(v_0,..., v_n)$$
\par $(v_n)$ est définie par la relation :
\par $\begin{cases} v_0 = u_0 \\ v_{n+1} = u_{n+1} - \sum\limits_{i=1}^n\frac{\langle u_{n+1}, v_i\rangle}{\Vert v_i\Vert^2}v_i \end{cases}$
\par Pour rendre cette famille orthonormale, il suffit de prendre la famille et de diviser chaque vecteur par sa norme}
\Pre{Pour montrer que la famille est orthogonale, on procède par récurrence pour montrer que l'espace engendré est bien le même à chaque nouvel élément.}

\Prop{}{Si $E$ est euclidien, $F$ et $G$ deux sev de $E$.\begin{itemize}
\item $$(F\cap G)^\perp = F^\perp + G^\perp$$
\item $$ F = (F^\perp)^\perp$$
\end{itemize}}
\Pre{On a déjà les inclusions, on se sert juste de l'égaité des dimensions en dimension finie.}

\section{Projections orthogonales}
\subsection{Généralités}
\Def{}{Dans tous les cas où $F\oplus F^\perp = E$, on peut définir :\begin{itemize}
\item la projection orthogonale sur $F$ (la projection sur $F$ parallèlement à $F^\perp$)
\item la symétrie orthogonale par rapport à $F$
\end{itemize}}
\Thr{de la meilleure approximation}{Soit $E$ un espace préhilbertien et $F$ un sev de dimension finie.
\par Alors pour tout $x$ de $E$,
\par \begin{center} $d(x,F)$ est atteinte en un unique vecteur de $F$, le projeté orthogonal de $x$ sur $F$\end{center}}
\Pre{$d(x, F) = \inf_{y\in F}d(x, y)$
\par Dans les conditions de l'énoncé, soit $y\in F$, on désigne $p$ la projection orthogonale sur $F$
\par $\Vert x-y\Vert^2 = \Vert x-p(x)+p(x)-y\Vert^2$
\par Or $x-p(x)\in F^\perp$ et $p(x)-y\in F$, donc les deux sont orthogonaux
\par Donc $\Vert x-y\Vert^2 = \Vert x - p(x)\Vert^2 + \Vert p(x)-y\Vert^2$ (par pythagore vectoriel)
\par Donc $\Vert x-y\Vert^2\geq \Vert x-p(x)\Vert^2$
\par et $\Vert x-y\Vert^2=\Vert x-p(x)^2\Leftrightarrow \Vert p(x)-y\Vert^2=0\Leftrightarrow y=p(x)$}

\subsection{Calcul pratique du projeté orthogonal}
\Prop{}{$F$ un sev de $E$ de dimension finie $p$, $(u_1, ..., u_p)$ une base de $F$
\par $x\in E$, $p(x)$ son projeté orthogonal sur $F$
\par Donc $(px)$ est entièrement défini par les équations :
\par $$(1)\left\{\begin{array}{l} p(x)\in F \\ x-p(x)\in F^\perp\end{array}\right.$$}
\Pre{$(1)\Leftrightarrow \left\{\begin{array}{l}\exists (\lambda_1, ...\lambda_p)\in\R^p, p(x) = \sum\limits_{i=1}^p\lambda_iu_i \\ \langle x - p(x), u_1\rangle = 0 \\ ... \\ \langle x - p(x), u_p\rangle = 0\end{array}\right.$
\par $\left\{\begin{array}{l}\Leftrightarrow \exists (\lambda_1, ...\lambda_p)\in\R^p, p(x) = \sum\limits_{i=1}^p\lambda_iu_i \\ \sum\limits_{j=1}^p\langle u_1, u_j\rangle \lambda j = \langle x, u_1\rangle \\ ... \\ \sum\limits_{j=1}^p\langle u_p, u_j\rangle = \langle x, u_p\rangle\end{array}\right.$
}
On note $(S)$ ce dernier système, et $(S)\Leftrightarrow G(u_1,..., u_p)\begin{pmatrix} \lambda_1 \\ ...\\ \lambda_p\end{pmatrix} = \begin{pmatrix} \langle x,u_1\rangle\\ ... \\ \langle x, u_p\rangle\end{pmatrix}$ 
\par Où $G(u_1, ..., u_p)\in \mathcal{M}_p(\R)$ tel que $G(u_1,..., u_p) = (\langle u_i, u_j\rangle)_{(i,j)\in\llbracket 1,n\rrbracket^2}$
\par Dans le cas où $(u_1,..., u_p)$ est une BON de $F$ :
\par $$ p(x) = \langle x, u_1\rangle u_1 + ... + \langle x, u_p\rangle u_p$$
\par Si $(u_1, ..., u_p)$ est seulement une base orthogonale :
\par $$p(x) = \frac{\langle x, u_1\rangle}{\Vert u_1\Vert^2}u_1+...+\frac{\langle x, u_p\rangle}{\Vert u_p\Vert^2}u_p$$


\subsection{Exemples d'utilisation du projeté orthogonal}
\begin{Exe}
\underline{Les séries de Fourier} : soit $E = \mathcal{C}_T(\R)$ l'ensemble des fonctions continues $T$ périodiquies.
\par On munit $E$ du produit scalaire $\forall f, g\in E, \langle f, g\rangle = \frac{1}{T}\int_0^Tf(t)g(t)dt$
\par Pour $n\in\N$, on considère $c_n:\left\{\begin{array}{rcl}\R & \to & \R \\ t & \mapsto & \cos\left(\frac{2\pi n}{T} t\right)\end{array}\right.$
\par Pour $n\in\N^*$, on considère $s_n:\left\{\begin{array}{rcl}\R & \to & \R \\ t & \mapsto & \sin\left(\frac{2\pi n}{T} t\right)\end{array}\right.$
\par Pour $n\in\N$, on note $F_n= Vect((c_k)_{0\leq k\leq n}, (s_k)_{1\leq k\leq n})$
\par $\forall n\in\N, c_n\in E, \forall n\in\N^*, s_n\in E$
\par $\forall n,p\in\N, \langle c_n,c_p\rangle  = \frac{1}{T}\int_0^T\cos\left(\frac{2\pi n}{T}t\right)\cos\left(\frac{2\pi p}{T}t\right)dt$
\par $ = \frac{1}{2T}\int_0^T\cos\left(\frac{2\pi (n+p)}{T}t\right) + \cos\left(\frac{2\pi (n-p)}{T}t\right)dt = 0$
\par De même, $\forall n,p, \langle c_n, sp\rangle = 0$ et $\forall n, p, n\neq p\Rightarrow \langle s_n, s_p\rangle = 0$
\par Donc pour tout $n\in\N$ $(c_0,..., c_n, s_1,..., s_n)$ est une base orthogonale (famille orthogonale de vecteurs non-nuls) de $F_n$.
\par $\langle c_0, c_0\rangle = 1$
\par $\forall n\in\N^*, \langle c_n, c_n\rangle = \frac{1}{2}$
\par $\forall n\in\N^*, \langle s_n, s_n\rangle = \frac{1}{2}$
\par Pour $f\in E$, et $n\in\N$, on pose :
\par $$S_n(f) = \frac{\langle f, c_0\rangle}{\Vert c_0\Vert}^2c_0 + \sum\limits_{k=1}^n\frac{\langle f, c_k\rangle}{\Vert c_k\Vert^2}c_k + \frac{\langle s, s_k\rangle}{\Vert s_k\Vert^2}s_k$$
\par $$ = \langle f, c_0\rangle c_0 + \sum\limits_{k=1}^n2\langle f, c_k\rangle c_k +2\langle f, s_k\rangle s_k$$
\par $S_n(f)$ est la meilleure approximation de $f$ dans $F_n$
\par $\Vert f\Vert^2 = \Vert f-S_n(f)\Vert^2+\Vert S_n(f)\Vert^2$
\par où $\Vert S_n(f)\Vert^2 = \langle f, c_0\rangle^2 + 2\sum\limits_{k=1}^n\langle f, c_k\rangle^2 + \langle f, s_k\rangle$
\par $(\Vert S_n(f)\Vert^2)$ est la suite des sommes partielles d'une série à termes positifs majorée par $f$, donc $\Vert S_n(f)\Vert^2$ converge en $+\infty$ et $\Vert S_\infty(f)\Vert^2\leq \Vert f\Vert^2$ (inégalité de Parceval)
\end{Exe}

\begin{Exe}
\underline{Régression linéaire} : On dispose de $p$ points de $\R^n$
\par On note $\left\{\begin{array}{lr}x_{1,\cdot}=x_{1,1}, x_{1, 2},..., x_{1, n} & z_1 \\ x_{2, \cdot} = x_{2,1}, x_{2,2},..., x_{2,n} & z_2 \\ ... & ... \\ x_{p, \cdot} = x_{p, 1},..., x_{p,n} & z_p\end{array}\right.$
\par On cherche le "meilleur modèle" affine expliquant $z$, ie les "meilleurs" coefficients $a_0, ..., a_n$ tels que $a_0 + a_1x_{i,1} +...+a_nx_{i, n}$ approxime le mieux possible $z_i$ 
\par On va chercher :
\par $$\inf\limits_{(a_0,..., a_n)\in\R^{n+1}}\left(\sum_{i=1}^p z_i-(a_0+a_1x_{i,1}+...+a_nx_{i,n}\right)^2$$
\par On munit $\R^p$ du produit scalaire canonique :
\par $\sum\limits_{i=1}^p(z_i-(a_0+a_1x_{i,1}+...+a_nx_{i, n}))^2 = \Vert z - (a_0u+a_1x_{\cdot, 1} + ...+a_nx_{\cdot, n})\Vert^2$
\par Où $u = \begin{pmatrix} 1 \\ .\\.\\ 1\end{pmatrix}$ et $z=\begin{pmatrix} z_1 \\.\\.\\ z_p\end{pmatrix}$
\par Si $(x_{\cdot, 1},..., x_{\cdot, n}, u)$ est libre, le problème a une unique solution $\tilde{a}_0, \tilde{a}_1,..., \tilde{a}_n$ telle que $\tilde{a}_0u + \tilde{a}_1x_{\cdot, 1}+...+\tilde{a}_nx_{\cdot n}$ est le projeté orthogonal de $z$ sur $F = Vect(u, x_{\cdot, 1}, ..., x_{\cdot, n})$
\end{Exe}

\section{Endormorphismes particuliers des espaces euclidiens}
\Def{}{Soit $E$ un $\K$-ev. On appelle dual de $E$ l'ensemble $E^*=\mathcal{L}(E, \K)$ des formes linéaires sur $E$.}
\Prop{}{$E^*$ est un $\K$ ev. Si $E$ est de dimension finie, la dimension de $E^*$ est de dim finie, et $\dim E^* = \dim E$}
\Thr{de représentation de Reese (cas euclidien)}{Soit $E$ un espace euclidien, l'application :
\par $\psi\left\{\begin{array}{rcl} E & \to & E^* \\a & \mapsto \varphi_a:\left\{\begin{array}{rcl} E & \to & \K\\ x & \mapsto & \langle a, x\rangle\end{array}\right.\end{array}\right.$ est un isomorphisme}
\Pre{$\psi$ est linéaire : immédiat ($\psi(a+b)=\psi(a)+\psi(b)$ ie $\forall x\in E, (\psi(a+b))(x) =\psi(a)(x)+\pi(b)(x)$)
\par Soit $a\in \ker\psi$, alors $\forall x\in E, \langle a, x\rangle = 0$
\par Donc $a\in E^\perp$
\par Donc $a=0$
\par Donc $\psi$ est une application linéaire injective de $E$ dans l'espace de même dimension $E^*$, donc $\psi$ est bijective.}

\Def{Adjoint}{Soit $E$ un espace euclidien et $u\in\mathcal{L}(E)$.
\par On appelle adjoint de $u$ l'application de $E$ dans $E$ $u^\star$ telle que :
\par $$\forall x,y\in E\times E, \langle u(x), y \rangle = \langle x, u^\star(y)\rangle$$}
\begin{Rem}
Le fait que cette définition soit correcte découle du théorème de représentation des formes linéaires (théorème précédent)
\par En effet pour $y\in E$ fixé : $x\mapsto \langle u(x), y\rangle\in E^*$
\par Donc $\exists! u^\star(y), \forall x\in E, \langle y, u(x)\rangle = \langle u^\star(y), x\rangle$
\end{Rem}
\Prop{propriétés de l'ajdoint}{Soit $E$ euclidien, $u,v\in\mathcal{L}(E)$.\begin{itemize}
\item $$u^\star\in\mathcal{L}(E)$$
\item $$(u^\star)^\star = u$$
\item $$(u\circ v)^\star =v^\star \circ u^\star$$
\item $u\to u^\star$ est linéaire, ie $(u+v)^\star = u^\star + v^\star$ et $\forall \lambda\in\R, (\lambda u)^\star = \lambda u^\star$
\end{itemize}}
\Pre{$y,z\in E$
\par Soit $x\in E, \langle x, u^\star(y+z)\rangle = \langle u(x), y+z\rangle$
\par $ = \langle u(x), y\rangle + \langle u(x), z\rangle = \langle x, u^\star(y)\rangle + \langle x, u^\star(z)\rangle$
\par Donc $\langle x, u^\star(y+z)\rangle = \langle x, u^\star(y)+u^\star(z)\rangle$
\par Donc $\forall x\in E, \langle x, u^\star (y+z) - (u^\star(y)+u^\star(z)) \rangle =0$
\par Donc $u^\star(y+z) - u^\star(y) - u^\star(z) \in E^\perp$
\par De même, $\forall \lambda\in\R,\forall y\in E, u^\star(\lambda y) = \lambda u^\star(y)$
\par $\forall (x,y)\in E\times E, \langle u^\star(x), u\rangle = \langle x, u(y)\rangle$ donc $(u^\star)^\star = u$
\par Soit $x\in E$ et $y\in E$
\par $\langle u\circ v(x), y\rangle = \langle v(x), u^\star(y)\rangle = \langle x, v^\star\circ u^\star(y)\rangle$
\par Donc $(u\circ v)^\star = v^\star\circ u^\star$
\par La dernière propriété est immédiate.}
\Thr{}{Soit $E$ un espace euclidien. Soit $B$ une BON de $E$. Soit $u\in\mathcal{L}(E)$
\par On note $A = Mat_B(u)$
\par Alors $Mat_B(u^\star)= A^T$}
\Pre{Pour $x\in E$, on note $X = Mat_B(x)\in\mathcal{M}_{n,1}(\R)$
\par Pour $y\in E$, on note $Y = Mat_B(y)\in\mathcal{M}_{n,1}(\R)$
\par $\forall x, y\in E, \langle u(x), Y\rangle = (AX)^TY = X^T (A^TY)$ (par propriétés de la transposée)
\par Si on note $B=Mat_B(u^\star)$, alors $\langle x, u^\star(y)\rangle = X^T BY$
\par Donc $\forall X,Y\in\mathcal{M}_{n,1}(\R), X^T(A^TY - BY)=0$
\par Donc $A^T=B$}
\Prop{}{Soit $E$ un espace euclidien, $u\in\mathcal{L}(E)$, $F$ un sev de $E$.
\par Si $F$ est stable par $u$ alors $F^\perp$ est stable par $u^\star$}
\Pre{Soit $y\in F^\perp$. Alors $\forall x\in F, \langle x,y\rangle = 0$
\par Or $\forall x\in F, u(x)\in F$
\par Donc $\forall x\in F, \langle u(x), y\rangle = \langle x, u^\star(y)\rangle=0$
\par Donc $u^\star(y)\in F^\perp$
\par Donc $F^\perp$ est stable par $u^\star$.}


\section{Isométries vectorielles}
\Def{}{Soit $E$ un espace euclidien et $f\in\mathcal{L}(E)$. On dit que $f$ est une isométrie si
\par $$\forall x\in E, \Vert f(x)\Vert = \Vert x\Vert$$}
\Prop{Caractérisation}{Soit $f\in\mathcal{L}(E)$ avec $E$ euclidien muni d'une base $\mathcal{B}$ orthonormée, $f$ est une isométrie si, et seulement si, elle vérifie l'une des propriétés suivantes :\begin{enumerate}
\item $$\forall x\in E, \Vert f(x)\Vert=\Vert x\Vert$$
\item $$\forall x,y\in E, \langle f(x), f(y)\rangle = \langle x, y\rangle$$
\item $f(\mathcal{B})$ est une base orthonormée.
\item $f\in\mathcal{GL}(E)$ et $f^\star=f^{-1}$
\end{enumerate}}
\Pre{$\underline{1\Rightarrow 2}$ : on suppose que $f$ conserve la norme.
\par Soit $x,y\in E$, alors $\langle f(x), f(y)\rangle = \frac{1}{2}\left(\Vert f(x)+f(y)\Vert^2 - \Vert f(x)\Vert^2 - \Vert f(y)\Vert^2\right) = \frac{1}{2}\left(\Vert f(x+y)\Vert^2 - \Vert f(x)\Vert^2 - \Vert f(y)\Vert^2\right)$
\par $ = \frac{1}{2}\left(\Vert x+y\Vert^2 - \Vert x\Vert^2-\Vert y\Vert^2\right) = \langle x, y\rangle$
\par $\underline{2\Rightarrow 3}$ : immédiat, une isométrie conserve le produit scalaire.
\par $\underline{3\Rightarrow 1}$ : On suppose que $f(\mathcal{B})$ est une base orthonormée. On note $\mathcal{B}=(e_1,..., e_n)$
\par $\forall x\in E,x =\sum\limits_{i=1}^n x_ie_i$ et $\Vert x\Vert^2=\sum\limits_{i=1}^n x_i^2$ car $\mathcal{B}$ est une base orthonormale.
\par $f(x) = \sum\limits_{i=1}^nx_if(e_i)$, donc $\Vert f(x)\Vert^2 = \sum\limits_{i=1}^nx_i^2$ car $f(\mathcal{B})$ est une base orthonormale.
\par Pour la 4 : 
\par Si $f$ est une isométrie : l'image d'une base par un isomorphisme est une base
\par Donc $f$ est bijective
\par $\forall x,y\in E, \langle x, f(y)\rangle = \langle f(f^{-1}(x)), f(y)\rangle = \langle f^{-1}(x), y\rangle$
\par Donc $f^\star = f^{-1}$
\par Réciproquement, on suppose que $f\in\mathcal{GL}(E)$ et $f^\star=f^{-1}$
\par Soit $x\in E$ :
\par $\Vert f(x)\Vert^2 = \langle f(x), f(x)\rangle = \langle x, f^\star\circ f(x)\rangle = \langle x, x\rangle = \Vert x\Vert^2$}
\begin{Rem}
Attention : on emploie parfois le terme d'endomorphisme orthogonal pour désigner les isométries.
\par Ce vocabulaire est trompeur :\begin{itemize}
\item Ene projection orthogonale n'est pas un endomorphisme orthogonal (sauf l'identité)
\item Une symétrie orthogonale est un endomorphisme orthogonal. En effet, notons $x\in E, x= x_F + x_{F^\perp}$
\par Donc $s(x) = x_F-x_{F^\perp}$
\par $\Vert x\Vert^2 = \Vert x_F\Vert^2+\Vert x_{F^\perp}\Vert^2=\Vert x_F\Vert^2+\Vert -x_{F^\perp}\Vert^2 = \Vert s(x)\Vert^2$
\par $s\in\mathcal{L}(E)$ conserve la norme, c'est une isométrie.
\end{itemize}
\end{Rem}

\begin{Exe}
Soit $E$ un espace euclidien et $s$ une symétrie. Montrez que $s$ est une isométrie si, et seulement si, $s$ est une symétrie orthogonale.
\par On a déjà la première implication par la remarque précédente, raisonnons par contraposée :
\par Si $s$ est une symétrie non-orthogonale :
\par $f$ est une symétrie par rapport à $F$ parallèlement à $G$. $s$ n'est pas orthogonale, donc $F$ et $G$ ne sont pas orthogonaux.
\par Donc on peut noter $f\in F, g\in G$ tels que $\langle f, g\rangle\neq 0$
\par $\Vert f+g\Vert^2 =\Vert f\Vert^2+2\langle f, g\rangle+\Vert g\Vert^2$
\par $\Vert f-g\Vert^2 =\Vert f\Vert^2 - 2\langle f, g\rangle + \Vert g\Vert^2$
\par Et donc $\Vert s(f+g)\Vert\neq \Vert f+g\Vert$
\par Donc $s$ n'est pas une isométrie.
\end{Exe}

\subsection{Isométries directes, indirectes}
\Prop{}{Soit $E$ un espace euclidien, $u$ une isométrie. Alors :
\par $$(\det u)\in \{-1, 1\}$$
\par On dit que $u$ est une isométrie directe si $\det(u)=1$, indirecte sinon.}
\Pre{Si $u$ une isométrie.
\par $$\det(u)=\det(u^\star)=\det(u^{-1})=\frac{1}{\det(u)}$$}
\Prop{}{\begin{itemize}
\item L'ebnsemble des isométries est un groupe pour la loi $\circ$
\par On l'appelle le groupe orthogonal noté $\mathcal{O}(E)$.
\item L'ensemble des isométries directes est un groupe pour la loi $\circ$.
\par On l'appelle le groupe spécial orthogonal, noté $\mathcal{SO}(E)$ ou $\mathcal{O}_+(E)$.
\item L'ensemble des isométries indirectes \textbf{n'est pas} un groupe pour la loi $\circ$.
\par On le note $\mathcal{O}_-(E)$
\end{itemize}}
\Pre{On démontre que $\mathcal{O}(E)$ est un sous-groupe de $(\mathcal{GL}(E),\circ)$
\par Si $u,v\in\mathcal{O}(E)$, alors :
\par $\forall x\in E, \Vert u\circ v(x)\Vert = \Vert u(v(x))\Vert = \Vert v(x)\Vert = \Vert x\Vert$
\par Donc $u\circ v\in\mathcal{O}(E)$
\par $\forall x\in E, \Vert u^{-1}(x)\Vert^2 = \langle u^\star(x), u^{-1}(x)\rangle =\langle x, (u^\star)^{-1}u^{-1}(x) \rangle$
\par $= \langle x, u\circ u^{-1}(x)\rangle = \langle x,x\rangle$
\par Donc $\mathcal{O}(E)$ est un sous-groupe de $\mathcal{GL}(E)$}
"Vous revenez pas à Mathusalem" - Chakroun, pour dire de ne pas prouver que quelque chose est un groupe et de prouver que c'est un sous-groupes parce que c'est quand même plus simple.

\subsection{Orientation de l'espace}
\Def{}{$E$ un espace euclidien, on définit la relation sur les bases de cet espace :
\par $$B\mathcal{R}B'\Leftrightarrow \det_B(B') > 0$$
\par $\mathcal{R}$ est une relation d'équivalence.
\par Il y a deux classes d'équivalence.}
\Pre{Soit $B=(e_1,e_2,..., e_n)$ une base, alors $B'=(-e_1, e_2,..., e_n)$ n'est pas dans la classe de $B$
\par Soit $B''$ une base, si $B''$ n'est pas dans la classe de $B$
\par Alors $\det_{B'}B'' = \det_{B}B'' \det_{B'}{B'}B>0$ ($\det_B(B'')<0$ et $\det_{B'}(B')<0$) donc $B''$ est dans la classe de $B'$.}
Orienter l'espace, c'est choisir une classe d'équivalence ie choisir la classe des bases directes.

\Prop{}{Soit $E$ un espace euclidien orienté, $\mathcal{B}$ une base orthonormale directe (BOND)
\par $u\in\mathcal{L}(E)$ est une isométrie directe si, et seulement si, $u(B)$ est une BOND.
\par $$ \det_B(u(e_1),..., u(e_n)) = \det u\det_B(e_1, ..., e_n)$$}
\Def{Produit mixte}{Soit $E$ un espace euclidien orienté et $\mathcal{B}$ une base orthonormale directe.
\par L'application $(x_1,..., x_n)\mapsto \det_\mathcal{B}(x_1,..., x_n)$ ne dépend pas de $\mathcal{B}$
\par On l'appelle produit mixte noté $[x_1, ..., x_n]$}
\Pre{B une base orthonormale directe.
\par $\det_{\mathcal{B}'}(x_1,..., x_n) = \det_\mathcal{B}(x_1,..., x_n)\det_{\mathcal{B}'}(B)$
\par L'application linéaire qui envoie $\mathcal{B}'$ sur $\mathcal{B}$ est une isométrie directe.
\par Donc $\det_{\mathcal{B}'}\mathcal{B}=+1$}


\section{Matrices orthogonales}
\subsection{Généralités}
\Def{}{On dit qu'une matrice $M\in\mathcal{M}_n(\K)$ est orthogonale si elle vérifie l'une des propriétés équivalentes suivantes :\begin{enumerate}
\item $$M^TM = I$$
\item $$MM^T = I$$
\item $M\in\mathcal{GL}_n(\R)$ et $M^T=M^{-1}$
\item Les vecteurs colonnes de $M$ forment une base orthonormale pour le produit scalaire canonique de $\mathcal{M}_{n,1}(\R)$
\item Les vecteurs lignes de $M$ forment une base orthonormale pour le produit scalaire canonique de $\mathcal{M}_{1,n}(\R)$
\end{enumerate}}
\Pre{Les seules implications à montrer sont $1\Leftrightarrow 4$, puisque $1\Leftrightarrow 2\Leftrightarrow 3$ et $4\Leftrightarrow 5$
\par Soit $M\in\mathcal{M}_n(\R)$
\par On considère $C_1,..., C_n$ les vecteurs colonnes de $M$
\par $\forall j\in\llbracket 1,n\rrbracket, C_j = \begin{pmatrix}M_{1,j}\\ .\\.\\ M_{n, j}\end{pmatrix}$
\par Pour $j, k\in\llbracket 1, n\rrbracket, \langle C_j, C_k\rangle = \sum\limits_{i=1}^nM_{i,j}M_{i,k} = \sum\limits_{i=1}^nM^T_{j,i}M_{i,k} = (M^TM)_{j,k}$
\par Donc $M^TM=I\Leftrightarrow (C_1,..., C_n)$ est une BON.}
\begin{Exe}
$\begin{pmatrix}\frac{1}{\sqrt{5}} & \frac{-2}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} & \frac{1}{\sqrt{5}}\end{pmatrix}$
\par $\begin{pmatrix} \frac{1}{\sqrt{14}}&\frac{2}{\sqrt{14}} &\frac{3}{\sqrt{14}} \\\frac{-2}{\sqrt{5}} &\frac{1}{\sqrt{5}} & 0 \\ \frac{-3}{\sqrt{14\times 5}} &\frac{-6}{\sqrt{14\times 5}} &\frac{5}{\sqrt{14\times 5}} \end{pmatrix}$
\end{Exe}
\Prop{}{La matrice de passage entre deux BON est une matrice orthogonale.}
\Prop{}{Le déterminant d'une matrice orthogonale est égal à $1$ ou à $-1$}
\Def{}{Pour $u\in\mathcal{L}(E)$, on a équivalence entre :\begin{itemize}
\item $u$ est une isométrie vectorielle
\item la matrice associée à $u$ dans toute BON est orthogonale
\item il existe une BON dans laquelle la matrice associée à $u$ est orthogonale
\end{itemize}}

\subsection{Classification des matrices orthogonales du plan}
($\mathcal{O}_2(\R)$ l'ensemble des matrices orthogonales de $\mathcal{M}_2(\R)$)
\par $\mathcal{O}_2(\R)=\{R_\theta\vert\theta\in\R\}\cup\{S_\theta\vert\theta\in\R\}$
\par $R_\theta = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$
\par $S_\theta = \begin{pmatrix} \cos\theta & \sin\theta \\ \sin\theta & -\cos\theta\end{pmatrix}$
\par $\mathcal{SO}_2(\R) = \mathcal{O}_2^+(\R) = \{R_\theta\vert\theta\in\R\}$ (ensemble des rotations d'angle $\theta$, ensemble des matrices de $\mathcal{O}_2(\R)$ de déterminant 1)
\par $\mathcal{O}_2^-(\R)=\{S_\theta\vert\theta\in\R\}$ (ensemble des matrices de $\mathcal{O}_2(\R)$ de déterminant $-1$)
\Prop{}{$\forall(\theta,\varphi)\in\R^2$ :\begin{itemize}
\item $$R_\theta R_\varphi = R_{\theta+\varphi}$$
\item $$R_\theta^{-1} = R_{-\theta} =R_\theta^T$$
\item $\mathcal{SO}_2(\R)$ est un groupe commutatif, en particulier :
\par $$\forall \theta,\varphi\in\R, R_\theta R_\varphi R_\theta^{-1}=R_\varphi$$
\par Dans ce cas, $R_\theta$ est la matrice de passage d'une BOND dans une BOND.
\end{itemize}}
\Pre{Se prouvent par du calcul matriciel, et un peu de trigonométrie.}
$S_\theta$ est une symétrie orthogonale par rapport à une droite
\par $S_\theta\times \begin{pmatrix}\cos(\frac{\theta}{2})\\\sin(\frac{\theta}{2})\end{pmatrix} = \begin{pmatrix}\cos(\frac{\theta}{2})\\ \sin\frac{\theta}{2}\end{pmatrix}$
\par $S_\theta\times \begin{pmatrix}-\sin(\frac{\theta}{2})\\\cos(\frac{\theta}{2})\end{pmatrix} = -\begin{pmatrix}-\sin(\frac{\theta}{2})\\ \cos\frac{\theta}{2}\end{pmatrix}$
\par Donc $S_p(S_\theta) = \{-1,1\}$ Et $E_1(S_\theta)$ est orthogonal à $E_{-1}(S_\theta)$

\subsection{Classification des matrices orthogonales d'un espace de dimension 2}
(où $E_2$ un espace euclidien de dimension 2)
\par D'après l'étude de $\mathcal{O}_2(\R)$, on peut définir dans $E_2$ la rotation d'angle $\theta$:
\Def{}{Dans $E_2$ euclidien de dimension 2, on définit la rotation d'angle $\theta$ comme $r_\theta$ l'endomorphisme dont la matrice dans une BOND est $R_\theta$}
\begin{Rem}
Donc $\mathcal{SO}(E_2) =\{r_\theta\vert\theta\in\R\}$
\par $\mathcal{O}^-(E_2)$ est alors l'ensemble des symétries orthogonales de $E_2$ par rapport à des droites.
\end{Rem}
\Prop{}{Si $u$ et $v$ sont deux vecteurs normés de $E_2$ il existe une unique rotation $r_\theta$ telle que $v=r_\theta(u)$ 
\par On appellera angle entre $u$ et $v$ cette rotation ($\theta$ est la mesure de cet angle défini à $2\pi$ près)
\par On retrouve la propriété de Chasles sur les angles.
}
\Pre{$u$ et $v$ sont normés
\par $u^\perp=Vect(w)$ où $\Vert w \Vert=1$
Donc la droite $Vect(w)$ il y a seulement deux vecteurs normés : $w$ et $-w$ 
\par Donc il y a exactement 2 BON de premier vecteur $u$, $(u,w)$ et $(u, -w)$
\par Une seule d'entre eux est directe.
\par Quitte à échanger $w$ et $-w$, on peut considérer que $(u,w)$ est une BOND.
\par Donc $v\in Vect(u, w), v= au + bw$ où $a^2+b^2=1$
\par L'unique rotation $r$ telle que $r(u)=v$ a pour matrice dans $(u,w)$ : $\begin{pmatrix}a & -b\\ b&a\end{pmatrix}$}
\begin{Rem}
Par définition pour 2 vecteurs non nuls $u,v$, on appellera angle de $u$ et $v$ l'angle entre $\frac{u}{\Vert u\Vert}$ et $\frac{v}{\Vert v\Vert}$
\end{Rem}
\Prop{}{L'angle de deux vecteurs est caractérisé par sa mesure notée $\widehat{u, v}$ définie à $2\pi$ près, donc caractérisée par $\cos(\widehat{u,v})$ et $\sin(\widehat{u,v})$
\par $$\cos(\widehat{u,v}) = \frac{\langle u,v\rangle}{\Vert u\Vert \Vert v\Vert}$$
\par $$\sin(\widehat{u,v}) = \frac{[u,v]}{\Vert u\Vert \Vert v\Vert}$$}
\Pre{$u,v$ non nuls.
\par On considère $(\frac{u}{\Vert u\Vert}, w)$ l'unique BOND de premier vecteur $\frac{u}{\Vert u\Vert}$
\par On note $\theta = \widehat{u,v}$
\par $\frac{v}{\Vert v\Vert} = \cos\theta\frac{u}{\Vert u\Vert}+\sin\theta w$
\par Donc $\langle \frac{v}{\Vert v\Vert}, \frac{u}{\Vert u\Vert}\rangle = \cos\theta$
\par $\left[\frac{u}{\Vert u\Vert}, \frac{v}{\Vert v\Vert}\right] = \left\vert\begin{matrix} 1 & \cos\theta \\ 0 & \sin\theta \end{matrix}\right\vert =\sin\theta$}
\begin{Exe}
$R^2$ euclidien oritenté (la base canonique est une BOND)
\par Donner une mesure de l'angle entre $u = (5,3)$ et $v=(1,-2)$
\par $\cos(\widehat{u,v}) = \frac{5-6}{\sqrt{34}\sqrt{5}}=\frac{-1}{\sqrt{34}\sqrt{5}}$
\par $\sin\theta = \frac{-10-3}{\sqrt{34}\sqrt{5}} = \frac{-13}{\sqrt{34}\sqrt{5}}$
\par Donc $\theta =-\arccos\left(\frac{-1}{\sqrt{34}\sqrt{5}}\right)$ (comme le produit mixte de $u$ et $v$ est négatif, ce qui permet de choisir le signe)
\end{Exe}

\subsection{Classification des matrices orthogonales d'un espace euclidien orienté de dimension 3}
Pour $u\in\mathcal{O}(E_3)$, on note $F = \ker (u-id)$
\par Si $\dim F = 3$ : alors $u = id$, donc $u$ est nécessairement dans $\mathcal{O}_+(E_3)$
\par Si $\dim F=2$, alors $\dim F^\perp=1$, l'endomorphisme induit par $u$ sur $F^\perp$ est égal à $id_{F^\perp}$ ou $-id_{F^\perp}$
\par Si $u_{F^\perp}$ était égal à $id_{F^\perp}$, alors $u$ serait l'identité. Donc nécessairement, $u_{F^\perp} = -id_{F^\perp}$
\par Donc $u$ est la symétrie orthogonale par rapport à $F$
\par Si $\dim F=1$, alors $\dim F^\perp = 2$. Notons $v$ l'endomorphisme induit par $u$ sur $F^\perp$.
\par $v$ est donc soit une symétrie orthogonale par rapport à une droite, soit une rotation du plan. Si $v$ était une symétrie orthogonale, on aurait $\dim F = 2$, donc $v$ est une rotation du plan $F^\perp$
\par Donc si on choisit $w$ un vecteur qui oriente $F$ et qu'on choisit $u, v$ dans $F^\perp$ tels que $(u,v)$ soit une base orientée de $F^\perp$ et que $(w, u, v)$ soit une base directe de $E_3$, on a alors que $u$ est la rotation d'axe $w$ orienté par $w$ d'angle $\theta$
\par Si $\dim F=0$, alors ça veut dire que $1$ n'est pas valeur propre, donc $-1$ est valeur propre. $u$ est donc la composée d'une symétrie orthogonale et d'une rotation.
\begin{Exe}
Prenons $u\in\mathcal{L}(E_3)$ avec $\mathcal{B}$ une BOND de $E_3$ telle que $Mat_\mathcal{B}(u) = \frac{1}{3}\begin{pmatrix} 2 & 1 & 2 \\ 2 & -2 & -1 \\ 1 & 2 & -2\end{pmatrix}$
\par On peut faire les produits scalaires des colonnes les unes avec les autres, et on obtiendra que les produits scalaires sont nuls. Donc $A\in\mathcal{O}_3(\R)$, soit $u\in\mathcal{O}(E_3)$
\par Le calcul du déterminant de $A$ donne $1$, donc $u\in\mathcal{SO}(E_3)$, et $u\neq id$. Donc $u$ est une rotation d'axe d'angle $\theta$ orientée par $w$.
\par $w$ est solution de $(id-u)(w)=0$. On échelonne le système, et on obtient que $w$ est solution de $\left\{\begin{array}{ccccl} x & -y & -2z & = & 0 \\ & 3y & -3z & = & 0 \\ & -3y & +3z & = & 0\end{array}\right.$
\par Choisissons $w = 3e_1+e_2+e_3$, qu'on norme en $e_1' = \frac{1}{\Vert w\Vert}w=\frac{1}{\sqrt{11}}w$
\par Pour le choix du deuxième élément de la base, on a juste besoin d'un élément de $F^\perp$, donc on peut choisir le vecteur normé qu'on veut. On prend $e_2' = \frac{1}{\sqrt{10}}(e_1-3e_2)$
\par On n'a plus de choix pour le troisième élément de la base cependant. Pour déterminer un vecteur qui soit orthogonal aux deux précédents, on peut utiliser le produit vectoriel, et on a $e_3' = \frac{1}{\sqrt{10}\sqrt{11}}(3e_1+e_2-10e_3)$
\par On note $\mathcal{B}' = (e_1', e_2', e_3')$
\par On a alors $Mat_{\mathcal{B}'}(u) = \begin{pmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta\end{pmatrix}$
\par (La matrice de passage de $\mathcal{B}$ dans $\mathcal{B}'$ est la matrice d'une BOND dans une BOND, donc son inverse est égal à sa transposée.)
\par On a que $tr(A)=tr(A')$, donc $\frac{-2}{3} = 1 + 2\cos\theta$.
\par D'où $\cos\theta = \frac{-5}{6}$
\par Et donc $\theta=\pm\arccos\left(\frac{-5}{6}\right)$
\par Pour déterminer le signe, on fait le produit mixe de $w, e_2', r_{w, \theta}(e_2')$, qui vaut $\Vert w\Vert\sin \theta$ dans la base $\mathcal{B}'$.
\par Dans la base $\mathcal{B}$ d'origine, on a que leur produit mixte vaut $\left\vert\begin{matrix} 3 & \frac{1}{\sqrt{10}} & \frac{-5}{3\sqrt{10}} \\ 1 & \frac{-3}{\sqrt{10}} & \frac{8}{3\sqrt{10}} \\ 1 & 0 & \frac{-5}{3\sqrt{10}}\end{matrix}\right\vert>0$
\par Donc $\sin\theta>0$
\par Donc $\theta = +\arccos\left(\frac{-5}{6}\right)$
\end{Exe}


\section{Réduction des isométries dans un espace euclidien}
\Thr{}{Soit $E$ un espace euclidien et $u\in\mathcal{O}(E)$ alors il existe une base orthonormale $\mathcal{B}$ telle que $Mat_B(u)$ est diagonale par bloc et chaque bloc est de la forme :
\par $$\begin{matrix} [1] \\ [-1] \\ [R_\theta]\end{matrix}$$
\par pour $\theta\in\R$
\par ie chaque bloc est soit $I$, soit $-I$, soit une rotation.}
\begin{Rem}
Que l'isométrie soit directe ou non dépend du nombre de $-1$ dans cette matrice
\par Les seules valeurs propres réelles possibles pour une isométrie sont $1$ et $-1$ comme elle conserve la norme.
\end{Rem}
\Pre{\underline{Lemme préparatoire :} tout endomorphisme d'un $\R$-ev E de dimension finie possède une droite stable ou un plan stable.
\par Si $E$ un $\R$-ev de dimension finie, et $u\in\mathcal{L}(E)$ :\begin{itemize}
\item Si $u$ possède une valeur propre réelle $\lambda$
\par Alors on dispose de $x\in E, x\neq 0$ tel que $u(x)=\lambda x$
\par Donc $Vect(x)$ est stable par $u$
\item Si $u$ n'a pas de valeur propres réelles : la décomposition en facteurs premiers de $\pi_u$ dans $\R[X]$ ne comporte que des facteurs de degré 2 irréductibles.
\par $\pi_u = (X^2+aX+b)Q$ où $Q\in\R[X]$
\par On a donc $(u^2 + au + bid)\circ Q(u) =0$
\par Par l'absurde, si $\ker (u^2+au+bid) = \{0\}$, alors $u^2+au+bid$ serait injectif. Comme on est en dimension finie, il serait bijectif.
\par Donc $Q(u)=0$, et donc $Q$ est annulateur de $u$ avec $\deg Q < \deg \pi_u$
\par D'où la contradiction
\par On peut donc prendre $x\in E$ non-nul tel que $x\in\ker (u^2+au+bid)$
\par $(x, u(x))$ est librre car $u$ n'a pas de valeurs propres réelles, donc $F = Vect(x, u(x))$ est un plan, $u(x)\in F$ et $u(u(x))=-au(x)-bx$ donc dans $F$
\par Donc $F$ stable par $u$ 
\end{itemize}
Récurrence sur $n\in\N^*$, $H_n$: Si $E$ est un espace de dimension inférieure ou égale à $n$ alors il existe une BON $B$ de $E$ dans laquelle la matrice est de la bonne forme.
\par $H_1$ et $H_2$ sont vérifiées
\par Soit $\in \N^*$, on suppose $H_n$
\par Soit $E$ un espace euclidien de dimension $n+1$, soit $u\in\mathcal{O}(E)$ :\begin{itemize}
\item Si $u$ possède une valeur propre réelle $\lambda$ : cette valeur propre est $1$ ou $-1$. Alors $E_\lambda(u)$ est stable par $u$ donc $E_\lambda(u)^\perp$ est stable par $u^\star$ donc $E_\lambda(u)^\perp$ est stable par $u$
\par On prend $B_1$ une BON de $E_\lambda(u)$, et on note $u_1$ l'endomorphisme induit par $u$ sur $E_\lambda(u)$. Alors $Mat_{B_1}(u_1) = \lambda id$
\par Par $H_n$, on prend $B_2$ une BON de $E\lambda(u)^\perp$ telle que l'endomorphisme induit par $u$ sur $E_\lambda(u)^\perp$ a une matrice de la forme recherchée.
\par Donc dans la BON $B=(B_1, B_2)$, $Mat_{B}(u)$ est de la forme recherchée.
\item Si $u$ n'a pas de valeurs propres réelles : il existe un plan stable de $u$, donc soit $u$ induit sur ce plan est une symétrie orthogonale, soit c'est une rotation. Si c'était une symétrie orthogonale, sa matrice n'aurait que des $1$ et des $-1$ sur la diagonale dans une BON, mais $u$ n'a pas de valeurs propres réelles donc c'est la matrice d'une rotation qu'on doit utiliser et on concatène avec une BON de l'espace orthogonal (l'endomorphisme induit dessus est de la bonne forme par $H_n$)
\end{itemize}}


\section{Les endomorphismes auto-adjoints}
\Def{}{Soit $E$ un espace euclidien et $u\in\mathcal{L}(E)$.
\par On dit que $u$ est auto-adjoint si $u^\star = u$
\par ie $\forall x,y\in E, \langle u(x),y\rangle = \langle x, u(y)\rangle$}
\Prop{}{Soit $E$ un espace euclidien de dimension $n$, soit $\mathcal{B}$ une \underline{base orthonormale} de $E$, soit $u\in\mathcal{L}(E)$
\par $u$ est auto-adjoint si $Mat_B(u)\in\mathcal{S}_n(\R)$}
\begin{Rem}
On dit parfois que $u$ est symétrique pour désigner un endomorphisme auto-adjoint.
\end{Rem}
\begin{Exe}
Un projecteur orthogonal est un endomorphisme symétrique/auto-adjoint.
\end{Exe}

\Thr{Théorème spectral}{Tout endomorphisme auto-adjoint d'un espace euclidien est diagonalisable dans une BON.}
\Pre{\underline{Lemme :} si $E$ est euclidien et $u\in\mathcal{L}(E)$ auto-adjoint, alors $u$ possède une valeur propre réelle.
\par On considère $\mathcal{B}$ une base orthonormale de $E$ et $S=Mat_\mathcal{B}(u)\in\mathcal{S}_n(\R)$.
\par On considère $v$ l'endomorphisme de $\C^n$ canoniquement associé à $S$
\par Alors $\chi_v=\chi_S=\chi_u$
\par Soit $\lambda$ une valeur propre de $v$
\par On a donc $X\in\mathcal{M}_{n,1}(\C), X\neq 0$ tel que $SX=\lambda X$
\par $\overline{X}^T SX = \lambda \overline{X}^TX$
\par mais on a aussi $\overline{X}^TSX=(S\overline{X})^TX=(\bar{S}\bar{X})^TX=\bar{\lambda}\bar{X}^TX$
\par Et $\overline{X}^TX\in\R_+^*$
\par Donc $\lambda\in\R$
\par Or $v\in\mathcal{L}(\C^n)$ donc $\chi_v$ est scindé, donc $v$ possède au moins une valeur propre qui est réelle. Donc $S$ possède une valeur propre réelle.
\par Donc $u$ en possède une.
\par Par récurrence forte, montrons que $\forall n\geq 1$, $H_n$ : tout endomorphisme auto-adjoint d'un espace euclidien de dimension inférieure ou égale à $n$ est diagonalisable en BON.
\par $H_1$ est vérifiée.
\par Soit $n\in\N^*$, on suppose $H_n$
\par Soit $E$ un espace euclidien de dimension $n+1$ et $u\in\mathcal{S}(E)$ (l'ensemble des endormorphismes auto-adjoints/symétriques)
\par Soit $\lambda$ une valeur propre réelle de $u$ (qui existe d'après le lemme)
\par On note $F= E_\lambda(u)$
\par $F$ est stable par $u$, donc $F^\perp$ est stable par $u^\star$ donc par $u$ et l'endomorphisme $u'$ induit par $u$ sur $F^\perp$
\par $u'$ est auto-adjoint
\par $\dim F\geq 1$ donc $\dim F^\perp\leq n$
\par Donc par $H_n$, il existe $\mathcal{B'}$ une BON de $F^\perp$ telle que $Mat_{\mathcal{B}'}u'$ est diagonale.
\par On prend $\mathcal{B}_1$ une BON de $F$
\par $\mathcal{B}=(\mathcal{B}_1,\mathcal{B}')$ est une BON de $E$ et $Mat_\mathcal{B}$ est donc diagonale.
\par Ce qui conclut la récurrence.}

\Prop{théorème spectral matriciel}{Toute matrice symétrique réelle est orthogonalement semblable à une matrice diagonale.
\par ie $\forall S\in\mathcal{S}_n(\R), \exists D\in\mathcal{D}_n(\R), \exists P\in\mathcal{O}_n(\R), D = P^{-1}SP = P^TSP$}
\Pre{Soit $S\in\mathcal{S}_n(\R)$
\par Soit $u\in\mathcal{L}(\R^n)$ canoniquement associé à $S$, on munit $\R^n$ du produit scalaire canonique (ie sa base canonique est une BON).
\par On note $\mathcal{B}$ la base canonique de $\R^n$
\par $u$ est auto-adjoint.
\par Donc d'après le théorème spectral, $u$ est diagonalisable dans une BON $\mathcal{B}'$
\par La matrice $P$ de passage de $\mathcal{B}$ dans $\mathcal{B}'$ est orthogonale et on a :
\par $Mat_{\mathcal{B}'}(u) = P^{-1}SP = P^TSP$ où $Mat_{\mathcal{B}'}\in\mathcal{D}_n(R)$}

\section{Endomorphismes auto-adjoints et formes bilinéaires symétriques}
\Thr{}{Soit $E$ un espace euclidien et $\varphi$ une fbs sur $E$. Il existe un unique endomorphisme auto-adjoint $u$ tel que :
\par $$\forall x,y\in E, \varphi(x,y) = \langle x, u(y)\rangle$$}
\Pre{Soit $y\in E$, on note $\psi_y$ la forme linéaire $x\mapsto \varphi(x,y)$
\par Par théorème de représentation des formes linéaires, il existe un unique vecteur noté $u(y)$ tel que $x\mapsto \langle u(y), x\rangle =\psi_y$
\par On a montré l'existence d'une unique application de $E$ dans $E$ $u$ vérifiant $\forall x,y\in E, \langle u(x), y\rangle =\varphi(x,y)$
\par Montrons que $u$ est linéaire et $u=u^\star$
\par Soient $x, x'\in E$, soit $\lambda\in\R$, soit $y\in E$
\par $\langle u(\lambda x + x'), y\rangle = \varphi(\lambda x +x',y) = \lambda \varphi(x,y)+\varphi(x', y)$
\par $ = \lambda \langle u(x), y\rangle + \langle u(x'), y\rangle = \langle \lambda u(x)+u(x'),y\rangle$
\par Donc $\langle u(\lambda x+x')-\lambda u(x)-u(x'), y\rangle=0$
\par Donc $u(\lambda x+x')-\lambda u(x) - u(x')\in E^\perp$
\par Donc $u\in\mathcal{L}(E)$
\par De plus, $\forall x,y\in E, \varphi(x,y)=\varphi(y,x)$
\par Donc $\langle u(x), y\rangle =\langle x, u(y)\rangle$
\par Donc $u=u^\star$}
\Def{}{Soit $E$ un espace euclidien, $u\in\mathcal{S}(E)$ un endomorphisme auto-adjoint.
\par On dit que :\begin{itemize}
\item $u$ est positif si $(x,y)\mapsto \langle u(x),y\rangle$ est une forme bilinéaire symétrique positive. On note l'ensemble de ces endomorphismes $\mathcal{S}_+(E)$.
\item $u$ est défini positif si $(x,y)\mapsto \langle u(x), y\rangle$ est un produit scalaire. On note l'ensemble de ces endomorphismes $\mathcal{S}_{++}(E)$.
\end{itemize}}
\Def{}{Soit $S\in\mathcal{S}_n(\R)$.
\par On dit que :\begin{itemize}
\item $S$ est positive si $(X,Y)\mapsto X^T SY$ est une forme bilinéaire symétrique positive sur $\mathcal{M}_{n,1}(\R)$. On note l'ensemble de ces matrices $\mathcal{S}_n^+(\R)$.
\item $u$ est défini positif si $(X,Y)\mapsto X^TSY$ est un produit scalaire sur $\mathcal{M}_{n,1}(\R)$. On note l'ensemble de ces endomorphismes $\mathcal{S}_n^{++}(\R)$.
\end{itemize}}
\Thr{}{Soit $E$ un espace euclidien et $u\in\mathcal{S}(E)$ alors :\begin{itemize}
\item $u$ est positif si, et seulement si, $S_p(u)\subset\R_+$
\item $u$ est défini positif si, et seulement si, $S_p(u)\subset\R_+^*$
\end{itemize}}
\Pre{$u$ est auto adjoint
\par Par théorème spectral, on peut trouver une BON $(e_1,..., e_n)$ de vecteurs propres.
\par Les valeurs propres de $u$ $\lambda_1,...,\lambda_n$
\par $\forall x\in E, x=\sum\limits_{i=1}^nx_ie_i$
\par $\langle x, u(x)\rangle  = \langle \sum\limits_{i=1}^n x_ie_i, \sum\limits_{i=1}^n\lambda_ix_ie_i\rangle$
\par $=\sum\limits_{i=1}^n\lambda_i\langle x_i, x_i\rangle=\sum\limits_{i=1}^n\lambda_ix_i^2$
\par Ensuite, on raisonne par double implication :
\par Si $S_p(u)\subset R_+$ : Alors $\langle x, u(x)\rangle\geq 0$ comme on a une somme de termes positifs, donc $u$ est positif
\par Si $u$ est défini positif : Alors $\forall x\in E, \langle x, u(x)\rangle\geq 0$. En particulier, $\langle e_i, u(e_i)\rangle = \lambda_i \times 1^2$ (la base est normée), donc tous les $\lambda_i$ sont positifs ou nuls.
\par De même pour $u$ défini positif.}
\begin{Exe}
$E=\R^3$ avec $(e_1,e_2,e_3)$ la base canonique.
\par $\varphi$ la fbs définie par :
\par $$\forall x = x_1e_1+x_2e_2+x_3e_3, \varphi(x,x) =x_1^2 + 2x_2^2 + x_3^3+x_1x_2-x_1x_3$$
\par Pour se mettre dans le cadre, on munit $E$ du produit scalaire canonique. Déterminer si $\varphi$ est positive ou positive définie.
\par On pose $\varphi(x,x) = X^TSX$, et on obtient alors $S=\begin{pmatrix}1 & \frac{1}{2} & \frac{-1}{2} \\ \frac{1}{2} & 2 & 0 \\ \frac{-1}{2} & 0 & 1\end{pmatrix}$
\par On a $\chi_S =  X^3 - 4X^2 + (\frac{7}{4}+2+\frac{3}{4})X -(2-\frac{1}{2}-\frac{1}{4}) = X^3-4X^2+ \frac{9}{2}X - \frac{5}{4}$
\par On peut répondre à la question en dérivant le polynôme, en trouvant les racines de ce polynôme du second degré pour avoir un tableau de variations.
\par Cela permet de savoir que les racines sont strictement positives, et donc que $\varphi$ est un produit scalaire.
\end{Exe}


\section{Hors-programme}
\subsection{Racine carrées des matrices symétriques définies positives}
\Thr{}{Soit $S\in\mathcal{S}_n^{++}(\R)$, alors il existe une unique racine carrée définie positive de $S$.}
\Pre{Analyse : soit $s$ l'endomorphisme canoniquement associé à $S$ dans $\R^n$ muni du produit scalaire et de la base canonique.
\par On pose $\delta\in\mathcal{S}^{++}(\R)$ tel que $\delta\circ\delta=s$
\par On a alors $\delta\circ s = \delta^3=s\circ\delta$
\par Donc $s$ et $\delta$ commutent, et les espaces propres de $s$ sont stables par $\delta$.
\par Soit $\lambda\in S_p(s)$, on note $s_\lambda$ l'endomorphisme induit par $s$ sur $E_\lambda(s)$ et $\delta_\lambda$ l'endomorphisme induit par $\delta$ sur $E_\lambda(s)$
\par Prenons $\mu\in S_p(\delta_\lambda)$ et $x$ un vecteur propre de $\delta_\lambda$ associé à la valeur propre $\mu$
\par $s_\lambda(x) = \lambda x$ et $\delta_\lambda(x) = \mu x$
\par Donc $s_\lambda(x)=\delta_\lambda\circ\delta_\lambda(x) = \mu^2x$
\par Donc $\mu^2 = \lambda$
\par $\delta$ est définie positive donc $\mu>0$, d'où $\mu = +\sqrt{\lambda}$
\par $\delta_\lambda$ est auto-adjoint et diagonalisable, il a dnc $1$ valeur propre sur chaque espace propre.
\par Donc $\delta_\lambda (x) =\sqrt{\lambda}x$
\par Et comme $E = \bigoplus_{\lambda\in S_p(s)}E_\lambda(s)$, $\delta$ est unique.
\par Synthèse : Soit $s\in\mathcal{S}^{++}(\R^n)$, notons $\delta$ l'endomorphisme tel que $\forall\lambda\in S_p(s)\forall x\in E_\lambda(s), \delta(x)=\sqrt{\lambda}x$
\par On a bien que $\delta$ est un endomorphisme.
\par Pour $\lambda\in S_p(s)$, notons $\mathcal{B}_\lambda$ une base orthonormale de $E_\lambda(s)$.
\par $\mathcal{B}=(\mathcal{B}_{\lambda_1},...,\mathcal{B}_{\lambda_p})$ est une base orthonormale de $E$ telle que $Mat_\mathcal{B}(\delta)\in\mathcal{S}_n(\R)$
\par Donc $\delta$ est bien auto-adjoint.
\par On a de plus que $S_p(\delta) =\{\sqrt{\lambda}\vert \lambda\in S_p(s)\}$, donc $S_p(\delta)\subset\R_+^*$
\par Donc $\delta\in\mathcal{S}^{++}(\R^n)$}


\subsection{Matrices de Gramm d'une famille de vecteurs}
\Def{}{$E$ un $\R$-ev euclidien de dimension $n$ ou un espace préhilbertien.
\par $(u_1,..., u_p)$ une famille de $E$.
\par $G(u_1,..., u_p)  = (\langle u_i, u_j\rangle)_{(i,j)\in\llbracket 1,p\rrbracket^2}$
\par (immédiatement, $G\in S_p(\R)$)}
\Prop{}{$F=Vect(u_1,..., u_p)$ avec $\dim F = q \leq p$
\par $\mathcal{B}$ une BON de $F$, $\mathcal{B}=(f_1,...,f_q)$
\par $A =Mat_\mathcal{B}(u_1,...,u_p)\in\mathcal{M}_{q,p}(\R)$
\par Alors $A = (\langle u_j, f_i\rangle)_{(i,j)\in\llbracket 1,p\rrbracket\times\llbracket 1,q\rrbracket}$
\par De plus, $A^TA = G(u_1,..., u_p)$}
\Pre{$A^TA = \sum\limits_{k=1}^qA_{i,k}^TA_{k,j} = \sum\limits_{k=1}^qA_{k,i}A_{k,j}$
\par $ = \sum\limits_{k=1}^q\langle u_i, f_k\rangle\langle u_j, f_k\rangle = \langle u_i, u_j\rangle$
\par (puisque $f_1,...,f_q$ est une BON de $F$)}
\Prop{Corollaire}{$(u_1,...,u_p)$ et $G(u_1,...,u_p)$ sont de même rang.}
\Pre{Soit $A$ la matrice de $(u_1,..., u_p)$ dans une BON de $F$
\par $rg(A) = rg(u_1,..., u_p)$ et $G(u_1,..., u_p)=A^TA$
\par Soit $X\in\mathcal{M}_{p,1}(\R)$
\par $X\in \ker A\Rightarrow X\in\ker(A^TA)$
\par Réciproquement :
\par Si $X\in\ker(A^TA)$, alors $A^TAX = 0$
\par Donc $X^TA^TAX=0$
\par D'où $(AX)^T(AX)=0$ qui est le produit scalaire canonique de $AX$ avec lui-même.
\par Par définition du produit scalaire, $AX=0$ donc $X\in \ker A$
\par Donc $\ker A=\ker A^TA$
\par $A\in\mathcal{M}_{q,p}(\R)$ et $A^TA\in\mathcal{M}_p(\R)$
\par $a\in\mathcal{L}(\R^p,\R^q)$ canoniquement associée à $A$, $b\in\mathcal{L}(\R^p)$ canoniquement associée à $A^TA$
\par $\ker a=\ker b$ donc par théorème du rang, $rg (a)= rg (b)$
\par D'où le résultat.}
\Prop{}{$G(u_1,...,u_p)\in \mathcal{S}_p^+(\R)$
\par et $G(u_1,...,u_p)\in \mathcal{S}_p^{++}(\R)$ si, et seulement si, $(u_1,..., u_p)$ est libre}
\Pre{Rappel : $S\in\mathcal{S}_p(\R)$ : $S\in \mathcal{S}_p^+(\R) \Leftrightarrow S_p(S)\subset\R_+\Leftrightarrow \forall X\in\mathcal{p,1}(\R), X^TSX \geq 0$
\par On prend $A\in\mathcal{M}_{q,p}(\R)$ telle que $G(u_1,...,u_p) = A^TA$
\par Soit $X\in\mathcal{M}_{p,1}(\R)$, alors $X^TG(u_1,..., u_p)X = (AX)^T(AX)\geq 0$
\par Donc $G(u_1,..., u_p)\in\mathcal{S}_p^+(\R)$
\par Le rang donne l'équivalence.}
\begin{Rem}
On a le problème réciproque : Si $S\in\mathcal{S}_p^{++}(\R)$, $E$ un espace euclidien de dimension $n\geq p$ (ou un espace préhilbertien). Existe-t-il $p$ vecteurs de $(u_1,..., u_p)$ tels que $G(u_1,...,u_p)=S$ ?
\par On fixe $\mathcal{B}$ une BON de $E$ et on considère $s\in\mathcal{L}(E)$ tel que $Mat_\mathcal{B}(s) =S$
\par Comme $\mathcal{B}$ est une BON, alors $s\in S^{++}(E)$
\par Par théorème spectral, on considère $\mathcal{B}'=(e_1',..., e_p')$ une BON de $E$ des vecteurs propres de $s$.
\par Alors $Mat_{\mathcal{B}'} = Diag(\lambda_1,...,\lambda_p)$, et $\forall i\in\llbracket 1,p\rrbracket, \lambda_i>0$
\par Soit $\Delta = Diag(\sqrt{\lambda_1},...,\sqrt{\lambda_p})$
\end{Rem}


\subsection{Familles isogonales}
Dans un espace euclidien $E$
\Def{}{Une famille $(u_i)_{1\leq i\leq p}$ d'un espace eudlidien est dite isogonale si :
\par $$\left\{\begin{array}{l}\forall i\in\llbracket 1, p\rrbracket, \Vert u_i\Vert = 1 \\ \forall i,j\in\llbracket 1,p\rrbracket, i\neq j \Rightarrow \Vert u_i-u_j\Vert = d\end{array}\right.$$}
\begin{Exe}
On a $\Vert u_i-u_j\Vert^2=\Vert u_i\Vert^2 + \Vert u_j\Vert^2 - 2\langle u_i, u_j\rangle = 2 (1-\langle u_i, u_j\rangle) = d^2$
\par  On note $K = \langle u_i,u_j \rangle$ pour $i\neq j$, qui vaut donc $\frac{2-d^2}{2}$
\par On a $G(u_1,..., u_p) = S_K$ une matrice avec deux blocs $K$ au-dessus de la diagonale des $1$ (la matrice est symétrique).
\par Les valeurs propres de $S_K$ sont $1-k$, qui est d'ordre $p-1$. Comme la trace correspond à la somme des valeurs propres, on déduit que la dernière valeur propre est $x = p-(p-1)(1-K)$
\par $S_K$ est symétrique positive si, et seulement si, $1-K\geq 0$ et $p-(p-1)(1-K)\geq 0$
\par $$\Leftrightarrow \frac{-1}{p-1}\leq K\leq 1$$
\par Cas $K= \frac{-1}{p-1}$ :
\par $S_K$ est de rang $p-1$ (une valeur propre est nulle, mais toutes les autres sont positives), si $(u_1,..., u_p)$ est une rotation, $rg(u_1,..., u_p)=p-1$
\par Dans $E$ de dimension $n$, trouver $n+1$ vecteurs $(u_1,..., u_{n+1})$ des vecteurs tels que $G(u_1,..., u_{n+1})=S_n=\begin{pmatrix} 1 & \frac{-1}{n} \\ \frac{-1}{n} & 1\end{pmatrix}$
\par La matrice $S_n$ de $\mathcal{S}_n(\R)$ est symétrique définie positive, on peut bien trouver $(u_1,..., u_n)$ telle que $G(u_1,..., u_n)=S_n$
\par On cherche $u_{n+1}$ : $u_{n+1}=\sum\limits_{i=1}^n\alpha_iu_i$, où $\forall j\in\llbracket 1, n\rrbracket, \langle u_{n+1}, u_j\rangle =\frac{-1}{n}$
\par$= \sum\limits_{i=1}^n\alpha_i\langle u_i, u_j\rangle = \frac{-1}{n}\sum\limits_{1\leq i\leq n, i\neq j} \alpha_i + \alpha_j$
\par Si on prend $u_{n+1} = -\sum\limits_{i=1}^nu_i$, on a bien le résultat.
\par Vérifions que $\langle u_{n+1}, u_{n+1}\rangle= \langle -\sum\limits_{i=1}^nu_i, -\sum\limits_{i=1}^nu_i\rangle=\sum\limits_{1\leq i, j\leq n}\langle u_i, u_j\rangle =-\frac{1}{n}n(n-1) + n = 1$
\par On a donc cette formule pour les familles isogonales maximales.
\end{Exe}



\chapter{Equations différentielles linéaires}
Dans le cadre de ce chapitre, $E$ est un $\K$-ev de dimension finie ($\K$ est $\R$ ou $\C$) et $I$ est un intervalle de $\R$.
\par On s'intéresse aux équations différentielles dont les solutions sont des fonctions de $I$ dans $E$.
\begin{Exe}
$y'+xy = e^x$ est une équation différentielle linéaire du premier ordre, scalaire, normalisée (le coefficient devant $y'$ vaut $1$)
\par Ses solutions sont les fonctions $y\in\mathcal{D}(I,\K)$ telles que $\forall x\in\R, y'(x)+xy(x) =e^x$
\par $y' + xy^2=e^x$ n'est pas linéaire.
\par $y'y + 3y = e^x$ n'est pas linéaire non plus.
\par $\left\{\begin{array}{l} x'(t) = 3x(t) + t^2 y(t)+e^t \\ y'(t) = 2x(t) + 3y(t) - e^t\end{array}\right.$ est une équation différentielle linéaire, mais non scalaire.
\par $y'' +t^2y' + (e^t+1)y = \sin(t)$ est une équation différentielle linéaire du premier ordre scalaire normalisée.
\end{Exe}
\section{Définitions}
\Def{Premier ordre}{Soit $I$ un intervalle, soit $E$ un $\K$-ev de dimension finie.
\par Soit $a$ une application continue de $I$ dans $\mathcal{L}(E)$
\par Soit $b$ une application continue de $I$ dans $E$
\par On appelle $y' + a\cdot y = b$ équation différentielle linéaire du premier ordre normalisée.
\par Ses solutions sont les fonctions $y\in\mathcal{D}(I, E)$ vérifiant $\forall t\in I, y'(t)+a(t)\cdot y(t)=b(t)$}
\Def{Traduction matricielle}{Avec les notations ci-dessus, on fixe une base $\mathcal{B}$ de $E$.
\par $\forall t\in I, Y(t) = Mat_{\mathcal{B}}y(t) \Rightarrow Y'(t) = Mat_\mathcal{B}y'(t)\in\mathcal{M}_{n,1}(\K)$
\par $A(t) = Mat_\mathcal{B}a(t)\in\mathcal{M}_n(\K)$
\par $B(t) = Mat_\mathcal{B}b(t)\in\mathcal{M}_{n,1}(\K)$
\par L'équation devient :
\par $$Y'(t) = A(t)Y(t) +B(t)$$}
\begin{Exe}
$\left\{\begin{array}{l}x'(t) 2tx(t) + 3t^2y(t) + \sin(t) \\ y'(t) =  (3t+1)x(t)  + \frac{1}{1+t^2}y(t) + t\end{array}\right.$
\par Alors $Y'(t) =\begin{pmatrix} 2t & 3t^2 \\ (3t+1) & \frac{1}{1+t^2}\end{pmatrix}Y(t) +\begin{pmatrix}\sin(t) \\ t\end{pmatrix}$
\end{Exe}
\begin{Exe}
Exemple de vectorialisation d'une équation du second ordre :
\par $(E) : y''(t) + 3ty'(t) + \sin(t)y(t) = t^2$
\par On pose $Y(t) = \begin{pmatrix} y(t) \\ y'(t)\end{pmatrix}$
\par $Y$ solution de $(E)$ $\Leftrightarrow$ $Y'(t) = \begin{pmatrix} 0 & 1 \\ -\sin(t) & -3t \end{pmatrix}Y(t) + \begin{pmatrix} 0\\ t^2 \end{pmatrix}$
\par On transforme donc cette équation différentielle linéaire scalaire du second ordre en une équation différentielle linéaire vectorielle du premier ordre.
\end{Exe}
\Def{Equations différentielles linéaires normalisées scalaires d'ordre p}{L'équation $y^{(p)} + a_{p-1}y^{(p-1)}+...+a_0y = b$ où :
\par $\forall i\in\llbracket 0, p-1\rrbracket$, $a_i$ est une application continue de $I$ dans $\K$
\par et $b$ est une application continue de $I$ dans $\K$
\par est une équation différentielle différentielle linéaire normalisée scalaire d'ordre $p$
\par Ses solutions sont les fonctions $y\in\mathcal{D}_p(I, \K)$ vérifiant :\par $$\forall t\in I, y^{(p)}(t) + a_{p-1}(t)y^{(p-1)}(t)+...+a_0(t)y(t) =b(t)$$}
\Prop{Vectorialisation}{Toute équation différentielle linéaire normalisée scalaire (éqdlns) d'ordre $p$ est équivalente à une équation différentielle linéaire d'ordre $1$ dont les solutions sont à valeurs dans $\K^p$}
\Pre{$(E) : y^{(p)} + a_{p-1}y^{(p-1)}+...+a_0y = b$ une telle équation.
\par Pour $y\in\mathcal{D}_p(I, \K)$, on pose $z = (y, y', ..., y^{(p-1)})$
\par On a $z\in\mathcal{D}_1(I, \K)$.
\par $y$ est solution de $(E)$ si, et seulement si, $\forall t\in I, z'(t) = (y'(t),..., y^{(p)}(t)) = a(t)\cdot z(t) + b_2(t)$
\par où $a(t):\left\{\begin{array}{rcl} \K^p & \to & \K^p \\ (x_0,..., x_p) & \mapsto & (x_1,..., x_{{p-1}},-\sum\limits_{i=1}^{p-1}a_i(t)x_i)\end{array}\right.$
\par et $b_2(t) = (0,..., 0, b(t))\in\K^p$}
\Prop{Traduction matricielle}{On va noter $Y(t) = \begin{pmatrix} y(t) \\ . \\. \\.\\ y^{(p-1)}(t)\end{pmatrix}$, et dans ce cas la vectorialisation sera :
\par $$Y'(t) = \begin{pmatrix}y'(t)\\.\\.\\.\\y^{(p)}(t)\end{pmatrix} = \begin{pmatrix} 0 & 1 & 0 & . & 0 \\ 0 & 0 & 1 & . & 0 \\.&.&.&.&. \\ 0 & 0 & 0 & . & 1 \\ a_0 & -a_1 & -a_2 & . & -a_{p-1}\end{pmatrix}\begin{pmatrix} y(t)\\.\\.\\.\\ y^{(p-1)}(t)\end{pmatrix} +\begin{pmatrix}0 \\.\\.\\0\\b(t) \end{pmatrix}$$}

\section{Généralités sur l'équation du premier ordre}
\Def{Equation homogène associée}{Avec $(E_q) : y' + a\cdot y = b$
\par (où $a\in\mathcal{C}(I, \mathcal{L}(E))$ et $b\in\mathcal{C}(I,E)$)
\par L'équation homogène associée est :
\par $$(H): y' + a\cdot y = 0$$}
\Prop{}{$S_H$, l'ensemble des solutions de $(H)$, est un $\K$-ev.}
\Pre{On peut montrer que c'est un sous-espace vectoriel de $\mathcal{D}(I,E)$
\par En effet, $0$ est nécessairement solution de $(H)$
\par Et si on prend $y_1, y_2$ deux solutions et $\lambda\in\K$, par linéarité de $a(t)$ on obtient que $y_1+\lambda y_2$ est solution de $(H)$ aussi.}
\Prop{}{$S_E$, l'ensemble des solutions de $(E_q)$, est un espace affine.
\par ie si $y_0$ est une solution particulière de $(E_q)$, alors on a que $S_{E_q} = y_0+S_H = \{y_0 + y_H\vert y_H\in S_H\}$}
\Pre{Soit $y_0$ une solution de $(E_q)$
\par Soit $y\in\mathcal{D}(I, E)$
\par $y$ est solution $\Leftrightarrow$ $\forall t\in I, y'(t)+a(t)\cdot y(t) = y_0'(t)+a(t)\cdot _0(t)$
\par $$\Leftrightarrow\forall t\in I, (y-y_0)'(t) + a(t)\cdot ((y-y_0)(t))=0$$
\par $$\Leftrightarrow y-y_0\in S_H$$}
\Thr{Superposition}{$(E_{q_1})$ et $(E_{q_2})$ deux équations de même équation homogène associé et $\lambda\in\K$ :
\par $(E_{q_1}) : y' + a(t)\cdot y = b_1$
\par $(E_{q_2}) : y' + a(t)\cdot y = b_2$
\par $$(E_{q_+}) : y' + a(t)\cdot y = b_1+b_2$$
\par $$(E_{q_\lambda}) : y' + a(t)\cdot y = \lambda b_1$$
\par Si $y_1\in S_{E_{q_1}}$ et $y_2\in S_{E_{q_2}}$, alors :
\par $$ y_1+y_2\in S_{E_{q_+}}$$
\par et$$ \lambda y_1 \in S_{E_{q_\lambda}}$$}
\Pre{Immédiate, en se servant du fait que les ensembles de solutions sont des espaces affines.}
\Thr{Cauchy-Lipschitz linéaire}{Soit $I$ un intervalle de $\R$
\par Soit $E$ un $\K$-ev de dimension finie
\par Soit $(E_q)$ une équation différentielle linéaire normalisée $y'+a\cdot y = b$
\par Soit $t_0\in I$, $y_0\in E$, alors :
\par Il existe une unique solution $f$ de $(E_q)$ vérifiant :
\par $$f(t_0)=y_0$$}
\begin{Rem}
Toutes les conditions sont importantes :
\par \underline{Si on n'est pas sur un intervalle} : $(E_q) y'=0$ sur $\R^*$ a comme solutions les fonctions qui valent une constante sur $\R_-^*$ et une autre constante sur $\R_+^*$. Il y a donc un $\R$-ev de dimiension $2$ de solutions.
\par \underline{Si l'équation n'est pas normalisée (ou normalisable)} : $(E_q) = t^3y'-y=0$ (on cherche des solutions réelles)
\par Sur $I=\R_+^*$ ou $\R_-^*$, l'équation est normalisable et $S_I = \{t\mapsto \alpha e^{-\frac{1}{2t^2}}\vert \alpha\in\R\}$
\par Quelles sont les solutions sur $\R$ ? On procède par analyse-synthèse.
\par Soit $f$ une solution sur $\R$, alors $f_{\vert\R_+^*}$ et $f_{\vert\R_-^*}$ sont solutions.
\par Donc il existe $\alpha, \beta\in\R$ tels que : $\forall t\in\R_+^*, f_{\vert\R_+^*}(t) =\alpha e^{-\frac{1}{2t^2}}$ et $\forall t\in\R_-^*, f(t) = \beta e^{-\frac{1}{2t^2}}$
\par Par continuité de $f$ en $0$, on a $f(0)=0$, et donc aucune condition en plus.
\par Synthèse : Pour tout $\alpha, \beta\in\R$, l'application $f:t\mapsto \left\{\begin{array}{rl}\alpha e^{-\frac{1}{2t^2}} & t>0 \\ \beta e^{-\frac{1}{2t^2}} & t<0 \\ 0 & t=0 \end{array}\right.$ est solution de $(E_q)$
\par Si on ajoute $t_0, y_0\in\R$ tel que $f(t_0)=y_0$ alors : si $t_0<0$ alors on peut toujours faire varier $\alpha$, si $t_0>0$ on pourra faire varier $\beta$ ; dans les deux cas on aura une infinité de solutions. Si $t_0=0$ et $y_0\neq 0$, on n'a aucune solution. Donc il n'y a ni existence ni unicité de la solution.
\end{Rem}
\begin{Exe}
Exemple d'équation non-linéaire : $(E_q) : y' = 1+y^2$ sur $\R$
\par Soit $f$ une solution de $(E_q)$ sur un intervalle $I$
\par Alors $\frac{f'}{1+f^2}=1$
\par Donc il existe $K\in\R$ tel que $\forall t\in I, \arctan f(x) = x-k$
\par Donc $\forall x\in I, f(x) =\tan (x-k)$
\par Les solutions ne sont pas un espace affine.
\end{Exe}
\Prop{Corollaire}{Si $I$ est un intervalle et $(H) : y' = a\cdot y$ une équation différentielle linéaire \textbf{normalisée}
\par L'ensemble des solutions de $(H)$ sur $I$ à valeurs dans $E$ est un $\K$-ev de dimension $\dim E$}
\Pre{On fixe $t_0$ et on prend $\varphi:\left\{\begin{array}{rcl} S_H(I) & \to & E \\ f & \mapsto & f(t_0)\end{array}\right.$
\par $\varphi$ est un isomorphisme, donc $\dim E = \dim S_H(I)$}
\Def{Wronskien}{Soit $I$ un intervalle, $E$ un $\K$-ev de dimension $p$, $\mathcal{B}$ une base de $E$
\par Soit $(H)$ une équation différentielle linéaire homogène normalisée $(H) : y' + a\cdot y=0$
\par Pour $(u_1,..., u_p)$ une famille de $p$ fonctions solutions de $(H)$, on définit leur wronskien relativement à la base $\mathcal{B}$ :
\par $$w(u_1,..., u_p) :\left\{\begin{array}{rcl}I & \to & E \\ t & \mapsto & \det_\mathcal{B}(u_1(t),..., u_p(t))\end{array}\right.$$
\par (Ou $w(u_1,...,u_p) = \det_\mathcal{B}(u_1,..., u_p)$)}
\Prop{}{$w(u_1,..., u_p)=0$ si, et seulement si, $\exists t\in I, w(u_1(t),...,u_p(t))=0$}
\Pre{Si $\forall t\in I, w(u_1(t),..., u_p(t)) = 0$, alors il existe $t\in I$ tel que $w(u_1(t),..., u_p(t))=0$
\par Réciproquement, supposons qu'il existe $t_0\in I$ tel que $\det_\mathcal{B}(u_1(t_0),...,u_p(t_0)) =0$
\par On a alors $(u_1(t_0),...,u_p(t_0))$ famille liée de $E$
\par On a donc $\lambda_1,..., \lambda_p\in\K$ non tous nuls tels que $\lambda_1u_1(t_0) +...+\lambda_pu_p(t_0)=0$
\par Donc $\lambda_1u_1+...+\lambda_pu_p$ est une solution de $(H)$ qui s'annule en $t_0$
\par Donc par Cauchy-Lipschitz, $\lambda_1u_1+...+\lambda_pu_p=0$ (comme $0$ est déjà solution qui s'annule en $t_0$ et qu'on a l'unicité de la solution du problème de Cauchy)
\par Donc $\forall t\in I, (u_1(t),..., u_p(t))$ est liée.}
\Prop{Autre formulation}{$(u_1,...,u_p)$ est une base de $S_H$ si, et seulement si, $\exists t\in I, (u_1(t),...,u_p(t))$ est une base de $E$.
\par $\Leftrightarrow \forall t\in I, (u_1(t),...,u_p(t))$ est une base de $E$}
\begin{Exe}
$(H) : t^2y'' + ty' + y = 0$ sur $\R_+^*$ :
\par $\R_+^*$ est un intervalle, $(H)$ est une équation différentielle linéaire normalisable donc $S_H$ est un $\K$-ev de dimension 2.
\par Cherchons les solutions de la forme $t\mapsto t^\alpha$.
\par On constate que $f_1:t\mapsto t^i = e^{i\ln(t)}$ et $f_2:t\mapsto t^{-i} = e^{-i\ln(t)}$ sont solutions.
\par Pour $t\in\R_+^*$, $w(f_1(t), f_2(t)) = \left\vert\begin{matrix}f_1(t) & f_2(t) \\  f_1'(t) & f_2'(t)\end{matrix}\right\vert = \left\vert\begin{matrix}e^{i\ln(t)} & e^{-i\ln(t)} \\ \frac{i}{t}e^{iln(t)} & \frac{-i}{t}e^{-i\ln(t)}\end{matrix}\right\vert = \frac{-2i}{t}$
\par Donc $w(f_1,f_2)\neq 0$
\par $f_1$ et $f_2$ sont donc indépendantes
\par Donc dans $\C$, $S_H(\R_+^*)=Vect(f_1, f_2) = \{\lambda_1f_1 + \lambda_2f_2 \vert \lambda_1,\lambda_2\in\C\}$
\par Pour les solutions à valeurs réelles, on peut prendre $g_1=\frac{f_1+f_2}{2} =\cos(\ln(t))$ et $g_2 =\frac{f_1-f_2}{2i}=\sin(\ln(t))$ qui sont deux solutions indépendantes et à valeurs réelles, donc une base des solutions réelles.
\end{Exe}
\Thr{Variation des constantes}{Soit $I$ un intervalle, $(E_q)$ une équation différentielle linéaire normalisée $y'=a\cdot y + b$
\par $(H)$ l'équation homogène associée.
\par Si $(u_1,...,u_p)$ est une base de $S_H$
\par Alors $(E_q)$ possède une solution particulière de la forme $t\mapsto =\lambda_1(t) +...+\lambda_p(t)u_p(t)$
\par où $\lambda_1,...,\lambda_p$ sont des fonctions dérivables à valeurs dans $\K$}
\Pre{$(u_1,...,u_p)$ base de $S_H$, pour $\lambda_1,...,\lambda_p\in\mathcal{D}(I,\K)$, on considère $v = \lambda_1u_1+...+\lambda_pu_p$
\par $v$ est dérivable et $v' =\lambda_1'u_1 + \lambda u_1' + ...+\lambda_p'u_p + \lambda_pu_p'  = \sum\limits_{i=1}^p(\lambda_i'u_i +\lambda_iu_i')$
\par $(1)$ : $v$ est solution de $(E_q)$ si, et seulement si, $\forall t\in I, \sum\limits_{i=1}^p (\lambda_i'(t)u_i(t) + \lambda_i(t)u_i'(t)) = a(t)\cdot \left(\sum\limits_{i=1}^p \lambda_i(t)u_i(t)\right) + b(t)$
\par $= \sum\limits_{i=1}^p\lambda_i(t)a(t)u_i(t)+b(t)$
\par $(1)\Leftrightarrow \sum\limits_{i=1}^p\lambda_i'(t)u_i(t) = b(t)$
\par ce qui vient du fait que $\forall i\in\llbracket 1,p\rrbracket, u_i'(t) = a(t)\cdot u_i(t)$
\par On a vu que $\forall t\in I, (u_1(t),...,u_p(t))$ est une base de $E$
\par Donc pour $t\in I$, on peut écrire $b(t) = \sum\limits_{i=1}^pb_i(t)u_i(t)$ où $(b_1(t),..., b_p(t))\in\K^p$ sont les coordonnées de $b(t)$ dans la base $(u_1(t),..., u_p(t))$
\par Donc $(1) \Leftrightarrow \left\{\begin{array}{rcl} \lambda_1' & = & b_1 \\ . & . & . \\ . & . & . \\ \lambda_p' & = & b_p\end{array}\right.$
\par On admet que les $b_i$ sont continus, et alors le système a une solution.}
\begin{Exe}
Avec $(E_q) : y'' + y = \frac{1}{\cos(t)}$ sur $]-\frac{\pi}{2},\frac{\pi}{2}[$, cherchons les solutions réelles.
\par Les solutions de $(H)$ ($y''+y=0$) sont $S_H = \{t\mapsto \alpha\cos(t)+\beta\sin(t) \vert \alpha, \beta\in\R\}$
\par On recherche une solution particulière $y$ de $(E)$ telle que :
\par $\forall t\in ]-\frac{\pi}{2}, \frac{\pi}{2}[, y(t) = \alpha(t)\cos(t) + \beta(t)\sin(t)$ et $y'(t) = \alpha(t)(-\sin(t))+\beta(t)\cos(t)$
\par On a alors que $\alpha'(t)\cos(t) + \beta'(t)\sin(t) = 0$
\par Et $y''(t) = \alpha(t)(-\cos(t)) + \beta(t)(-\sin(t)) + \alpha'(t)(-\sin(t))+\beta'(t)\cos(t)$
\par Et on a alors que $\alpha'(t)(-\sin(t))+\beta'(t)\cos(t)=\frac{1}{\cos(t)}$
\par Donc $\begin{pmatrix}\alpha' \\ \beta'\end{pmatrix} = \begin{pmatrix} \cos t & -\sin t \\ \sin t & \cos t\end{pmatrix}\begin{pmatrix} 0 \\ \frac{1}{\cos t}\end{pmatrix}$
\par Donc $\alpha'(t) = -\tan(t)$ et $\beta'(t) = 1$
\par Et donc, à constante près : $\alpha (t) = \ln(\cos t)$ et $\beta(t) = t$
\end{Exe}

\Thr{Résolution de l'équation différentielle linéaire normalisée à coefficients constants}{Notons $\forall t\in I,y'(t) = a\cdot y(t)$, où $a\in\mathcal{L}(E)$
\par L'ensemble des solutions de cette équation est : $\{t\mapsto \exp(t a)\cdot x\vert x\in E\}$
\par plus précisément, la solution du problème de Cauchy $\left\{\begin{array}{l} y' = a\cdot y \\ y(t_0) = x_0\end{array}\right.$ est $t\mapsto \exp((t-t_0)a)\cdot x_0$
}
\Pre{Rappel sur $\exp(ta-t_0a)\cdot x_0 = (\exp(ta)\circ(\exp(-t_0a)))\cdot x_0$ : pour le démontrer, on passe aux matrices, et on le prouve coordonnées par coordonnées en utilisant un binôme de Newton et un produit de Cauchy.
\par Si $\varphi:\left\{\begin{array}{rcl} \R & \to & \R^p \\ t & \mapsto & \begin{pmatrix} f_1(t) \\ f_2(t)\\.\\f_p(t)\end{pmatrix}\end{array}\right.$, on dit que $\varphi$ est dérivable si toutes ses coordonnées le sont.
\par Notons $f:\left\{\begin{array}{rcl}\R & \to & \mathcal{L}(E) \\ t & \mapsto & \exp(ta)\end{array}\right.$
\par Il faut montrer que $f$ est dérivable et que $f':t\mapsto (\exp(ta)\circ a)x = a\cdot f(t)$, puis dans une deuxième partie montrer que toutes les équations différentielles linéaires à coefficients constants ont cette solution.
\par Utilisons d'abord le théorème de dérivation terme à terme :
\par On note $f(t) = \sum\limits_{i=1}^{+\infty}u_n(t)$
\par $u_n = \frac{t^n}{n!}a^n\in\mathcal{C}^1(I, E)$
\par $\sum u_n$ converge simplement
\par $\sum u_n'$ converge uniformément
\par Donc $f$ est $\mathcal{C}^1$ et $f'=\sum u_n'$
\par $\Vert .\Vert$ une norme d'algèbre, $\forall t\in\R, \Vert \frac{t^n}{n!}a^n\Vert \leq \frac{(\Vert t\Vert \Vert a\Vert)^n}{n!}$
\par On a $\forall t\in\R, \forall n\in\N^*, u_n'(t) =\frac{t^{n-1}}{n!}a^n$
\par Et donc $\forall [\alpha,\beta]\subset \R,\forall t\in[\alpha,\beta], \forall n\in\N^*, \Vert u_n(t)\Vert \leq \frac{\max(\vert \alpha\vert,\vert\beta\vert)\Vert a\Vert^n}{n!}$ 
\par Donc $f'(t) = \sum\limits_{n=1}^{+\infty}\frac{t^{n-1}}{(n-1)!}a^n=a\circ\exp(ta)=\exp(ta)\circ a$
\par Prouvons le reste : posons $\psi:\left\{\begin{array}{rcl} \R & \to & E \\ t &\mapsto & \varphi(t)\cdot x\end{array}\right.$ pour $\varphi:\R\to\mathcal{L}(E)$
\par Alors $\psi$ est dérivable et $\forall t\in\R, \psi'(t) = \varphi'(t)\cdot x$, donc $\varphi$ admet un DL au premier ordre.
\par $\psi(x+h) = \varphi(t+h)\cdot x = (\varphi(t)+h\varphi'(t)+h\varepsilon(h))\cdot x$
\par $ = \varphi(t)\cdot x + h\varphi'(t)\cdot x + h\varepsilon(h)\cdot x$
\par et $\Vert \varepsilon(h)\cdot x\Vert \leq \vert\Vert\varepsilon(h)\Vert\vert\Vert x\Vert$ qui tent vers 0 quand $h$ tend vers $0$}
\begin{Exe}
$\left\{\begin{array}{rcl} x' & = & 2x+y+z \\ y' &=&x+2y+z \\ z'& = & x+y+2z\end{array}\right.$
\par On a $Y'=AY$ avec $A=\begin{pmatrix} 2 & 1 & 1 \\ 1 & 2 & 1 \\ 1 & 1 & 2\end{pmatrix}$ et $Y=\begin{pmatrix} x \\ y\\ z\end{pmatrix}$
\par On a immédiatement que $1$ est vp d'ordre $2$, donc $4$ est vp d'ordre $1$ et $A$ est diagonalisable.
\par $D$ est semblable à $A$ avec $D = \begin{pmatrix} 1 & 0 & 0 \\ 0&1&0 \\ 0&0&1\end{pmatrix}$, avec la matrice de passage $P=\begin{pmatrix} 1 & 1 & 1 \\ -1 & 0 & 1\\ 0 & -1 & 1\end{pmatrix}$
\par On aura $A = PDP^{-1}$, et donc $\exp(tA) = P\begin{pmatrix} \exp(t) & 0 & 0 \\ 0 & \exp(t) & 0 \\ 0 & 0 & \exp(4t)\end{pmatrix}P^{-1}$
\par On peut poser $Y = PZ$, avec $Z = \begin{pmatrix} u \\ v\\ w\end{pmatrix}$
\par Alors $PZ' = APZ \Leftrightarrow Z' = P^{-1}APZ = DZ$
\par Donc $Z = \exp(tD)\cdot X$
\par Donc $Y = P\begin{pmatrix} \alpha e^t \\ \beta e^t \\ \gamma e^{4t}\end{pmatrix}$
\end{Exe}

\section{Topo sur les exponentielles}
\subsection{Rappel sur les familles sommables}
Soit $(a_i)_{i\in I}\in \C^I$ avec $I$ dénombrable est. $(a_i)$ est sommable si $\sum\vert a_i\vert < +\infty$
\par Alors pour toute bijection $\sigma:\N\to I$, on a $\sum\limits_{i\in I} a_i = \sum\limits_{n\in\N}a_{\sigma(n)}$ ($\sum\limits_{n\in\N} a_{\sigma(n)}$ ne dépend pas de la bijection $\sigma$)
\par Si $(I_j)_{j\in J}$ est une partition de $I$ dénombrable, et $(a_i)_{i\in I}\in \C^I$, si $\sum\limits_{i\in I}\vert a_i\vert<+\infty$, alors $\sum\limits_{i\in I} a_i = \sum\limits_{j\in J}\sum\limits_{k\in I_j} a_k$
\par Extension pour $(a_i)\in E^I$ où $E$ un $\K$-ev de dimension finie, où $\Vert.\Vert$ est une norme (équivalente aux autres normes comme on est en dimension finie) :
\par Si $\sum\limits_{i\in I}\Vert a_i\Vert<+\infty$, alors la famille est sommable et pour toute bijection $\sigma:\N\to I$, on a que $\sum\limits_{n\in\N}a_{\sigma(n)}$ ne dépend pas de la bijection choisie et $\sum\limits_{i\in I} a_i = \sum\limits_{n\in\N}a_{\sigma(n)}$
\par On le prouve en prenant une base $(e_1,..., e_p)$ de $E$ et en se munissant de la $\Vert.\Vert_{\infty}$
\par $\forall i\in I, a_i = [a_i]^1e_1+...+[a_i]^pe_p$ et $\forall i\in I, \forall j\in\llbracket 1, p\rrbracket, \vert [a_i]^j\vert\leq \Vert a_i\Vert_\infty$

\subsection{Exponentielle dans une algèbre de dimension finie}
\Def{Exponentielle}{Si $E$ est une algèbre de dimension finie, on note :
\par $$\exp(u) = \sum\limits_{n\in\N}\frac{1}{n!}u^n$$
\par On munit $E$ d'une norme d'algèbre, alors $\Vert\frac{1}{n!}u^n\Vert \leq \frac{\Vert u\Vert^n}{n!}$ et donc $\sum\limits_{n\in\N}\frac{1}{n!}u^n$ est sommable.}
\Prop{Morphisme d'algèbre}{Si $u,v\in E$ tels que $uv = vu$
\par $\left(\frac{1}{p!}u^p\frac{1}{q!}v^q\right)_{p,q\in\N}$ est sommable et :
\par $$\exp(u)\exp(v) = \sum\limits_{p,q\in\N^2} \frac{1}{p!}u^p\frac{1}{q!}u^q = \sum\limits_{s\in\N}\frac{1}{s!}\sum\limits_{p+q=s}\frac{s!}{p!q!}u^pv^q$$
\par $$=\sum\limits_{s\in\N}\frac{1}{s}\sum\limits_{p+q=s}\binom{s}{p}u^pv^q = \sum\limits_{s\in\N}\frac{1}{s!}(u+v)^s = \exp(u+v)$$}
\Prop{Changement de base}{Si $M\in\mathcal{M}_n(\K), P\in\mathcal{GL}_n(\K)$ :
\par Alors $\exp(P^{-1}MP)=P^{-1}\exp(M)P$ }
\Pre{On voit l'égalité pour tous les rangs ($P$ et $P^{-1}$ se compensent) et ensuite on se sert de la continuité de $M\mapsto P^{-1}M$}
\Def{Série entière à coefficient matriciel}{On définit l'application :
\par $\varphi\left\{\begin{array}{rcl}\R & \to & \mathcal{M}_n(\K) \\ t & \mapsto & \exp(tA)=\sum\limits_{n=0}^{+\infty}\frac{t^n}{n!}A^n\end{array}\right.$
\par On a $\varphi'(t) = \sum\limits_{n=1}^{+\infty}\frac{nt^{n-1}}{n!}A^n = \sum\limits_{n=0}^{+\infty}A\frac{t^n}{n!}A^n = A\exp(tA) = \exp(tA)A$}




\chapter{Calcul différentiel}\subsection{Conditions du chapitre}
Les fonctions considérées dans ce chapitre sont définies sur un ouvert d'un $\R$-espace vectoriel normé de dimension finie et à valeurs dans un  $\R$-espace vectoriel normé de dimension finie.
\par Dans tout ce chapitre, sauf mention contraire, $E$ et $F$ désignent deux $\R$-espaces vectoriels normés de dimension finies dont les normes sont notées $\Vert.\Vert$. $U$ désigne un ouvert de $E$.
\par Si nécessaire, $E$ sera euclidien.

\subsection{Rappels sur la continuité}
\Def{Limite}{Pour $l\in F$ et $a\in\bar{A}$ avec $A\subset E$, $f\to_a l$ si $\forall \varepsilon\in\R_+^*, \exists \alpha\in \R_+^*, \forall x\in A, x\in\mathcal{B}(a,\alpha)\Rightarrow f(x)\in\mathcal{B}(l, \varepsilon)$}
\Prop{}{En dimension finie, une fonction respecte une propriété locale si, et seulement si, toutes ses fonctions coordonnées les respectent.}
\begin{Rem}
Pour déterminer si une fonction $f$ est continue en $a$, on se sert de $f(x, \lambda x)$ en le faisant tendre vers $a$ pour obtenir une limite.
\par Si $f$ est continue, c'est la seule limite et on peut trouver une majoration par quelque chose qui tend vers 0.
\par Sinon, on cherche à trouver une autre limite (par exemple avec $f(a - (x, x+x^{2023}))$)
\end{Rem}
\begin{Exe}
Soit $f:(x, y)\mapsto \frac{2y}{(x^2+y^2)}$ et vaurt $0$ en $(0,0)$
\par Déjà, $f$ n'est pas continue en $0$ : pour $\lambda_1,\lambda_2\in\R_+, \lambda_1\neq\lambda_2$ :
\par $\lim\limits_{x\to 0}f(x,\lambda_1 x) = \frac{\lambda_1}{1+\lambda_1}\neq\frac{\lambda_2}{1+\lambda_2}=\lim\limits_{x\to 0}f(x, \lambda_2x)$ 
\end{Exe}
\Def{o}{Pour $E, F, G$ $\R$-ev de dimension finie, $A\subset E$.
\par Soit $f:A\to F$ et $g:A\to G$
\par On dit que :
\par $$f =_a o(g)\text{ si } \exists\varepsilon\in\mathcal{F}(A,\R_+), \varepsilon\to_a 0 \text{ et } \Vert f\Vert =\varepsilon\Vert g\Vert$$
\par Si $g$ ne s'annule pas au voisinage de $a$, on peut écrire :
\par $$f =_a o(g)\text{ si }\frac{\Vert f\Vert}{\Vert g\Vert}\to_a 0$$}
\Prop{}{Si on a $x = (x_1,..., x_n)$, $\alpha_1,..., \alpha_n\in\N$ et $\gamma <\alpha_1+\alpha_2+...+\alpha_n$, alors :
\par $$(x_1^{\alpha_1} x_2^{\alpha_2}...x_n^{\alpha_n})=_0 o(\Vert x\Vert^\gamma)$$}


\section{Applications différentiables}
\subsection{Dérivée selon un vecteur}
\Def{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$, $a$ un point de $U$ et $v$ un vecteur de $E$.
\par On dit que $f$ admet une dérivée selon le vecteur $v$ au point $a$ si la fonction $t\mapsto f(a+tv)$ est dérivable en $0$.
\par Si elle existe, cette dérivée est appelée dérivée de $f$ selon le vecteur $v$ au point $a$ et est notée $D_vf(a)$ :
\par $$D_vf(a) = \lim\limits_{t\to 0}\frac{f(a+tv)-f(a)}{t}$$}
\begin{Exe}
Soit $f$ une fonction définie d'un ouvert $U$ dans $F$, $a$ un point de $U$. La fonction $f$ admet une dérivée selon le vecteur $0$ au point $a$ et $D_0f(a) = 0$
\end{Exe}
\begin{Exe}
$f:(x,y)\mapsto x^2y + e^{xy}$
\par Trouver la dérivée en $(0,0)$ suivant $(1, 1)$ :
\par Pour $t\in\R$, posons $g =f(t,t) = t^3 + e^{t^2}$
\par Cette fonction est dérivable de dérivée $t\mapsto 3t^2 + 2te^{t^2}$
\par Donc $D_{(1,1)}f(0,0) = g'(0) = 0$
\end{Exe}
\begin{Rem}
La fonction $t\mapsto f(a+tv)$ est définie pour tout $t\in\R$ tel que $a+tv\in U$. Comme $U$ est un ouvert de $E$ contenant $a$, elle est bien définie sur une voisinage de $0$ dans $\R$.
\end{Rem}
\begin{Rem}
Une fonction peut avoir des dérivées selon tout vecteur en un point et ne pas être continue en ce point.
\end{Rem}
\begin{Rem}
Si $f$ possède une dérivée suivant $v$ en $a$, alors $f$ possède une dérivée selon  $\alpha v$ pour tout $\alpha\in \R^*$
\par On suppose que $f$ possède une dérivée en $a$ suivant $v$ :
\par La fonction $g:t\mapsto f(a+tv)$ est dérivable en $0$ et $D_vf(a) = g'(0)$
\par Considérons $h:t\mapsto f(a+t\alpha u) = g(\alpha t)$
\par $h$ est dérivable en $0$ et $h'(0) = \alpha g'(0)$
\par Donc $D_{\alpha v}f(a) = \alpha D_vf(a)$
\end{Rem}
\begin{Exe}
Soit $f:(x,y)\mapsto \frac{xy}{(x^2+y^2)^2}$ et qui vaut $0$ en $(0,0)$.
\par Cette fonction n'est pas continue en $0$ : $\lim_{x\to 0} f(x, x) = +\infty$
\par Cette fonction admet des dérivées en $(0,0)$ suivant $(1,0)$ et $(0,1)$ :
\par $D_{(1,0)}f(1,0) = 0$ et $D_{(0, 1)}f(0, 0) = 0$
\end{Exe}
\begin{Exe}
Soit $f:(x, y)\mapsto \frac{x^2y}{(x^4+y^2)}$ et qui vaut $0$ en $(0,0)$
\par Pour tout $x\in\R$, on a $f(x, x^2) = \frac{x^4}{2x^4}=\frac{1}{2}\to_0\frac{1}{2}$
\par Donc $f$ n'est pas continue en $(0,0)$
\par Si $(\alpha,\beta)\in\R^2\neq (0,0)$ :
\par $f(t\alpha,t\beta) = \frac{t^3\alpha^3\beta}{t^4\alpha^4+t^2\beta^2}$
\par Si $\beta = 0$ : $\forall t\in\R, f(t\alpha, t\beta)=0$ donc $D_{(\alpha, 0)}f(0,0)$ existe et vaut $0$
\par Sinon : $f(t\alpha,t\beta) =t\frac{\alpha^2}{\beta} + o(t)$
\par Donc $D_{(\alpha, \beta)}f(0,0)$ existe et vaut $\frac{\alpha^2}{\beta^2}$
\end{Exe}
\begin{Rem}
On constate que même si une fonction admet une dérivée suivant n'importe quel vecteur en un point, ça ne veut pas dire qu'elle est continue en ce point.
\end{Rem}

\subsection{Dérivées partielles}
\Def{}{On suppose ici que $E = \R^n$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a=(a_1,..., a_n)$ un point de $U$.
\par Si elle existe, on appelle $i$-ème dérivée partielle de $f$ en $a$ la dérivée de $f$ selon le vecteur $(0,...,0, 1, 0,...,0)$ (avec le $1$ en $i$-ème place) au point $a$
\par et on note $\dfrac{\partial f}{\partial x_i}(a)$ ou $\partial_i f(a)$}
\Def{}{Soit $\mathcal{B}=(e_1,..., e_n)$ une base de $E$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a$ un point de $U$.
\par Si elle existe, on appelle $i$-ème dérivée partielle de $f$ dans $a$ dans la base $\mathcal{B}$ la dérivée de $f$ selon le vecteur $e_i$ au point $a$
\par et on note $\dfrac{\partial f}{\partial x_i}(a)$ ou $\partial_if(a)$}
\begin{Rem}
Lorsqu'une base $\mathcal{B}=(e_1,..., e_n)$ de $E$ est fixée, on sait que l'application :
\par $$\Phi:\left\{\begin{array}{rcl}\R^n &\to & E \\ (x_1,..., x_n) & \mapsto & \sum\limits_{i=1}^n x_ie_i\end{array}\right.$$
\par est un isomorphisme. À une application $f$ de $U$ dans $E$ on peut associer l'application :
\par $$f_\mathcal{B} :\left\{\begin{array}{rcl}\Phi^{-1}(U) & \to & F \\ (x_1,..., x_n) & \mapsto & f\left(\sum\limits_{i=1}^nx_ie_i\right)\end{array}\right.$$
\par Comme $\Phi$ est une application linéaire et que $\R^n$ est de dimension finie, $\Phi$ est continue et donc $\Phi^{-1}(U)$ est un ouvert de $\R$.
\par On autorise l'identification entre $f$ et $f_\mathcal{B}$, et donc on s'autorisera à identifier $f(x)$ et $f(x_1,..., x_n)$ où $x= \sum\limits_{i=1}^nx_ie_i$
\end{Rem}
\begin{Rem}
Si $E = \R^n$ et que $\mathcal{B}$ est la base canonique de $\R^n$ alors $f = f_\mathcal{B}$ et la $i$-ème dérivée partielle de $f$ en $a$ est la $i$-ème dérivée partielle de $f$ en $a$ dans la base canonique $\mathcal{B}$ (i.e. la dérivée selon le $i$-ème vecteur de la base canonique)
\end{Rem}

\subsection{Différentielle}
\Def{}{Soit $f$ un efonction définie d'un ouvert $U$ dans $F$ et $a$ un point de $U$.
\par On dit que $f$ est différentiable au point $a$ s'il existe une application $u\in\mathcal{L}(E,F)$ telle qu'au voisinage de $0$ :
\par $$f(a+h) = f(a) + u(h) + o(h)$$
\par Dans ce cas, une telle application linéaire $u$ est unique, on la note $df(a)$ et on l'appelle différentielle de $f$ en $a$.
\par On l'appelle aussi application linéaire tangente à $f$ en $a$.}
\begin{Rem}
La notation $o(h)$ désigne une fonction négligeable devant $\Vert h\Vert$
\end{Rem}
\Prop{}{Il y a unicité de la différentielle d'une application différentiable.}
\Pre{Supposons l'existences de deux applications linéaires $u$ et $v$ telles que pour $h$ dans un certain voisinage de $0$, on ait :
\par $$ f(a+h) - f(a) = u(h) + o(h) = v(h)+o(h)$$
\par Considérons un vecteur non nul $x\in E$ et $t$ un réel positif.
\par On obtient pour $t$ dan sun voisinage à droite de $0$ :
\par $$(u-v)(tx) = \Vert tx\Vert\varepsilon(t)$$
\par Avec $\varepsilon\to_00$
\par On en déduit immédiatement que $\Vert (u-v)(x)\Vert = \Vert x\Vert\Vert \varepsilon(t)\Vert$
\par Par passage à la limite en $9$, on obtient $(u-v)(x)=0$.
\par La fonction $u-v$ s'annule donc sur $E$, donc $u=v$}
Note : pour alléger l'écriture, il est d'usag ede noter $df(a)\cdot h$ au lieu de $(df(a))(h)$.
\par On utilise parfois $df_a(h)$, mais le problème de cette notation, c'est qu'elle ne conïncide pas bien avec la notation dans $\R$   
\begin{Rem}
On dit que $f$ admet un développement limité d'ordre $1$ en $a$ s'il existe $u\in\mathcal{L}(E, F)$ telle qu'au voisinage de $0$ :
\par $$f(a+h)=f(a)+u(h)+o(h)$$
\par Une fonction $f$ est différentiable au point $a$ si, et seulement si, elle admet un développement limité d'ordre 1 en $a$.
\par Dans ce cas, $h\mapsto f(a)+df(a)\cdot h$ est une approximation affine de $f$ au voisinage de $a$.
\end{Rem}
\begin{Rem}
Comme la notion de continuité, la notion de différentiabilité en un point est une notion locale :
\par Soit $f$ une fonction définie sur un ouvert $U$, $a\in U$ et $V$ un ouvert de $U$ contenant $a$.
\par La fonction $f$ est différentiable en $a$ si, et seulement si, $f_{|V}$ est différentiable en $a$.
\end{Rem}
\Prop{}{Si $E=\R$, soit $f$ une fonction définie de l'ouvert $U$ dans $F$ et $a\in U$. La fonction $f$ est différentiable en $a$ si, et seulement si, $f$ est dérivable en $a$ et dans ce cas $df(a)$ est l'application $h\mapsto hf'(a)$
\par On a en particulier $f'(a) = df(a)\cdot 1$.}
\Pre{C'est une des définitions de la dérivabilité : $f$ est dérivable en $a$ si, et seulement si, $f$ possède un développement limité à l'ordre $1$ en $a$ :
\par $$f(a+h) = f(a)+\lambda h+o(h)$$
\par $h\mapsto \lambda h$ est linéaire. La définition de la dérivabilité est donc celle de la différentiabilité dans le cas particulier d'un $\R$-espace vectoriel de dimension $1$}
\Def{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$. Si $f$ est différentiable en tout point de $U$, on dit que $f$ est différentiable sur $U$ et on appelle différentielle de $f$ sur $U$ l'application :
\par $$df:\left\{\begin{array}{rcl}U & \to & \mathcal{L}(E,F) \\ a & \mapsto & df(a)\end{array}\right.$$}
\begin{Rem}
Si $f$ est différentiable :
\par $$df\in\mathcal{F}(U, \mathcal{L}(E, F))$$
\par $\forall a\in U, df(a)\in\mathcal{L}(E,F)$
\end{Rem}
\begin{Exe}
$$f:\left\{\begin{array}{rcl}\mathcal{M}_n(\R)^2 & \to & \mathcal{M}_n(\R) \\ (A, B) & \mapsto & AB\end{array}\right.$$
\par Soit $(A, B)\in\mathcal{M}_n(\R)^2$
\par $f((A,B)+(H,K)) = (A+H)(B+K) = AB+HB+AK+HK$
\par On a que $(H,K)\mapsto HB+AK\in\mathcal{L}(\mathcal{M}_n(\R)^2, \mathcal{M}_n(\R))$.
\par Il suffit alors de montrer que $HK = o(\Vert H\Vert)$ pour obtenir que $f$ est différentiable en $(A,B)$
\par On prend $\Vert.\Vert$ une norme d'algèbre sur $\mathcal{M}_n(\K)$ et on prend la norme infinie associée sur $\mathcal{M}_n(\K)\times\mathcal{M}_n(\K)$ (le max des normes d'algèbres des deux)
\par $\frac{\Vert HK\Vert}{\Vert (H, K)\Vert}\leq\frac{\Vert H\Vert\Vert K\Vert}{\max(\Vert H, \Vert K)}\leq \Vert(H, K)\Vert$
\par Donc $HK = o(\Vert (H, K)\Vert)$
\par Donc $f$ est différentiable en $AB$ et $df(A,B)\cdot(H,K) = HB+AK$
\end{Exe}
\Prop{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
Si $f$ est différentiable en $a$, alors $f$ est continue en $a$.}
\Pre{Si $f$ est différentiable en $a$, il existe une application linéaire $u$ (continue car $F$ est de dimension finie) telle que, pour $h$ dans un voisinage de 0 :
\par $$f(a+h)-f(a) = u(h)+o(h)$$
\par Par continuité de $u$, $f(a+h)-f(a)\to_{h\to 0}0$
\par Donc par définition, $f$ est continue en $a$}
\Prop{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
\par Si $f$ est différentiable en $a$, alors $f$ est dérivable en $a$ selon tout vecteur $v\in E$ et :
\par $$D_vf(a) = df(a)\cdot v$$}
\Pre{Supposons $f$ différentiable en $a$
\par Soit $v$ un vecteur de $E$, alors pour tout $t$ réel dans un voisinage de $0$, on a :
\par $$f(a+tv)(a)=f(a)+df(a)\cdot(tv)+o(t) = f(a)+tdf(a)\cdot v+o(t)$$
\par Donc $t\mapsto f(a+tv)$ est dérivable en $0$ de dérivée $df(a)\cdot v$
\par Ce qui conclut la preuve.}
\begin{Rem}
La réciproque est fausse.
\end{Rem}
\Prop{Corollaire}{Soit $\mathcal{B}=(e_1,..., e_n)$ une base de $E$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
\par Si $f$ est différentiable en $a$, alors $f$ admet des dérivées partielles (dans la base $\mathcal{B}$) et pour $v=\sum\limits_{i=1}^nv_ie_i\in E$ :
\par $$D_vf(a) = df(a)\cdot f = \sum\limits_{i=1}^nv_i\partial_if(a)$$}
\Pre{Pour $i\in\llbracket 1, n\rrbracket$, la $i$-ème dérivée partielle est la dérivée suivant le vecteur $e_i$.
\par On déduit immédiatement de la proposition précéddente l'existence des $n$ dérivées partielles et par linéarité de $df(a)$, pour tout vecteur $v=\sum\limits_{i=1}^nv_ie_i$ :
\par $$D_vf(a) = df(a)\cdot\left(\sum\limits_{i=1}^nv_ie_i\right) = \sum\limits_{i=1}^nv_idf(a)\cdot e_i=\sum\limits_{i=1}^nv_i\partial_if(a)$$}

\subsection{Matrice jacobienne}
\Prop{}{Soit $\mathcal{B}=(e_1,..., e_m)$ une base de $E$ et $\mathcal{B}'=(e_1',..., e_n')$ une base de $F$.
\par Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ différentiable sur $U$. Notons $f_1,..., f_n$ les fonctions composantes de $f$ dans la base $\mathcal{B}'$.
\par Soit $a\in U$. La matrice dans les bases $\mathcal{B}$ et $\mathcal{B}'$ de l'application linéaire $df(a)$ est :
\par $$J_f(a) = \begin{pmatrix} \partial_1f_1(a) & \partial_2f_1(a) & \cdot\cdot\cdot & \partial_mf_1(a)
\\ \partial_1f_2(a) & \partial_2f_2(a) & \cdot\cdot\cdot & \partial_mf_2(a)
\\ . & . & . & .
\\ . & . & . & .
\\ . & . & . & .
\\ \partial_1f_n(a) & \partial_2f_n(a) & \cdot\cdot\cdot & \partial_mf_n(a)\end{pmatrix}$$}
\Pre{Pour tout $j\in\llbracket 1,n\rrbracket$ :
\par $$df(a)\cdot (e_j) =\sum\limits_{i=1}^ndf_i(a)\cdot(e_j)e_i' = \sum\limits_{i=1}^n\partial_jf_i(a)e_i'$$}
\Def{Matrice Jacobienne}{Si $E=\R^m$ et $F=\R^n$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ différentiable sur $U$.
\par Soit $a\in U$. La matrice, dans les bases canoniques, de l'application linéaire $df(a)$ est :
\par $$J_f(a) = \begin{pmatrix} \dfrac{\partial f_1}{\partial x_1}(a) & \dfrac{\partial f_1}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_1}{\partial x_m}(a)
\\ \dfrac{\partial f_2}{\partial x_1}(a) & \dfrac{\partial f_2}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_2}{\partial x_m}(a)
\\ . & . & . & .
\\ . & . & . & .
\\ . & . & . & .
\\ \dfrac{\partial f_n}{\partial x_1}(a) & \dfrac{\partial f_n}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_n}{\partial x_m}(a)\end{pmatrix}$$
\par et est appelée matrice jacobienne de $f$ en $a$.}
\begin{Exe}
$$\varphi:\left\{\begin{array}{rcl}\R^2 & \to & \R^2 \\ (r,\theta) & \mapsto & (r\cos\theta, r\sin\theta) = (x, y)\end{array}\right.$$
\par C'est une application surjective. On admet qu'elle est différentiable, et on calcule sa jacobienne :
\par $\dfrac{\partial x}{\partial r} = \cos\theta$
\par $\dfrac{\partial y}{\partial r} = \sin\theta$
\par $\dfrac{\partial x}{\partial \theta} =-r\sin\theta$
\par $\dfrac{\partial y}{\partial \theta} =r\cos\theta$
\par Donc $J_\varphi(r, \theta) = \begin{pmatrix} \cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta \end{pmatrix}$
\par Pour ne pas se tromper en l'écrivant, on peut écrire :
\par avec $(r,\theta)\mapsto x(r, \theta)$, on prend $dx = \dfrac{\partial x}{\partial r}dr + \dfrac{\partial x}{\partial \theta}d\theta$, qui est la somme des expressions sur la ligne $x$, et les éléments différentiels $dr, d\theta$ indiquent dans quelle colonne ils vont.
\par On note aussi $dy = \dfrac{\partial y}{\partial r}dr + \dfrac{\partial y}{\partial \theta}d\theta$
\par Et alors $\begin{pmatrix} dx \\ dy\end{pmatrix} = J_\varphi(r,\theta)\begin{pmatrix}dr \\d\theta\end{pmatrix}$
\end{Exe}


\section{Opérations sur les applications différentiables}
\subsection{Différentielle d'une combinaison linéaire d'applications différentiables}
\Prop{}{Soit $f$ et $g$ deux fonctions définies d'un ouvert $U$ dans $F$, différentiables sur $U$. Soit $\lambda, \mu$ deux réels.
\par La fonction $\lambda f +\mu g$ est différentiable sur $U$ et :
\par $$d(\lambda f+\mu g) = \lambda df+\mu dg$$}
\Pre{Soit $a\in U$
\par Soit $h\in E$ dans un voisinage de $0$
\par \begin{align*}\lambda f +\mu g)(a+h)& = (\lambda (f(a)+df(a)\cdot h + o(h)) + \mu (g(a)+dg(a)\cdot h + o(h))) \\ &= (\lambda f + \mu g)(a) +(\lambda df+\mu dg)(a)\cdot h + o(h)\end{align*}
\par Donc $\lambda f + \mu g$ est différentiable en $a$ de différentielle en $a$ $(\lambda df+\mu dg)(a)$.}

\subsection{Différentielle de B(f,g) où B est bilinéaie et f et g sont deux applications différentiables}
\Prop{}{Soit $F, G, H$ des espaces vectoriels normés de dimension finie. Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par Il existe $C\in\mathcal{R}_+$ tel que :
\par $$\forall (x, y)\in F\times G, \Vert B(x, y)\Vert\leq C\Vert x\Vert\Vert y\Vert$$}
\Pre{En dimension finie, une application bilinéaire est continue (chaque composante est polynômiale en les coordonnées).
\par Le produit cartésien des disques unités de $F$ et $G$ est compact comme produit de compacts.
\par Donc $B$ est bornée par une constantce $C$ sur ce compact.
\par On en déduit :
\par Pour $(x,y)\in F\times G$ non-nuls, $\left\Vert B\left(\frac{1}{\Vert x\Vert}x\right)\left(\frac{1}{\Vert y\Vert}y\right)\right\Vert\leq C$
\par Soit, en utilisant la bilinéarité de $B$ et la positivité de la norme: :
\par $$ \Vert B(x, y)\Vert\leq C\Vert x\Vert\Vert y\Vert$$
\par L'égalité reste valide pour $x$ ou $y$ nuls.
\par Ce qui conclut la preuve.}
\Prop{}{Soit $E, F, G, H$ quatre espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$. Soit $f$ (respectivement $g$) une fonction définie de $U$ dans $F$ (respectivement $G$) et différentiable sur $U$.
\par Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par La fonction $B(f, g):x\mapsto B(f(x), g(x))$ est différentiable sur $U$ et :
\par $$d(B(f, g)) = B(df, g)+B(f, dg)$$}
\Pre{Soit $a\in U$
\par Soit $h\in E$ dans un voisinage de $0$.
\par $$B(f(a+h), g(a+h)) = B(f(a)+df(a)\cdot h+o(h), g(a)+dg(a)\cdot h+o(h))$$
\par $$B(f(a), g(a))+B(df(a)\cdot h, dg(a)\cdot h)+o(h)$$
\par Ce qui conclut la preuve.
\par On a utilisé la proposition précédente pour montrer que les termes restants étaient bien négligeables devant $h$. Par exemple il existe une constante $C$ et une constante $M$ telles que :
\par $$ \Vert B(df(a)\cdot h, dg(a)\cdot h)\Vert\leq C\Vert df(a)\cdot h\Vert\Vert dg(a)\cdot h\Vert\leq M\Vert h\Vert^2$$}
\begin{Rem}
La proposition s'étend à la différentiation de $M(f_1,..., f_n)$ lorsque $M$ est $n$-linéaire et que les $f_i$ sont différentiables.
\end{Rem}

\subsection{Différentielle d'une composée d'applications différentiables}
\Prop{}{Soit $E,F, G$ trois espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$, différentiable sur $U$. Soit $g$ une fonction définie de $V$ dans $G$, avec $V$ un ouvert de $F$ contenant $f(U)$, différentiable sur $V$.
\par La fonction $g\circ f$ est différentiable sur $U$ et, pour tout $a\in U$,
\par $$d(g\circ f)(a) = dg(f(a))\circ df(a)$$}
\Pre{Soit $a\in U$
\par Soit $h\in E$ dans un voisinage de $0$
\par \begin{align*}(g\circ f)(a+h) &= g(f(a+h))\\
&=g(f(a)+df(a)\cdot h+o(h))\\
&=g(f(a))+dg(f(a))\cdot (df(a)\cdot h+o(h))+o(h)\\
&=g(f(a))+(dg(f(a))\circ df(a))\cdot h+ dg(f(a))\cdot (o(h)) +o(h)\\
&=g(f(a))+(dg(f(a))\circ df(a))\cdot h+o(h)\end{align*}
\par On a bien le résultat annoncé.}
\begin{Rem}
Avec les notations ci-dessus, si $f$ est à valeurs réelles et $g$ une fonction définie sur un intervalle réel, la formule se ré-écrit :
\par $$d(g\circ f)(a) =g'(f(a))f'(a)$$
\end{Rem}
\Prop{Cas d'une algèbre}{Si $F$ est une algèbre
\par Soient $f,g$ deux applications différentiables en $a$ définies de $U$ dans $F$
\par Alors $fg$ est différentiable en $a$ et :
\par $$d(fg):h\mapsto (df(a)\cdot h)g(a) + f(a)(dg(a)\cdot h)$$}
\Pre{Soit $h\in E$ :
\par $\varepsilon_1,\varepsilon_2$ des fonctions de $E$ dans $\R$ qui tendent vers $0$ quand $h$ tend vers $0$
\par \begin{align*} fg(a+h)&=f(a+h)g(a+h)\\
&=(f(a)+df(a)\cdot h+\varepsilon_1(h)\Vert h\Vert)(g(a)+dg(a)\cdot h+\varepsilon_2(h)\Vert h\Vert)\\
&= f(a)g(a) +(df(a)\cdot h)g(a) + f(a)(dg(a)\cdot h)\\
&+ \varepsilon_1(h)\Vert h\Vert[g(a) + (dg(a)\cdot h)]+\varepsilon_2(h)\Vert h\Vert[f(a)+df(a)\cdot h]\\
&=fg(a) + (df(a)\cdot h)g(a)+f(a)(dg(a)\cdot h)+o(h)\end{align*}}

\subsection{Règle de la chaîne}
\Prop{Règle de la châine}{Soit $m,p,n$ des entiers naturels non nuls.
\par Soit $f$ une application différentiable sur $U$ un ouvert de $\R^m$ et à valeurs dans $\R^p$.
\par Soit $g$ une application différentiable sur $V$ un ouvert de $\R^p$ contenant $f(U)$ et à valeurs dans $\R^n$.
\par On note $f_1,..., f_p$ les fonctions composantes de $f$ et on pose $h = g\circ f$.
\par On a donc le schéma suivant :
\par $$\begin{array}{ccccc} \R^m & \to^f & \R^p & \to^g & \R^n \\ (x_1,...,x_m) & \mapsto & (f_1,..., f_p) & \mapsto & (g_1,..., g_n)\end{array}$$
\par Soit $x=(x_1,..., x_m)\in U$. Pour $j\in \llbracket 1, m\rrbracket$ et $i\in\llbracket 1,n\rrbracket$ :
\par Avec des jacobiennes, on a $J_{g\circ f}(x) = J_g(f(x))J_f(x)$ et donc :
\par $J_g(f(x)) = \begin{pmatrix} \cdot&\cdot&\cdot \\ \dfrac{\partial g_i}{\partial f_1}(f(x)) & \cdot\cdot\cdot&\dfrac{\partial g_i}{\partial f_p}(f(x)) \\ \cdot&\cdot&\cdot\end{pmatrix} \begin{pmatrix} \cdot\cdot\cdot & \dfrac{\partial f_1}{\partial x_j}(x) & \cdot\cdot\cdot \\ . & .&.\\ \cdot\cdot\cdot & \dfrac{\partial f_p}{\partial x_j}(x)& \cdot\cdot\cdot\end{pmatrix}$
\par Ce qui donne bien :
\par $$\dfrac{\partial (g\circ f)_i}{\partial x_j}(x) = \sum\limits_{k=1}^p\dfrac{\partial g_i}{\partial f_k}(f(x))\cdot\dfrac{\partial f_k}{\partial x_j}(x)$$
\par $$\partial_jh(x)=\sum\limits_{k=1}^p\partial_kg(f(x))\partial_jf_k(x)$$
\par Ou encore sous forme plus mnémotechnique :
\par $$\dfrac{\partial h}{\partial x_j}(x) =\sum\limits_{i=1}^p\dfrac{\partial f_k}{\partial x_j}(x)\dfrac{\partial g}{\partial f_k}(f(x))$$ 
\par Et enfin de manière encore plus concise mais abusive (les $g_i$ désignent à la fois les fonctions coordonnées de $g$ et de $g\circ f$ et les points d'application ne sont pas indiqués) :
\par pour tout $i\in\llbracket 1, n\rrbracket$ et $j\in\llbracket 1,m\rrbracket$ :
\par $$\dfrac{\partial g_i}{\partial x_j}=\sum\limits_{i=1}^p\dfrac{\partial f_k}{\partial x_j}\dfrac{\partial g_i}{\partial f_k}$$}
\Pre{On note $(e_1,..., e_m)$ la base canonique de $\R^m$ et $(u_1,..., u_p)$ la base canonique de $\R^p$
\par Soit $j\in\llbracket 1, m\rrbracket$ et $x\in\R^m$
\par $$dh(x)\cdot e_j = d(g\circ f)(x)\cdot (e_j) = dg(f(x))\cdot (df(x)\cdot e_j)$$
\par On écrit $df(x)\cdot e_j$ dans la base canonique de $\R^p$ :
\par $$df(x)\cdot e_j = \sum\limits_{k=1}^p(df_k(x)\cdot e_j)u_k$$
\par Et par linéarité de $dg(f(x))$ :
\par $$dh(x)\cdot e_j = \sum\limits_{i=1}^p(df_k(x)\cdot e_j)dg(f(x))\cdot(u_k)$$
\par Ce qui est exactement la formule proposée.
\par On pouvait aussi, si on veut éviter le raisonnement intrinsèque, écrire le produit des jacobiennes des applications.}
\begin{Exe}
Exprimons la composée(/divergence) en polaire :
$$\begin{array}{ccccc}\R^2 & \to & \R^2 & \to &\R \\(r,\theta) & \mapsto & (x,y) = (r\cos\theta, r\sin\theta) & \mapsto &f(x,y)\end{array}$$
\par On note $\varphi:\left\{\begin{array}{rcl}\R^2 & \to & \R^2 \\ (r,\theta) & \mapsto & (r\cos\theta, r\sin\theta) = (x, y)\end{array}\right.$ et $f:\left\{\begin{array}{rcl}\R^2 & \to & \R \\ (x,y) & \mapsto &f(x, y)\end{array}\right.$
\par On suppose les deux différentiables (on montrera plus tard que $\varphi$ l'est)
\par Pour $r,\theta\in\R^2$ :
\par $\dfrac{\partial (f\circ \varphi)}{\partial r}(r,\theta) = \dfrac{\partial f}{\partial x}\dfrac{\partial \varphi}{\partial r}(r,\theta) + \dfrac{\partial f}{\partial y}\dfrac{\partial y}{\partial r} = \dfrac{\partial f}{\partial x}(x,y)\cos\theta+\dfrac{\partial f}{\partial y}(x,y)\sin\theta$
\par $\dfrac{\partial (f\circ \varphi)}{\partial \theta}(r,\theta) = \dfrac{\partial f}{\partial x}(x,y)(-r\sin\theta)+\dfrac{\partial f}{\partial y}(x,y)r\cos\theta$
\end{Exe}


\section{Applications de classe $\mathcal{C}^1$}
\subsection{Définition et caractérisation}
\Def{}{Une application $f$ d'un ouvert $U$ dans $F$ est dite de de classe $\mathcal{C}^1$ si elle est différentiable sur $U$ et si $df$ est continue sur $U$
\par (i.e. $\begin{array}{rcl} U & \to & \mathcal{L}(E,F) \\ x & \mapsto & df(x)\end{array}$ continue)}
\Thr{}{Soit $\mathcal{B}$ une base de $E$. Soit $f$ une application définie d'un ouvert $U$ dans $F$.
\par L'application $f$ est de classe $\mathcal{C}^1$ sur $U$ si, et seulement si, les dérivées partielles relativement à la base $\mathcal{B}$ existent en tout point de $U$ et sont continues sur $U$.}
\Pre{Hors-programme :
\par Si $f$ est $\mathcal{C}^1$ :
\par Alors $f$ admet une dérivée suivant un vecteur quelconque $u$ en tout point $x$, $df(x)\cdot u$.
\par L'application $x\mapsto df(x)\cdot u$ est la composée de $df:x\mapsto df(x)$ et de $\varphi:l\mapsto l(u)$ ($l\in\mathcal{L}(E,F)$)
\par $df$ est continue (comme $f$ est $\mathcal{C}^1$) et $\varphi$ est continue (linéaire en dimension finie).
\par Donc $\varphi\circ df$ est continue, et donc pour tout point $x$, les dérivées partielles selon tout élément d'une base sont continues.
\par Réciprouquement, on suppose que $f$ admet $p$ dérivées suivant les vecteurs $e_1,..., e_p$ continues sur $U$.
\par Soit $x = x_1e_1+...+x_pe_p$ et $h=h_1e_1+...+h_pe_p$ avec $x+h\in U$
\par \begin{align*}f(x+h)-f(x)&= f(x_1+h_1,...,x_p+h_p) - f(x_1, x_2+h_2,..., x_p+h_p)\\
&+f(x_1, x_2+h_2,..., x_p+h_p) -f(x_1,, x_2, x_3+h_3, ...,x_p+h_p)\\
&+...\\
&+ f(x_1, ..., x_{p-1}, x_p+h_p) -f(x)\\
&= h_1\partial_1f(x_1, x_2+h_2,..., x_p+h_p) + h_1\varepsilon_1(h_1)\\
&+h_2\partial_2f(x_1, x_2,x_3+h_3,..., x_p+h_p) + h_2\varepsilon_2(h_2)\\
&+ ...\\
&+h_p\partial_pf(x_1,..., x_p)+h_p\varepsilon_p(h_p)
\end{align*}
\par où $\varepsilon_1,..., \varepsilon_p$ sont des fonctions qui tendent vers $0$ en $0$.
\par Par hypothèse, $\forall i\in\llbracket 1,p\rrbracket$, $\partial_if$ est continue.
\par Donc $\partial_1f(x_1,x_2+h_2,..., x_p+h_p) = \partial_1f(x_1,..., x_p)+\varphi_1(h_1, h_2,..., h_p)$ avec $\varphi_1\to_00$
\par On raisonne de même pour les autres dérivées partielles.
\par Donc $f(x+h)-f(x) =\sum\limits_{i=1}^ph_i\partial f_i(x) + \sum\limits_{i=1}^ph_i(\varepsilon_i(h_i) +\varphi_i(h))$
\par Donc c'est la somme d'une application linéaire en $h$ et d'un $o(h)$.
\par Donc $f$ est différentiable, et $\forall x\in U, \forall h\in E, df(x)\cdot h = \sum\limits_{i=1}^ph_i\partial_if(x)$
\par $df$ est une application de $U$ dans $\mathcal{L}(E,F)$
\par C'est la somme des applications $g_i:x\mapsto (h\mapsto h_i\partial_if(x))$, composées de projections en dimension finie et de $\partial_if$ qui est continue.
\par Donc $df$ est $\mathcal{C}^1$, donc $df$ est continue.}
\begin{Rem}
Dire que $df$ est continue, c'est dire que $J_f$ est continue, donc que chacune des applications coordonnées de $J_f$ est continue.
\end{Rem}
\begin{Exe}
$$f:\left\{\begin{array}{rcl} \R^2 & \to & \R^2 \\ (x,y) & \mapsto & (2x^3ye^{x+y},\arctan(xy))\end{array}\right.$$
\par Montrer que $f$ est différentiable :
\par $f$ admet deux dérivées partielles sur $\R^2$ :
\par Pour $(x,y)\in\R^2$ :
\par $\dfrac{\partial f}{\partial x}(x,y)=\left(2x^2ye^{x+y}(3+x), \frac{y}{1+x^2y^2}\right)$
\par $\dfrac{\partial f}{\partial y}(x,y) = \left(2x^3e^{x+y}(1+y), \frac{x}{1+x^2y^2}\right)$
\par $(x, y)\mapsto \dfrac{\partial f}{\partial x}(x,y)$ est continue par opérations usuelles.
\par $(x, y)\mapsto \dfrac{\partial f}{\partial y}(x,y)$ est continue par opérations usuelles.
\par Donc $f$ est $\mathcal{C}^1$.
\par $$J_f(x) = \begin{pmatrix}2x^2ye^{x+y}(3+x) & 2x^3e^{x+y}(1+y) \\ \frac{y}{1+x^2y^2} & \frac{x}{1+x^2y^2} \end{pmatrix}$$
\end{Exe}
\subsection{Opérations algébriques sur les applications de classe C1}
\Prop{}{Soit $f$ et $g$ deux fonctions d'un ouvert $U$ dans $F$ de classe $\mathcal{C}^1$. Soit $\lambda, \mu$ deux réels.
\par La fonction $\lambda f+\mu g$ est de classe $\mathcal{C}^1$ sur $U$.}
\Prop{}{Soit $E,F,G,H$ quatre espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^1$.
\par Soit $g$ une fonction définie de $U$ dans $G$ de classe $\mathcal{C}^1$.
\par Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par La fonction $B(f,g):x\mapsto B(f(x), g(x))$ est de classe $\mathcal{C}^1$ sur $U$.}
\Pre{Immédiat : 
\par Pour $i\in\llbracket 1, n\rrbracket$, la $i$-ème dérivée partielle de $\lambda f+\mu g$ existe et vaut $\lambda\partial_if + \mu\partial_ig$
\par Elle est donc continue par opération usuelle.}
\Prop{}{Soit $f$ et $g$ deux applications de classe $\mathcal{C}^1$ d'un ouvert $U$ dans $\R$. La fonction $fg$ est $\mathcal{C}^1$.}
\Pre{Immédiat :
\par Pour $i\in\llbracket 1, n\rrbracket$, la $i$-ème dérivée partielle de $fg$ existe et vaut $(\partial_if)g + f\partial_ig$
\par Elle est donc continue par opération usuelle.}
\Prop{}{Supposons que $F$ est un espace vectoriel euclidien. Soit $f$ et $g$ deux applications de classe $\mathcal{C}^1$ de $U$ dans $F$.
\par La fonction $\left\{\begin{array}{rcl} U & \to & \R \\ t & \mapsto & \langle f(t), g(t)\rangle\end{array}\right.$ est $\mathcal{C}^1$}
\Pre{Immédiat :
\par Pour $i\in\llbracket 1, n\rrbracket$, la $i$-ème dérivée partielle de $\langle f,g\rangle$ existe et vaut $\langle \partial_if,g\rangle + \langle f,\partial_ig\rangle$
\par Elle est donc continue par opération usuelle.}
\Prop{}{Soit $E,F,G$ trois espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^1$ sur $U$.
\par Soit $g$ une fonction définie de $V$ un ouvert de $F$ contenant $f(U)$ dazns $G$, $g$ de classe $\mathcal{C}^1$ sur $V$.
\par La fonction $g\circ f$ est de classe $\mathcal{C}^1$ sur $U$.}
\Pre{Immédiat :
\par Pour $i\in\llbracket 1, n\rrbracket$, la $i$-ème dérivée partielle de $g\circ f$ existe et vaut
\par $$\partial_i (g\circ f) = \sum\limits_{k=1}^{\dim F} (\partial_kg\circ f)\partial_if_k$$
\par Elle est donc continue par opération usuelle.}

\section{Dérivées partielles d'ordre supérieur}
\subsection{Dérivées partielles d'ordre k}
\Def{}{Soit $\mathcal{B}$ une base de $E$ et $f$ une application d'un ouvert $U$ dans $F$.
\par Si $f$ admet une $i$-ème dérivée partielle $\partial_if$ sur $U$, on dit que $f$ admet une dérivée partielle seconde d'incides $i, j$ si $\partial_if$ admet une $j$-ème dérivée partielle et on la note alors $\partial_j\partial_i f$.
\par On utilise aussi la notation $\dfrac{\partial f}{\partial x_ix_j}$ et on l'appelle dérivée partielle seconde par rapport aux variables $x_i, x_j$.
\par Par récurrence on définit la notion de dérivée partielle d'ordre $k$ d'indices $i_1,..., i_k$ ou dérivée partielle d'ordre $k$ par rapport aux variables $x_{i_1},..., x_{i_k}$, que l'on note $\partial_{i_k}...\partial_{i_1}f$ ou $\dfrac{\partial^kf}{\partial x_{i_1}...x_{i_k}}$}
\Def{}{Une application $f$ définie d'un ouvert $U$ dans $F$ est dite de classe $\mathcal{C}^k$ sur $U$ si toutes ses dérivées partielles d'ordre $k$ existent et sont continues sur $U$.
\par Elle est dite de classe $\mathcal{C}^\infty$ si elle est de classe $\mathcal{C}^k$ pour tout $k\in\N^*$}

\subsection{Théorème de Schwarz}
\Thr{Schwarz}{Soit $\mathcal{B}$ une base de $E$ et soit $f$ définie d'un ouvert $U$ dans $F$.
\par Si $f$ est de classe $\mathcal{C}^2$ alors :
\par $$\forall i,j\in\llbracket 1, n\rrbracket^2, \partial_i\partial_jf = \partial_j\partial_if$$}
\Pre{Admis}

\subsection{Opérations sur les applications de classe Ck}
Dans ce paragraphe, $k\in\N\cup\{+\infty\}$
\Prop{}{Soit $f$ et $g$ deux fonctions de $U$ dans $F$ de classe $\mathcal{C}^k$. Soit $\lambda, \mu$ deux réels.
\par La fonction $\lambda f+\mu g$ est de classe $\mathcal{C}^k$ sur $U$.}
\Pre{Admis}
\Prop{}{Soit $E,F,G,H$ quatre espaces vectoriels normés de dimension finie et $U$ un ouver tde $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^k$.
\par Soit $g$ une fonction définie de $U$ dans $G$ de classe $\mathcal{C}^k$.
\par Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par La fonction $B(f, g)$ est de classe $\mathcal{C}^k$ sur $U$.}
\Pre{Admis}
\Prop{}{Soit $E, F,G$ trois espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^k$ sur $U$.
\par Soit $g$ une fonction définie de $V$ un ouvert de $F$ contenant $f(U)$ dans $G$ et de classe $\mathcal{C}^k$ sur $V$.
\par La fonction $g\circ f$ est de classe $\mathcal{C}^k$ sur $U$.}
\Pre{Admis}


\section{Cas des applications numériques}
Dans toute cette partie, $F=\R$
\subsection{Gradient}
Dans tout ce paragraphe, $E$ est un espace vectoriel euclidien (et $F=\R$)
\Def{}{On appelle dual de $E$ et on note $\mathcal{E}^*$ l'ensemble $\mathcal{L}(E,\R)$}
\Def{}{Soit $f$ une fonction définie et différentiable sur un ouvert $U$ et à valeurs dans $\R$. Soit $a\in U$.
\par On appelle gradient de $f$ en $a$ et on note $\nabla f(a)$ l'unique vecteur de $E$ tel que :
\par $$\forall h\in E, df(a)\cdot h=\langle \nabla f(a), h\rangle$$}
On rencontre parfois aussi la notation $\mathrm{Grad}f(a)$
\Prop{Interprétation géométrique du gradient}{Pour $h$ un vecteur de $E$, $D_hf(a)= df(a)\cdot h = \langle \nabla f(a), h\rangle$.
\par Si $\nabla f(a)\neq 0$, il est colinéaire et de même sens que le vecteur unitaire selon lequel la dérivée de $f$ en $a$ est maximale.}
\Pre{Notons $v = \frac{1}{\Vert\nabla f(a)\Vert}\nabla f(a)$ le vecteur normé colinéaire à $\nabla f(a)$ de même sens.
\par $$\D_vf(a) = df(a)\cdot v = \Vert\nabla f(a)\Vert$$
\par Considérons un vecteur unitaire $u$.
\par $$D_uf(a) = df(a)\circ u = \langle \nabla f(a), u\rangle$$
\par Et par inégalité de Cauchy-Schwarz :
\par $$\vert D_uf(a)\vert\leq \Vert\nabla f(a)\Vert \leq D_vf(a)$$}
\Prop{}{Soit $f$ une fonction définie et différentiable sur un ouvert $U$ et à valeurs dans $\R$. Soit $a\in U$.
\par Dans $(e_1,..., e_n)$ une base orthonormale de $E$, $\nabla f(a)$ s'écrit $\nabla f(a) = \sum\limits_{i=1}^n\partial_if(a)\cdot e_i$}
\Pre{C'est une simple écriture de la formule de différentiation :
\par Pour $h\in E$, $h= h_1e_1+...+h_ne_n$ :
\par $$df(a)\cdot h =\sum\limits_{i=1}^n\partial_if(a)h_i = \left\langle\sum\limits_{i=1}^n\partial_if(a)\cdot e_i, h\right\rangle$$}

\subsection{Dérivée le long d'un arc}
\Def{}{On appelle arc paramétré une application définie sur un intervalle $I$ de $\R$ à valeur dans $\R^p$.
\par Cette notion modélise la notion de courbe dans un espace de dimension $p$ décrite par une fonction horaire de parcours.
\par Si $\gamma$ est un arc paramétré, l'image de $\gamma$ est appelé son support.}
\begin{Exe}
La fonction $\gamma$ définie sur $\R$ à valeur dans $\R^2$ euclidien par :
\par $$\gamma:t\mapsto\left\{\begin{array}{rcl} x & = & \cos t\\ y & = & \sin t\end{array}\right.$$
\par $\gamma$ est un arc paramétré dont le support est le cercle centré en $0$ de rayon $1$.
\end{Exe}
\Prop{}{Soit $\gamma$ un arc paramétré dérivable sur un intervalle ouvert $I$ et $t_0\in I$.
\par Si $\gamma'(t_0)$ est un vecteur non nul, alors le support de $\gamma$ admet pourtangente au point $\gamma(t_0)$ la droite dirigée par $\gamma'(t_0)$ passant par le point $\gamma(t_0)$.}
\Pre{On peut considérer que $\gamma$ est à valeur dans un espace vectoriel normé.
\par On a tout simplement :
\par $$\frac{1}{h}(\gamma(t_0+h)-\gamma(t_0))\to_{h\to 0}\gamma'(t_0)$$
\par Donc le vecteur normé $\frac{1}{\Vert \gamma(t_0)-\gamma(t_0)+h\Vert}(\gamma(t_0)-\gamma(t_0+h))$ converge vers $\frac{1}{\Vert\gamma'(t_0)\Vert}\gamma'(t_0)$ en 0.}
\begin{Rem}
La proposition précédente peut être étendue au cas où $\gamma$ est dérivable sur un intervalle d'intérieur non vide. On parlera alors de demi-tangent en $\gamma(t_0)$ si $t_0$ est une extrémité de l'intervalle.
\end{Rem}
\Prop{Dérivée le long d'un arc}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$. Soit $\gamma$ une application définie sur un intervalle d'intérieur non-vide $I$ de $\R$ et à valeurs dans $U$.
\par Si $\gamma$ est dérivable en $t$ et si $f$ est différentiable en $\gamma(t)$, alors $f\circ\gamma$ est dérivable en $t$ et :
\par $$(f\circ\gamma)'(t) = df(\gamma(t))\cdot\gamma'(t)$$}
\Pre{Il s'agit simplement de l'application de la formule donnant la différentielle d'une composée dans un cas particulier.
\par Pour un réel $h$ au voisinage de $0$ :
\par \begin{align*}h(f\circ\gamma)'(t)&=d(f\circ\gamma)(t)\cdot h\\
&= df(\gamma(t))\cdot (d\gamma(t)\cdot(h))\\
&= df(\gamma(t))\cdot(h\gamma'(t))\\
&= hdf(\gamma(t))\cdot(\gamma'(t))\end{align*}}
\begin{Rem}
Interprétation géométrique :
\par Si $\gamma'(t)\notin\ker(df(\gamma(t)))$, la tangente au point de paramètre $t$ de l'arc paramétré $f\circ\gamma$ est dirigé par l'image d'un vecteur directeur de la tengente à $\gamma$ au point de paramètre $t$ par l'application $df(\gamma(t))$
\end{Rem}
\includegraphics{schemas/dérivée-long-arc.png}
\begin{Rem}
Cas particulier fondamental :
\par Soit $a\in U$, soit $h\in E$ et $\gamma$ l'application définie par $\gamma(t) = a+th$.
\par Si $f$ est différentiable en $a$, alors l'application $f\circ\gamma$ est dérivable en $0$ et :
\par $$(f\circ\gamma)'(0) =df(a)\circ h=D_hf(a)$$  
\end{Rem}

\subsection{Intégration le long d'un arc d'une application de classe C1}
\Prop{}{Si $f$ est une application de classe $\mathcal{C}^1$ d'un ouvert $U$ dans $F$, si $\gamma$ est une application de classe $\mathcal{C}^1$ de $[0,1]$ dans $\Omega$, si $\gamma(0)=a$, $\gamma(1)=b$, alors :
\par $$f(b)-f(a) = \int_0^1df(\gamma(t))\cdot\gamma'(t)dt$$}
\Pre{Dans les conditions de l'énoncé, $t\mapsto df(\gamma(t))\cdot\gamma'(t)$ est la dérivée de la fonction $g :t\mapsto f(\gamma(t))$
\par Donc :
\par $$\int_0^1df(\gamma(t))\cdot\gamma'(t)dt = g(1)-g(0) = f(b)-f(a)$$}
\begin{Rem}
La valeur de $\int_0^1df(\gamma(t))\cdot\gamma'(t)dt$ ne dépend par de l'application $\gamma$ mais uniquement de ses valeurs en $0$ et $1$.
\end{Rem}
\begin{Rem}
Pour $E=\R^2$ ou $\R^3$ :
\par En physique, on dit qu'un champ de vecteurs $V$ dérive d'un potentiel s'il existe une fonction $f$ différentiable telle que pour tout $h\in E$ et $a\in E$, $df(a)\cdot h = \langle V(a), h\rangle$.
\par Dans ce cas, $\int_0^1\langle\nabla f(\gamma(t)), \gamma'(t)\rangle dt$ dépend uniquement des valeurs de $f(\gamma(0))$ et $f(\gamma(1))$
\end{Rem}
\Prop{}{Si $f$ est une application d'un ouvert $U$ dans $F$.
\par Si $U$ est connexe par arcs, la fonction $f$ est constante sur $U$ si, et seulement si, elle est différentiable sur $U$ et si $df=0$}
\Pre{Preuve dans le cas $U$ convexe uniquement :
\par Si $f$ est constante alors $f$ est différentiable et $df=0$
\par Réciproquement, supposons que $f$ est différentiable sur un convexe $U$ de différentielle nulle.
\par Soit $(a,b)$ dans $U$, considérons l'arc $\gamma$ défini sur $[0,1]$ par $t\mapsto (1-t)a+tb$. $\gamma$ est à valeur dans $U$ car $U$ est convexe.
\par D'après la proposition précédente,
\par $$f(b)-f(a) = \int_0^1df(\gamma(t))\cdot\gamma'(t)dt=0$$
\par Donc $f$ est constante.}

\subsection{Vecteurs tangents à une partie d'un espace normée de dimension finie}
\Def{}{Si $X$ est une partie de $E$ et $x$ un point de $X$, un vecteur $v$ de $E$ est tangent à $X$ en $x$ s'il existe $\varepsilon>0$ et un arc $\gamma$ défini sur $]-\varepsilon, \varepsilon[$, dérivable en $0$, à valeurs dans $X$, tels que $\gamma(0)=x$ et $\gamma'(0)=v$
\par On note $T_xX$ l'ensemble des vecteurs tangents à $X$ en $x$.}
\begin{Exe}
L'ensemble des vecteurs tangents en un point à un ouvert de $E$ est $E$ lui-même puisque pour tout $x\in U$ et pour tout vecteur $u$ de $E$, on peut définir sur $]-\varepsilon, \varepsilon[$ l'arc $\gamma:t\mapsto x+tu$
\par (dont le support est une partie de la droite dirigée par $u$)
\par $\gamma(0)=x$ et $\gamma'$ est constante égale à $u$.
\end{Exe}
\Def{}{Si $E$ est un espace vectoriel euclidien.
\par Soit $f$ une fonction définie d'un ouvert $U$ dans $\R$.
\par Soit $w\in\R$.
\par On dit que $X = \{x\in U\vert w= f(x)\}$ est une ligne de niveau de $f$.
}
\Prop{}{Si $E$ est un espace vectoriel euclidien.
\par Soit $f$ une fonction définie d'un ouvert $U$ dans $\R$, différentiable.
\par Soit $X$ une ligne de niveau de $f$.
\par Les vecteurs tangents à $X$ au point $x_0$ de $X$ annulent la différentielle de $f$ en $x_0$.
Ce sont donc des vecteurs orthogonaux au gradient de $f$ en $x_0$.}
\Pre{Tout vecteur $u$ tangent à $X$ en $x_0$ est la dérivée en $0$ d'un arc paramétré $\gamma$ dérivable sur un voisinage de $0$, à valeurs dans $X$ et tel que $\gamma(0)=x_0$
\par Pour $t$ au voisinage de $0$, $f(\gamma(t))=f(x_0)$
\par Donc la fonction $t\mapsto f(\gamma(t))$ est constante au voisinage de $0$ donc de dérivée nulle.
\par On en déduit, toujours pour $t$ au voisinage de $0$ :
\par $$df(\gamma(t))\cdot\gamma'(t)=0$$
\par En particulier pour $t=0$ : $df(x_0)\cdot \gamma'(0)=0$, c'est à dire :
\par $$\langle\nabla f(x_0), u\rangle=0$$}
On admettra le théorème plus fort suivant :
\Thr{}{Soit $E$ un espace vectoriel eculdiein. Soit $f$ une fonction définie d'un ouvert $U$ dans $\R$, de classe $\mathcal{C}^1$. Soit $X$ un eligne de niveau de $f$. Soit $x_0\in X$ de $X$.
\par Si $df(x_0)\neq 0$, alors :
\par $$T_{x_0}X =\ker(df(x_0))=(\nabla f(x_0))^\perp$$}

\subsection{Optimisation}
\subsubsection{Conditions de premier ordre}
\Def{}{Soit $f$ une fonction défineis ur $A\subset E$ et à valeurs dans $\R$.
\par On dit que $f$ présente un maximum local en un point $a$ de $A$ s'il existe un voisinage $V$ de $a$ dans $A$ tel que :
\par $$\forall x\in V, f(x)\leq f(a)$$
\par On dit que $f$ présente un minimum local en un point $a$ de $A$ s'il existe un voisinage $V$ de $a$ dans $A$ tel que :
\par $$\forall x\in V, f(x)\geq f(a)$$
\par On dit que $f$ admet un extremmum local en un point $a$ de $A$ si elle admet un maximum ou un minimum local en $a$.}
\begin{Rem}
On dit que $f$ présente un maximum (global) en un point $a$ de $A$ si :
\par $$\forall x\in U, f(x)\leq f(a)$$
\par On définit de même la notion de minimum (global) et d'extremum (global). 
\end{Rem}
\Def{}{Soit $f$ une fonction définie sur un ouvert $U$ et à valeurs dans $\R$.
\par Le point $a\in U$ est appelé point critique de $f$ si $f$ est différentiable en $a$ et si $df(a)=0$.}
\begin{Rem}
Si $E$ est un espace euclidien, un point $a$ est un point critique si $\nabla f(a)=0$.
\end{Rem}
\Prop{}{Soit $f$ une fonction définie sur un ouvert $U$ et à valeurs dans $\R$ et $a\in U$.
\par Si $f$ admet un extremum local en $a$ et si $f$ est différentiable en $a$, alors $df(a)=0$.
\par (i.e. $a$ est un point critique de $f$)}
\Pre{Supposons $df(a)$ non nul.
\par On peut alors trouver un vecteur $u$ tel que $df(a)\cdot u\neq 0$.
\par On peut alors écrire pour $t\in\R$ suffisamment petit :
\par $$f(a+tu) -f(a) = df(a)\cdot tu+o(t)\sim_0 tdf(a)\cdot u$$
\par Donc $f(a+tu)-f(a)$ change de signe en $0$ et $f$ n'a donc pas d'extrémum en $a$.
\par On a montré le résultat par contraposée.
\par Notez l'importance du fait que $U$ est ouvert dans la démonstration.}
\begin{Exe}
$x\mapsto x^3$ admet un point critique en $0$ mais ce n'est pas un extremum local.
\par Ainsi, la réciproque est fausse.
\end{Exe}
\begin{Rem}
Si on doit étudier les extrema d'une fonction $f$ définie sur $A$ partie de $E$ et à valeurs dans $F$ :\begin{enumerate}
\item On se place sur $\overset{\circ}{A}$. On étudie si $f$ est différentiable sur $\overset{\circ}{A}$ et on détermine les points critiques.
\item Les points où $f$ admet un extremum sont à chercher dans $A\backslash\overset{\circ}{A}$, les points de $\overset{\circ}{A}$ où $f$ n'est pas différentiable et les points critiques.
\end{enumerate}
\end{Rem}
\Prop{}{Si $f$ est une fonction numérique définie sur l'ouvert $U$, si $X$ est une partie de $U$, si la restriction de $f$ à $X$ admet un extremum local en $x$ et si $f$ est différentiable en $x$, alors $df(x)$ s'annule en tout vecteur tangent à $X$ en $x$.}
\Thr{optimisation sous une contrainte}{Si $f$ et $g$ sont des fonctions numériques définies et de classe $\mathcal{1}^1$ sur l'ouvert $\Omega$ de $E$, si $X$ est l'ensemble des zéros de $g$, si $x\in X$ et $dg(x)\neq 0$ et si la restriction de $f$ à $X$ admet un extremum local en $x$, alors $df(x)$ est colinéaire à $dg(x)$.}

\subsubsection{Conditions de second ordre}
\Def{}{Soit $f$ une fonction de classe $\mathcal{C}^2$ sur un ouvert $U$ de $\R^n$ euclidien, à valeurs réelles. Soit $x\in U$.
\par La matrice hessienne de $f$ en $x$ est la matrice symétrique :
\par $$H_f(x) = (\partial^2 f_{i,j}(x))_{(i,j)\in\llbracket1,n\rrbracket^2}$$}
\Prop{Formule de Taylor-Young à l'ordre 2}{Soit $f$ une fonction de classe $\mathcal{C}^2$ sur un ouvert $U$ de $\R^n$ euclidien, à valeurs réelles. Soit $x\in U$.
\par $$f(x+h)=_{h\to 0} f(x) + \langle \nabla f(x), h\rangle + \frac{1}{2}\langle H_f(x)\cdot h, h\rangle + o(\Vert h\Vert^2)$$
\par $$f(x+h)=_{h\to 0} f(x) + \nabla f(x)^\top h + \frac{1}{2}h^\top H_f(x)\cdot h + o(\Vert h\Vert^2)$$}
\Prop{}{\begin{itemize}
\item Si $f$ est une fonction de classe $\mathcal{C}^2$ sur un ouvert de $\R^n$ et si $f$ admet un minimum local en $x$, alors $x$ est un point critique de $f$ et $H_f(x)\in\mathcal{S}_n^+(\R)$
\item Si $f$ est une fonction de classe $\mathcal{C}^2$ sur un ouvert de $\R^n$, si $x$ est point critique de $f$ et si $H_f(x)\in\mathcal{S}_n^{++}(\R)$, alors $f$ admet un minimum local strict en $x$.
\end{itemize}}
\begin{Rem}
\begin{itemize}
\item La proposition précédente s'applique aux maxima en remplaçant $H_f(x)$ par $-H_f(x)$
\item En particulier, en un point critique $x$ de $f$ tel que $H_f(x)\notin\mathcal{S}_n^+(\R)$ et $-H_f(x)\notin\mathcal{S}_n^+(\R)$, on peut conclure que $f$ n'admet pas d'extrémum en $x$.
\item La condition $x$ est point critique de $f$ et $H_f(x)\in\mathcal{S}_n^+(\R)$ n'est pas suffisante pour déterminer si $f$ admet un extrémum local en $x$ (voir par exemple $(x,y)\mapsto x^2$ et $(x,y)\mapsto x^2+y^2$)
\end{itemize}
\end{Rem}







\end{document}