\documentclass[a4paper,12pt]{book}
\usepackage{ae}
\usepackage{aeguill}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[left=1cm, right= 1cm, top=2cm, bottom = 2cm]{geometry}
\usepackage{array,multirow}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{tcolorbox}
\usepackage{lmodern}
\usepackage{esint}


\newcommand{\Def}[2]{\begin{tcolorbox}[colback=white,colframe=red!10!green!20!blue!75!, title=Définition : #1]#2\end{tcolorbox}}
\newcommand{\Thr}[2]{\begin{tcolorbox}[sharp corners, colback=white,colframe=red!10!blue!30!green!75!, title=Théorème : #1]#2\end{tcolorbox}}
\newcommand{\Pre}[1]{\begin{tcolorbox}[sharp corners, colback=white,colframe=green!60!green!30!black!75, title=Preuve]#1\end{tcolorbox}}
\newcommand{\Meth}[2]{\begin{tcolorbox}[colback=white,colframe=green!60!green!30!black!75, title=Méthode :  #1]#2\end{tcolorbox}}
\newtheorem{Exe}{Exemple}[section]
\newtheorem{Exes}{Exemples}[section]
\newtheorem{Rem}{Remarque}[section]
\newtheorem{Rems}{Remarques}[section]

\def\R{\mathbb{R}}
\def\D{\mathbb{D}}
\def\C{\mathbb{C}}
\def\Q{\mathbb{Q}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\K{\mathbb{K}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Rot}{\overrightarrow{\mathrm{rot}}}
\newcommand{\Grad}{\overrightarrow{\mathrm{grad}}}
\newcommand{\Div}{\mathrm{div}}
\renewcommand{\Vec}[1]{\overrightarrow{#1}}
\newcommand{\Phyvex}[3]{\left\vert\begin{matrix}#1\\#2\\#3\end{matrix}\right.}

\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\Roman{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\Roman{section}.\arabic{subsection}.\Alph{subsubsection}}


\begin{document}
\tableofcontents
\chapter{Maths - sup}
\section{Analyse pratique}
\begin{tcolorbox}[colback=white,colframe=black,title=Formulaire de trigo] 
Avec $a,b\in\R$, on a :\begin{itemize}
\item $\mathrm{cos}(a+b) = \mathrm{cos}(a)\mathrm{cos}(b) - \mathrm{sin}(a)\mathrm{sin}(b)$ ;
\item $\mathrm{cos}(a-b) = \mathrm{cos}(a)\mathrm{cos}(b) + \mathrm{sin}(a)\mathrm{sin}(b)$ ;
\item $\mathrm{sin}(a+b) = \mathrm{sin}(a)\mathrm{cos}(b) + \mathrm{cos}(a)\mathrm{sin}(b)$ ;
\item $\mathrm{sin}(a-b) = \mathrm{sin}(a)\mathrm{cos}(b) - \mathrm{cos}(a)\mathrm{sin}(b)$ ;
\item $\mathrm{cos}(a)\mathrm{cos}(b) = \dfrac{1}{2}\left(\mathrm{cos}(a+b) + \mathrm{cos}(a-b)\right)$ ;
\item $\mathrm{sin}(a)\mathrm{sin}(b) = \dfrac{1}{2}\left(\mathrm{cos}(a-b) - \mathrm{cos}(a+b)\right)$ ;
\item $\mathrm{sin}(a)\mathrm{cos}(b) = \dfrac{1}{2}\left(\mathrm{sin}(a+b)-\mathrm{sin}(a-b)\right)$ ;
\item $\mathrm{sin}(2a) = 2\mathrm{sin}(a)\mathrm{cos}(a)$ ;
\item $\mathrm{cos}(2a) = 1 - 2\mathrm{sin}^2(a) = \mathrm{cos}^2(a) - \mathrm{sin}^2(a) = 2\mathrm{cos}^2(a) - 1$ ;
\item $\mathrm{tan}(2a) = \dfrac{2\mathrm{tan}(a)}{1-\mathrm{tan}^2(a)}$ ;
\item $\mathrm{cos}(a) + \mathrm{cos}(b) = 2\mathrm{cos}\left(\dfrac{a+b}{2}\right)\mathrm{cos}\left(\dfrac{a-b}{2}\right)$ ;
\item $\mathrm{sin}(a) + \mathrm{sin}(b) = 2\mathrm{sin}\left(\dfrac{a+b}{2}\right)\mathrm{cos}\left(\dfrac{a-b}{2}\right)$ ;
\item $\mathrm{sin}(a) = \dfrac{2\mathrm{tan}\left(\dfrac{a}{2}\right)}{1+\mathrm{tan}^2\left(\dfrac{a}{2}\right)}$ ;
\item $\mathrm{cos}(a) = \dfrac{1-\mathrm{tan}^2\left(\dfrac{a}{2}\right)}{1+\mathrm{tan}^2\left(\dfrac{a}{2}\right)}$ ;
\item $\mathrm{tan}(a) = \dfrac{2\mathrm{tan}\left(\dfrac{a}{2}\right)}{1-\mathrm{tan}^2\left(\dfrac{a}{2}\right)}$ ;
\item $\mathrm{sh}^2 - \mathrm{ch}^2 = 1$ ;
\item $\mathrm{cos}(a) = \dfrac{e^{ia} + e^{-ia}}{2}$ ;
\item $\mathrm{sin}(a) = \dfrac{e^{ia} - e^{ia}}{2i}$.
\end{itemize}
\end{tcolorbox}

Dérivées et primitives des fonctions usuelles :\\
\begin{tabular}{|c|c|c|c|c|}
	\hline Fonction & Dérivée & Primitive & $\mathcal{D}_f$ & $\mathcal{D}_{f'}$
	\\ \hline $0$ & $0$ & $C$ & $\R$ & $\R$
	\\ \hline $c$ & $0$ & $cx + C$ & $\R$ & $\R$
	\\ \hline $\dfrac{1}{x}$ & $\dfrac{-1}{x^2}$ & $\mathrm{ln}|x| + C$ & $\R^*$ & $\R^*$
	\\ \hline $x^\alpha$ & $\alpha x^{\alpha - 1}$ & $\dfrac{x^{\alpha+1}}{\alpha+1} + C$ & $\mathcal{D}_\alpha$ & $\R_+$
	\\ \hline $e^{cx}$ & $ce^{cx}$ & $\dfrac{1}{c}e^{cx} + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{ln}(x)$ & $\dfrac{1}{x}$ & $x\mathrm{ln}(x) - x + C$ & $\R_+^*$ & $\R^*_+$
	\\ \hline $\mathrm{sin}(\alpha x)$ & $\alpha\mathrm{cos}(\alpha x)$ & $-\dfrac{1}{\alpha}cos(\alpha x) + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{cos}(\alpha x)$ & $-\alpha\mathrm{sin}(\alpha x)$ & $\dfrac{1}{\alpha}sin(\alpha x) + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{tan}(x)$ & $1+ \mathrm{tan}^2(x)$ & $-\mathrm{ln}|cos(x)| + C$ & $\mathcal{D}_{tan}$ & $\mathcal{D}_{tan}$
	\\ \hline $\mathrm{sh}(\alpha x)$ & $\alpha\mathrm{ch}(\alpha x)$ & $\dfrac{1}{a}ch(\alpha x) + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{ch}(\alpha x)$ & $\alpha\mathrm{sh}(\alpha x)$ & $\dfrac{1}{\alpha}sh(\alpha x) + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{th}(x)$ & $1-\mathrm{th}^2(x)$ & $\mathrm{ln}(\mathrm{ch}(x)) + C$ & $\R$ & $\R$
	\\ \hline $\mathrm{arcsin}(x)$ & $\dfrac{1}{\sqrt{1-x^2}}$ & $x\mathrm{arcsin}(x) + \sqrt{1-x^2} + C$ & $[-1;1]$ & $]-1;1[$
	\\ \hline $\mathrm{arccos}(x)$ & $\dfrac{-1}{\sqrt{1-x^2}}$ & $x\mathrm{arccos}(x) - \sqrt{1-x^2} + C$ & $[-1;1]$ & $]-1;1[$
	\\ \hline $\mathrm{arctan}(x)$ & $\dfrac{1}{1+x^2}$ & $x\mathrm{arctan}(x) - \dfrac{1}{2}\mathrm{ln}(1+x^2) + C$ & $\R$ & $\R$
\\ \hline\end{tabular}

\begin{tcolorbox}[colback=white,colframe=black,title=Développements limités]
On note $x$ un réel au voisinage de $0$ et :\begin{itemize}
    \item $\mathrm{e}^{x} = 1 + x + \dfrac{x^2}{2} + \dfrac{x^3}{6} + \dfrac{x^4}{24} + ... + \dfrac{x^n}{n!} + o(x^n)$ ;
    \item $\mathrm{cos}(x) = 1 - \dfrac{x^2}{2} + \dfrac{x^4}{24} + ... + (-1)^{p}\dfrac{x^{2p}}{(2p)!} + o(x^{2p})$ ;
    \item $\mathrm{sin}(x) = x - \dfrac{x^3}{6} + \dfrac{x^5}{120} + ... + (-1)^p\dfrac{x^{2p+1}}{(2p+1)!} + o(x^{2p+1})$ ;
    \item $\mathrm{ln}(1+x) = x - \dfrac{x^2}{2} + \dfrac{x^3}{3} + ... + (-1)^{n+1}\dfrac{x^n}{n} + o(x^n)$ ;
    \item $\dfrac{1}{1-x} = 1 + x + x^2 + x^3 + ... + x^n + o(x^n)$ ;
    \item $\dfrac{1}{1+x} = 1 - x + x^2 - x^3 + ... + (-1)^nx^n + o(x^n)$ ;
    \item $(1+x)^\alpha = 1 + \alpha x + \dfrac{\alpha(\alpha-1)x^2}{2} + \dfrac{\alpha(\alpha-1)(\alpha-2)x^3}{6} + ... + \dfrac{\alpha...(\alpha-n+1)x^n}{n!} + o(x^n)$ ;
    \item $\mathrm{ch}(x) = 1 + \dfrac{x^2}{2} + \dfrac{x^4}{24} + ... + \dfrac{x^{2p}}{(2p)!} + o(x^{2p})$ ;
    \item $\mathrm{sh}(x) = x + \dfrac{x^3}{6} + \dfrac{x^5}{120} + ... + \dfrac{x^{2p+1}}{(2p+1)!} + o(x^{2p+1})$.
\end{itemize}
Les trois DL à connaître absolument sont ceux de $\dfrac{1}{1-x}$, de $\mathrm{exp}(x)$ et de $(1+x)^\alpha$. En effet, tous les autres DL usuels découlent de ceux-ci :\begin{itemize}
    \item Le DL du sinus est celui des termes impairs de l'exponentielle, avec une alternance de signes, au contraire du sinus hyperbolique qui est aussi les termes impairs, mais avec un signe positif.
    \item Le DL du cosinus est celui des termes pairs de l'exponentielle, avec une alternance de signes, au contraire du cosinus hyperbolique qui est aisso les termes pairs, mais avec un signe positif signe positif.
    \item Le DL de $\dfrac{1}{1+x}$ découle de celui de $\dfrac{1}{1-x}$ en composant par $x\mapsto -x$. On a donc une alternance de signes.
    \item Le DL de $\mathrm{ln}(1+x)$ découle de celui de $\dfrac{1}{1+x}$, par primitivation d'un DL.
\end{itemize}
On notera l'équivalent de $n!$ obtenu par la formule de Stirling : $n!\sim \left(\dfrac{n}{e}\right)^n\sqrt{2\pi n}$.
\end{tcolorbox} 
    
\begin{tcolorbox}[colback=white,colframe=black,title=Résolution d'équations différentielles du second ordre] 
Une équation différentielle de second ordre est de la forme : $y'' = ay' + by = f(x)$.
\par Pour la résoudre, on trouve d'abord les racines du polynôme $x^2 + ax + b$.
\par Si $\Delta=0$, on a une seule racine $r_0$, et la solution générale est de la forme $(\lambda + \mu x)e^{r_0 x}$.
\par Sinon, dans les cas d'une équation différentielle complexe $\Delta\neq0$ ou réelle avec $\Delta>0$, on a avec les racines $r_1,r_2$ les solutions $\lambda e^{r_1x} + \mu e^{r_2x}$.
\par Dans le cas où $\Delta<0$, on note pour $r$ une des racines du polynôme caractéristique, $\rho = \Re(r)$ et $\omega = \Im(r)$ la solution de la forme :
$$e^{\rho x} (\lambda \mathrm{cos}(\omega x) + \mu \mathrm{sin}(\omega x))$$
\end{tcolorbox}


\newpage
\section{Théorèmes d'analyse}
\subsection{Fondamentaux}
\Def{Continuité}{$f$ continue en $x_0$ si
\par $$\forall\varepsilon>0, \exists\alpha>0,\forall x\in D_f, \vert x-x_0\vert<x_0\Rightarrow \vert f(x)-f(x_0)\vert<\varepsilon$$
$f$ uniformément continue sur $I$ si :
\par $$\forall \varepsilon>0, \exists\alpha>0, \forall (x,y)\in I, \vert x - y\vert<\alpha\Rightarrow \vert f(x)-f(y)\vert<\varepsilon$$}
\Thr{Limites monotones}{Avec $a<b$ : soit $g\in\mathcal{F}(]a,b[,\R)$ \begin{itemize}
\item S $g$ est croisssante majorée, alors $g$ a une limite en $b^-$
\item Si $g$ est croissante non-majorée, alors $g\to_{b^-}+\infty$
\item Si $g$ est croissante minorée, alors $g$ a une limite en $a^+$
\item Si $g$ est croissante non-minorée, alors $g\to_{a^-}-\infty$
\item S $g$ est décroisssante majorée, alors $g$ a une limite en $a^+$
\item Si $g$ est décroissante non-majorée, alors $g\to_{a^+}+\infty$
\item Si $g$ est décroissante minorée, alors $g$ a une limite en $b^-$
\item Si $g$ est décroissante non-minorée, alors $g\to_{b^+}-\infty$
\end{itemize}}
\Thr{Caractérisation séquentielle de la limite}{$\lim\limits_{x\to a}f(x)= l$ si, et seulement si, pour toute suite $(u_n)$ de limite $a$, $\lim\limits_{n\to +\infty}f(u_n)=l$}
\Thr{Encadrement}{Si au foisinage de $x_0\in\bar{\R}$, on a $f(x)\leq g(x)\leq h(x)$, et si les fonctions $h$ et $f$ admettent une limite commune en $x_0$, alors $g$ admet $l$ comme limite en $x_0$.}
\Thr{Valeurs intermédiaires}{Si $f$ est continue, l'image d'un intervalle par $f$ est un intervalle.}
\Thr{Bornes atteintes}{Si $f$ est continue, l'image d'un segment par $f$ est un segment.}
\Thr{Heine}{Si $f$ est continue sur un segment, alors $f$ est uniformément continue sur ce segment.}
\subsection{Fondamentaux de dérivabilité}
\Def{Dérivabilité}{$f$ est dérivable en $x_0$ si $\lim\limits_{h\to 0}\dfrac{f(x_0+h)-f(x_0)}{h}$ existe. En ce cas, on appelle la derivée de $f$ en $x_0$ cette limite, notée $f'(x)$. Tout point où $f'$ s'annule est un point critique.
\par $f$ est $\mathcal{C}^1$ sur $I$ si $f$ est dérivable en tout point de $I$ et que sa dérivée est continue. On peut étendre cette définition pour $k$ entier ou infini.
\par $f$ est convexe sur $I$ si $\forall x,y\in I,\forall t\in[0,1], f(tx+(1-t)y)\leq tf(x)+(1-t)f(y)$. Si $f$ est $\mathcal{C}^2$, alors $f$ est convexe quand $f''>0$. Tout point où $f''$ s'annule est un point d'inflexion.}
\Thr{Opérations de dérivation}{Si $f$ et $g$ sont $n$ fois dérivables :\begin{itemize}
\item pour $\lambda\in\R$, $(f+\lambda g)' = f'+\lambda g'$
\item $$(fg)' = f'g+fg'$$
\item $$(f^n)' = nf'f^{n-1}$$
\item $$(g\circ f)' =f'(g'\circ f)$$
\item $$(f^{-1})' = \dfrac{1}{f'\circ f^{-1}}$$
\item $$(fg)^{(n)} =\sum\limits_{k=0}^n\binom{n}{k}f^{(k)}g^{(n-k)}$$
\end{itemize}}
\Thr{Prolongement}{Si $f$ est de classe $\mathcal{C}^1$ sur $]a,b]$, continue en $a$ et que $f'$ admet une limite $l$ en $a$, alors $f$ est de classe $\mathcal{C}^1$ sur $[a,b]$ et $f'(a)=l$}
\Thr{Rolle}{Si $f(a)=f(b)$, alors $\exists c\in]a,b[, f'(c)=0$}
\Thr{Accroissements finis}{Il existe $c\in]a,b[$ tel que $f(b)-f(a) = (b-a)f'(c)$}
\Thr{Inégalité des accroissemens finis}{ Si $\forall t\in]a,b[, \vert f'(t)\vert\leq M$, alors $\vert f(b)-f(a)\vert\leq M(b-a)$}
\Thr{Inégalité de Jensen}{Si $f$ est convexe sur $I$, qu'on a $\lambda_1,...,\lambda_n\in[0,1]$ tels que $\lambda_1+...+\lambda_n=1$ et $x_1,..., x_n\in I^n$, alors :
\par $$f\left(\sum\limits_{i=1}^n \lambda_ix_i\right)\leq \sum\limits_{i=1}^n \lambda_i f(x_i)$$}


\subsection{Analyse asymptotique}
\Def{}{Avec $f$ et $g$ deux fonctions définies en $b$, on dit que :\begin{itemize}
\item $f =_b o(g)$ si $f = \varepsilon g$ sur un voisinage de $b$, avec $\varepsilon$ qui tent vers $0$ en $b$
\item $f =_b \mathcal{O}(g)$ si $f = M g$ sur un voisinage de $b$, avec $M$ bornée sur ce voisinage de $b$
\item $f \sim_b g$ si $f = \varepsilon g$ sur un voisinage de $b$, avec $\varepsilon$ qui tent vers $1$ en $b$
\end{itemize}
$f\sim_b g$ équivaut à $f =_b g+o(g)$ 
}
\Thr{Taylor-Reste-Intégral}{Si $f$ est $\mathcal{C}^{n+1}$ sur $[a,b]$, alors :
\par $$f(b) = \sum\limits_{k=0}^n \frac{f^{(k)}(a)}{k!}(b-a)^k +\int_a^b\frac{(t-a)^n}{n!}f^{(n+1)}(t)dt$$
\par On note $R_n(b) = \int_a^b\frac{(t-a)^n}{n!}f^{(n+1)}(t)dt$}
\Thr{Taylor-Lagrange}{En posant $t = a+(b-a)u$, on transforme l'expression du reste en $\int_0^1\frac{(b-a)^{n+1}(1-u)^n}{n!}f^{(n+1)}(a+(b-a)u)du$
\par On a donc : $R_n(b) = \frac{(b-a)^{n+1}}{n!}\int_0^1(1-u)^nf^{(n+1)}(a+(b-a)u)du$ \par De là on déduit la majoration de Lagrange, en posant $M_{n+1} = \sup\limits_{[a,b]}\vert f^{(n+1)}\vert$
\par $\vert R_n(b)\vert \leq \frac{\vert b-a\vert^{n+1}}{n!}\int_0^1\vert 1-u\vert^n\vert f^{(n+1)}(a+(b-a)u)\vert du \leq \frac{\vert b-a\vert^{n+1}}{n!}M_{n+1}\int_0^1(1-u)^ndu\leq \frac{\vert b-a\vert^{n+1}}{(n+1)!}M_{n+1}$}
\Thr{Taylor-Young}{Si $f$ est $\mathcal{C}^n$ sur $[a,b]$, alors :
\par $$f(x) = \sum\limits_{k=0}^n\dfrac{(x-a)^k}{n!}f^{(k)}(a)+o(x^{n+1})$$}

\subsection{Séries}
\Thr{Critères de convergence d'une série}{Si $(u_n), (v_n)\in\R_+^\N$ : \begin{itemize}
\item Si $\forall n\in\N, v_n\leq u_n$, alors si $\sum u_n$ converge alors $\sum u_n$ converge. De même, si $\sum u_n$ diverge, alors $\sum v_n$ aussi (critère de majoration positif)
\item Si $u_n = o(v_n)$, alors si $\sum v_n$ converge, alors $\sum u_n$ converge (critère de domination positif)
\item Si $u_n\sim v_n$, alors $\sum v_n$ et $\sum u_n$ sont de même nature (critère d'équivalent positif)\end{itemize}}
\Thr{Comparaisons séries-intégrales}{Si $f:\R_+\to\R$ fonction positive décroissante continue par morceaux, alors $\left(\sum f(n)\right)$ est de même nature que la suite $\left(\int_0^nf(t)dt\right)$}
\Thr{Conséquences de l'absolue convergence}{Toute série absolument convergente est convergente}
\Thr{Théorème spécial des séries alternées}{Soit $(a_n)$ une suite réelle positive décroissante de limite nulle. Alors $\sum(-1)^n a_n$ converge et $\forall n\in\N, \left|\sum\limits_{k=n}^{+\infty}(-1)^ka_k\right|\leq a_n$}
\Thr{Séries de Riemann}{Pour $\alpha\in\R$, alors $\left(\sum \dfrac{1}{n^\alpha}\right))$ converge si, et seulement si, $\alpha>1$.}
\Thr{Séries téléscopiques}{$(a_n)\in\K^\N$, la série $(\sum a_n - a_{n+1})$ converge si, et seulement si, la suite $(a_n)$ converge.}
\Thr{Séries géométriques}{On prend $a\in\C$, la série $(\sum a^n)$ converge si, et seulement si, $\vert a\vert<1$ et alors : $\sum_{n = 0}^{\infty} a^n = \dfrac{1}{1-a}$}

\subsection{Familles sommables}
\Thr{Sommation par paquets positif}{Soit $I$ dénombrable et $(J_j)_{j\in J}$ une partition de $I$ avec $J$ au plus dénombrable, ie $\cup_{j\in J} J_j = I, \forall j, h\in I, j\neq h \Rightarrow J_j\cap J_h = \emptyset$. Soit $(u_i)_{i\in I}\in\R_+^I$, alors : $\sum\limits_{i\in I} u_i = \sum\limits_{j\in J}\left(\sum\limits_{k\in J_j}u_k\right)$ }
\Thr{Sommation par paquets}{$I$ dénombrable, dont les $J_j$ sont une partition. Avec $(u_i)_{i\in I}\in\C^I$ sommable. Alors $\sum\limits_{i\in I}u_i = \sum\limits_{j\in J}\sum\limits_{k\in J_j} u_k$}
\Thr{de Fubini}{Soit $(u_{i,j})_{(i,j)\in I\times J}\in\C^{I\times J}$ sommable. Alors $\sum\limits_{i,j\in I\times J} u_{i,j} = \sum\limits_{i\in I}\sum\limits_{j\in J}u_{i,j} = \sum\limits_{j\in J}\sum\limits_{j\in J}$, avec le cas particulier où $u_{i,j} = a_ib_j$ où : $(a_ib_j)_{i,j\in I\times J}$ qui est sommable si, et seulement si, $(a_i)_{i\in I}$ et $(b_j)_{j\in J}$ sont sommables et dans ce cas, $\sum\limits_{(i,j)\in I\times J} a_ib_j = \left(\sum\limits_{i\in I}a_i\right)\left(\sum\limits_{j\in J}b_j\right)$}
\Thr{Produit de Cauchy}{Soit $(a_n), (b_n)\in\C^\N$. Si $\sum a_n, \sum b_n$ sont abolument convergentes alors $\sum\left(\sum\limits_{k=0}^na_nb_{n-k}\right)$ est absolument convergente et $\left(\sum\limits_{n=0}^{+\infty}a_n\right)\left(\sum\limits_{n=0}^{+\infty}b_n\right) = \sum\limits_{n=0}^{+\infty}\left(\sum\limits_{k=0}^n a_kb_{n-k}\right)$}


\newpage
\section{Arithmétique}
\Def{Division euclidienne}{Pour $a,b$ entiers, il existe un unique couple $(q,r)$ tel que :
\par $$\begin{cases} a = bq + r \\ r < b\end{cases}$$}
\Def{PGCD et PPCM}{Le PGCD de deux entiers est le maximum de l'intersection de l'ensemble de leurs diviseurs. On le note $pgcd(a,b) = a\wedge b$
\par Le PPCM de deux entiers est le minimum de l'intersection de l'ensemble de leurs multiples. On le note $ppcm(a,b) = a\vee b$
\par On dit que $a$ et $b$ sont premiers entre eux quand leur PGCD vaut $1$.
\par On a que $\vert ab\vert = (a\vee b)(a\wedge b)$}
\Def{}{Un entier est premier s'il admet deux diviseurs positifs : 1 et lui-même.}
\Thr{Théorème fondamental de l'arithmétique}{Tout entier peut s'écrire de manière unique sous la forme d'un produit de puissance de nombres premiers.}
\Thr{Lemme de Gauss}{Si $a$ et $b$ sont premiers entre eux alors :
\par $$ a\vert bc\Rightarrow a\vert c$$}
\Thr{Théorème de Bézout}{$a$ et $b$ sont premiers entre eux si, et seulement si, il existe $u,v$ un couple d'entiers tels que $au+bv=1$}

\section{Groupes et anneaux}
\Def{LCI}{Sur un ensemble $E$, une lci $\star$ est une application :
\par $$\star:\left\{\begin{array}{rcl} E\times E & \to & E \\ (x,y) & \mapsto & x\star y\end{array}\right.$$
\par On dit qu'elle est associative si :
\par $$\forall a,b,c\in E, a\star (b\star c) = (a\star b)\star c$$
\par On dit qu'elle est commutative si :
\par $$\forall a,b\in E, a\star b = b\star a$$}
\Def{Neutre}{Un élément neutre est un élément $e$ tel que :
\par $$\forall x\in E, x\star e= e\star x = x$$}
\Thr{Unicité du neutre}{Il n'y a qu'un seul élément neutre dans un groupe}
\Def{Symétrique}{Le symétrique $x'$ de $x\in E$ est un élément tel que, si la lci admet un neutre $e$ :
\par $$x\star x' = x'\star x = e$$
\par On note $x^{-1}$ le symétrique de $H$.}
\Thr{Unicité du symétrique}{Si la lci est associative, alors $x$ admet un unique symétrique}
\Def{Sous-groupe}{Si $G$ est un groupe pour la loi $\star$ de neutre $e$ :
\par $H$ est un sous-groupe de $G$ si :
\par $$\begin{cases} \forall (x,y)\in H, x\star y\in H \\
H\neq\emptyset (\Leftrightarrow e\in H)\\
\forall x\in H, x^{-1}\in H
\end{cases}$$}
\Def{}{Si $G$ et $G'$ sont deux groupes, alors $\varphi\in\mathcal{F}(G,G')$ est un morphisme si $\forall x,y\in G, \varphi(x\star y) = \varphi(x)\star'\varphi(y)$
\par On note $\ker(\varphi) =\varphi^{-1}(\{e_{G'}\})$ et $Im(\varphi) = \varphi(G)$}
\Thr{Propriétés du morphisme}{Si $\varphi$ est un morphisme de $G$ dans $G'$ :\begin{itemize}
\item $\varphi(e) = e'$ (avec $e$ neutre de $G$, $e'$ neutre de $G'$) ;
\item $$\forall x\in G, \varphi(x^{-1}) = (\varphi(x))^{-1}$$ ;
\item $$\forall n\in\Z,\forall x\in G, \varphi(x^n) = (\varphi(x))^n$$ ;
\item L'image directe d'un sous-groupe de $G$ par $\varphi$ est un sous-groupe de $G$ ; 
\item L'image réciproque d'un sous-groupe de $G'$ par $\varphi$ est un sous-groupe de $G$.
\item $\varphi$ est injective si, et seulement si, $\ker\varphi = \{e\}$
\end{itemize}}
\Def{}{Un morphisme bijectif est un isomorphisme, un automorphisme est un insomorphisme de $G$ dans $G$
\par Deux groupes sont isomorphes s'il existe une bijection entre les deux.
\par Un homomorphisme est un une application de $A$ dans $B$ (avec $A$ et $B$ deux anneaux) tel que :\begin{itemize}
\item $f(0_A) = 0_B$ et $f(1_A) = 1_B$
\item $f(x+_Ay) = f(x)+_B f(y)$
\item $f(x\times_A y) = f(x)\times_B f(y)$
\end{itemize}}

\Def{Anneau}{$(A,\intercal, \cdot)$ est un anneau si :\begin{itemize}
\item $(A, \intercal)$ est un groupe abélien (commutatif) ;
\item $\cdot$ est associative et possède un élément neutre ;
\item $\cdot$ est distributive par rapport à $\intercal$
\par i.e. $\forall a,b,c\in A, a\cdot (b\intercal c) = (a\cdot b)\intercal (a\cdot c)$
\end{itemize}}
\Def{Sous-anneau}{$B$ est un sous-anneau de $(A,+,\times)$ si :\begin{itemize}
\item $B$ sous-groupe de $A$
\item $B$ stable par $\times$
\item $B$ contient l'élément neutre pour la loi $\times$ de $A$.
\end{itemize}}
\Def{Intégrité}{Un anneau $A$non réduit à $0$ est intègre si il est commutatif et qu'il vérifie :
\par $$\forall a,b\in A, a\times b = 0\Rightarrow a = 0\text{ ou } b=0$$
\par On dit aussi que $A$ n'a pas de diviseurs de $0$}
\Def{Corps}{Un corps est un anneau intègre où tout élément admet un symétrique pour la loi multiplicative}

\section{Théorèmes d'algèbre}
\Def{Algèbre}{Un magma $(E, +,\times, \cdot)$ est une algèbre si, muni des lci $+$ et $\times$ et de la lce $\cdot$ :\begin{itemize}
\item $(E,+,\cdot)$ est un $\K$-ev 
\item $(E,+,\times)$ est un anneau 
\item pour $\lambda\in\K, a,b\in E, \lambda(ab)=a(\lambda b)$
\end{itemize}}
\Thr{Base incomplète}{Toute famille libre peut être complétée en base, on peut enlever des éléments à toute famille génératrice pour la transformer en base}
\Thr{du rang}{Si $f\in\mathcal{L}(E)$ et $E$ de dimension finie :
$$\dim E = \mathrm{rg}(f) + \dim\ker g$$}
\Thr{}{En dimension finie, l'injectivité, la surjectivité et la bijectivité sont équivalentes.}
\Thr{}{Si $p$ un projecteur, toutes ces conditions sont équivalentes :\begin{itemize}
\item $$\ker p\oplus Im p = E$$
\item $$\ker (p-id) = Im p$$
\item $$Im (p-id) = \ker p$$
\item $$p\circ p =p$$
\end{itemize}}
\Thr{}{Si $s$ est une symétrie, toutes ces conditions sont équivalentes :\begin{itemize}
\item $$\ker (s-id)\oplus \ker (s+id)=E$$
\item $$Im (s-id)\oplus Im(s+id)=E$$
\item $$s\circ s=id$$
\end{itemize}}
\Thr{Inversibilité d'une matrice}{Une matrice $M\in\mathcal{M}_n(\K)$ est inversible si, et seulement si, on a une des conditions équivalentes suivantes :\begin{itemize}
\item $\exists B\in\mathcal{M}_n(\K), MB = BM = I_n$
\item Il y a une suite de transformations élémentaires sur les lignes (resp. les colonnes) qui rend $M$ inversible
\item $M$ est la matrice canoniquement associée à un isomorphisme
\item $M$ est un produit de matrices inversibles
\item $M^T$ est inversible
\item Le système $AX =0$ admet une unique solution
\item $\det A\neq 0$
\end{itemize}}
\Thr{}{Le déterminant est une forme n-linéaire symétrique, et :
\par $$\det M =\sum\limits_{\sigma\in\mathfrak{S}_n}\varepsilon(\sigma)\prod\limits_{k=1}^nM_{\sigma(k),k}$$
\par Le déterminant ne dépend pas de la base choisie.
\par $$(\det MN) = (\det M)(\det N)$$
\par $$\det (\lambda M) = \lambda^n\det (M)$$
\par $$\det M^T = \det M$$
\par $$\det M^{-1} =\frac{1}{\det M}$$}

\section{Polynômes}
\Thr{Opérations sur le degré}{Avec $P,Q\in\K[X], \lambda\in\K, k\in\N$ :\begin{itemize}
\item Si $\lambda\neq 0$, alors $\deg\lambda P = \deg P$
\item $$\deg (PQ) = \deg P+\deg Q$$
\item $$\deg (P^k) = k\deg P$$
\item $$\deg (P+Q)\leq \max(\deg P, \deg Q)$$
\item $$\deg (P') = (\deg P )- 1$$
\item $\deg (P^{(k)}) = \deg (P) - k$ si $\deg P\geq k$ et $-\infty$ sinon
\end{itemize}}
\Thr{Division euclidienne}{Soient $A,B\in\K[X]$ avec $B\neq 0$, il existe un unique couple $(Q,R)\in\K[X]$ tel que :
\par $$A = BQ+R\text{ et }\deg R<\deg B$$}
\Thr{}{Si $P\in\K[X]$ non-nul, et $\lambda_1,...,\lambda_r$ ses racines deux à deux distinctes de multiplicités $m_1,...,m_r$, alors :
\par $$\prod\limits_{i=1}^r(X-\lambda_i)^{m_i}\vert P$$
\par Ce qui donne qu'un polynôme non-nul a au plus autant de racines comptées avec multiplicité que son degré.}
\Thr{Formule de Viète}{Soit $P = \sum a_kX^k$ scindé de degré $n$.
\par Notons $x_1,..., x_n$ ses racines.
\par Pour $k\in\llbracket 1, n\rrbracket$, on note $\sigma_k$ le $k$-ème polynôme symétrique élémentaire en les $x_i$ :
\par \begin{align*} \sigma_k &= \sum\limits_{1\leq i_1<...<i_k\leq n}x_{i_1}...x_{i_k}\\
&=(-1)^k\frac{a_{n-k}}{a_n}
\end{align*}}
\Thr{D'Alembert Gauss ou théorème fondamental de l'algèbre}{Tout polynôme non-constant de $\C[X]$ admet une racine.
\par De là, on déduit que tout polynôme de $\C[x]$ est scindé.
\par De là, on déduit que les polynômes irréductibles de $\C$ sont les $(X-\lambda)_{\lambda\in\C}$
\par De là, on déduit que les polynômes irréductibles de $\R$ sont les $(X-\lambda)_{\lambda\in\R}$ et les $(X^2+bX+c)_{(b,c)\in\R^2\vert b^2-4c<0}$}
\Thr{Bézout polynomial}{Deux polynômes $A,B\in\K[X]$ sont premiers entre eux si, et seulement si, il exists $U,V\in\K[X]$ tels que $AU+BV=1$}
\Thr{}{Si $A,B\in\K[X]$ sont unitaires, alors :
\par $$AB = (A\wedge B)(A\vee B)$$}
\Thr{Lemme d'Euclide}{Un polynôme irréductible divise un produit si, et seulement si, il divise l'un des facteurs.}
\Thr{}{Si $A\in\K[X]$ non-constant, alors il existe $\alpha\in\K^*$, des polynômes irréductibles unitaires deux à deux distincts $P_1,...,P_r$ et des entiers strictement positifs $m_1,..., m_r$ tels que :
\par $$A =\alpha\prod\limits_{i=1}^r P_i^{m_i}$$
\par Cette décomposition est unique à l'ordre des facteur près.}
\Thr{Décomposition en éléments simples}{Si $F=\frac{A}{B}$ est une fraction rationnelle, elle s'écrit de manière unique comme somme d'un polynômme (la partie entière de $F$) et d'éléments simples.}



\chapter{Maths - spé}
\section{Analyse - discret}
\subsection{Suites de fonctions}
\Def{Convergence simple}{Soit $E,F$, 2 $\K$-ev de dimension finie. Soit $A\subset E$. Soit $(f_n)\in\mathcal{F}(A,F)^\N$.
\par On dit que $(f_n)$ converge simplement vers $g\in\mathcal{F}(A,F)$ si $\forall t\in A, (f_n(t))\to g(t)$}
\Def{Convergence uniforme}{Soit $E,F$, 2 $\K$-ev de dimension finie. Soit $A\subset E$. Soit $(f_n)\in\mathcal{F}(A,F)^\N$.
\par On dit que $(f_n)$ converge uniformément vers $g\in\mathcal{F}(A,F)$ si $\sup\limits_{A}\Vert f_n-g\Vert \to_{n\to\infty} 0$}
\Thr{}{Toute suite de fonctions qui converge uniformément converge simplement.}
\Thr{Continuité uniforme}{Soit $(f_n)\in\mathcal{F}(A,F)^\N$ une suite de fonctions.
\par Si $\forall n\in\N$, $f_n$ est continue sur $A$ et que $(f_n)$ converge uniformémement vers $g$, alors $g$ est continue.
\par Ce qui correspond à : une limite uniforme de fonctions continues est continue.}
\Thr{Extension de limite uniforme}{Soit $(f_n)\in\mathcal{F}(A,F)^\N$, soit $a\in\bar{A}$.
\par Si $\forall n\in\N, f_n(x)\to_{x\to a}l_n\text{ et }f_n\text{ converge uniformement vers }g$
\par Alors $(l_n)$ est convergente et $g(x)\to_{x\to a}\lim\limits_{n\to+\infty}l_n$
\par ie : $g$ a une limite finie en $a$ et $\lim\limits_{x\to a} \lim\limits_{n\to+\infty}l_n = \lim\limits_{n\to+\infty}\lim\limits_{x\to a} f_n$
\par Ce théorème s'étend lorsque $E=\R$ et que $a=\pm\infty$}
\Thr{Intégration uniforme ou théorème d'échange limite-intégrale uniforme}{Soit ${a,b}$ un segment, $(f_n)\in\mathcal{C}_0([a,b],F)^\N$
\par si $(f_n)$ converge uniformément vers $g$ sur $[a,b]$, alors $\int_a^bf_n\to \int_a^bg$
\par Ce qui correspond à $\lim\limits_{n\to+\infty}\int_a^bf_n = \int_a^b\lim\limits_{n\to+\infty}f_n$}
\Thr{}{Soit $(f_n)$ une suite de fonctions continues d'un intervalle $I$ de $\R$ à valeurs dans $F$ convergeant uniformément vers $g$ sur tout segment de $I$
\par Soit $a\in I$, on a alors : $F_n:\left\{\begin{array}{rcl}I & \to & F \\ x & \mapsto & \int_a^bf_n(t)dt\end{array}\right.$ et $G:\left\{\begin{array}{rcl}I & \to & F \\ x & \mapsto & \int_a^bg(t)dt\end{array}\right.$
\par Alors $F_n$ converge uniformément vers $G$ sur tout segment de $I$.}
\Thr{Dérivation uniforme des suites de fonctions}{Avec $I$ un intervalle, si :\begin{itemize}\item $(f_n)\in\mathcal{C}^1(I,F)^\N$ \item $(f_n)$ converge simplement vers $g_0$ \item $(f'_n)$ converge uniformément vers $g_1$ sur tout segment de $I$\end{itemize} alors $g_0$ est $\mathcal{C}^1$ tel que $g_0'=g_1$ et $(f_n)$ converge uniformément sur tout segment de $I$.
\par Ce qui correspond à $g_0'=g_1 \Rightarrow (\lim f_n)=\lim (f_n')$}
\Thr{Théorème de dérivation des suites à l'ordre k}{Avec $I$ un intervalle, si :\begin{itemize}\item $(f_n)\in\mathcal{C}^k(I,F)^\N$ \item $\forall j\in\llbracket0,k-1\rrbracket, (f_n^{(j)})$ converge simplement vers $g_j$ \item $(f_n^{(k)})$ converge uniformément vers $g_k$ sur tout segment de $I$\end{itemize} alors $g_0$ est $\mathcal{C}^l$ tel que $\forall j\in\llbracket0,k\rrbracket, g_0^{(j)}=g_j$ et $(f_n^{(j)})$ converge uniformément sur tout segment de $I$.}
\Thr{Stone-Weierstrass (admis)}{Toute fonction continue sur un segment à valeurs dans $\K$ est limite uniforme d'une suite de fonctions polynomiales.
\par Ce qui correspond à : l'ensemble des fonctions polynomiales est dense dans $(\mathcal{C}([a,b],\K),\Vert.\Vert_\infty)$}
\Thr{Approximmation uniforme par des fonctions en escalier}{Toute fonction continue sur un segment à valeurs dans $F$ est limite uniforme d'une suite de fonctions en escaliers
\par Ce théorème est encore valable pour les fonctions continues par morceaux sur un segment.}


\subsection{Séries}
\Thr{Critère de d'Alembert}{Soit $(u_n)\in\R_+^{*\N}$. Si $\left(\dfrac{u_{n+1}}{u_n}\right)\to l\in\R$, alors :\begin{itemize}
\item Si $l<1$ alors $\sum u_n$ converge.
\item Si $l>1$ alors $\sum u_n$ diverge grossièrement.
\item Si $l=1$ alors on ne peut rien dire.\end{itemize}
}	
\Thr{Sommation des ordres de grandeur}{Avec $(a_n),(b_n)$ deux suites réelles positives :\begin{itemize}
\item Si $b_n = O(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k = O(\sum\limits_{k=n+1}^{+\infty} a_k)$ ; si $\sum a_n$ diverge, alors $\sum\limits_{k=0}^n b_k = O(\sum\limits_{k=0}^n a_k)$.
\item Si $b_n = o(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k = o(\sum\limits_{k=n+1}^{+\infty} a_k)$ ; si $\sum a_n$ diverge, alors $\sum\limits_{k=0}^n b_k = o(\sum\limits_{k=0}^n a_k)$.
\item Si $b_n\sim(a_n)$ : si $\sum a_n$ converge, alors $\sum b_n$ converge et $\sum\limits_{k=n+1}^{+\infty} b_k \sim \sum\limits_{k=n+1}^{+\infty} a_k$ ; si $\sum a_n$ diverge, alors $\sum b_n$ diverge et $\sum\limits_{k=0}^n b_k \sim \sum\limits_{k=0}^n a_k$.
\end{itemize}}

\subsection{Séries de fonctions}
\Def{Convergences}{Soit $(u_n)\in\mathcal{F}(A,F)^\N$, on dit que la série de fonction $(\sum u_n)$ converge simplement si la suite des sommes partielles $\left(\sum\limits_{k=0}^n u_k\right)$ converge simplement.
\par On dit que $(\sum u_n)$ converge uniformément si la suite des sommes partielles $\left(\sum\limits_{k=0}^nu_k\right)$ converge uniformément.
On dit que $\sum u_n$ converge normalement si $\sum \sup\limits_A \Vert u_n\Vert$ converge.
}
\Thr{}{Tout série de fonction qui converge normalement converge uniformément.}
\Thr{continuité uniforme des séries de fonctions}{Soit $(u_n)\in \mathcal{C}(A,F)^\N$, si $\sum u_n$ converge uniformément sur $A$, alors $x\mapsto\sum\limits_{n=0}^{+\infty}u_n(x)$ est continue.}
\Thr{échange de limites de séries}{Avec $(u_n)\in\mathcal{F}(A,F)^\N$ et $a\in\bar{A}$
\par Si $\forall n\in\N, u_n(x)\to_{x\to a}v_n$ et que $(\sum u_n)$ converge uniformément, alors $\left(\sum\limits_{n=0}^{+\infty}u_n\right)$ a une limite en $a$ et 
\par $$\lim\limits_{x\to a}\sum\limits_{n=0}^{+\infty}u_n(x)=\sum\limits_{n=0}^{+\infty}v_n$$}
\Thr{Théorème de dérivation terme à terme à l'ordre k}{Avec $I$ un intervalle, si :\begin{itemize}
\item $(u_n)\in\mathcal{C}^1(I,F)^\N$ 
\item $(\sum u_n)$ converge simplement 
\item $(\sum u_n^{(k)})$ converge uniformément sur tout segment de $I$
\end{itemize}
alors $\sum\limits_{n=0}^{+\infty}u_n$ est $\mathcal{C}^1$ tel que $\forall j\in\llbracket0,k\rrbracket, \left(\sum\limits_{n=0}^{+\infty}u_n\right)'=\sum\limits_{n=0}^{+\infty}u_n'$ et la somme converge uniformément sur tout segment de $I$.}
	

\subsection{Séries entières}
\Def{}{Soit $(a_n)\in\C^\N$. On appelle série entière associée à $(a_n)$ (de variable complexe) la série de fonctions $\left(\sum (z\mapsto a_nz^n)\right)$ qu'on notera en général $\left(\sum a_nz^n\right)$
\par $z\mapsto \sum\limits_{n=0}^{+\infty}a_nz^n$ est la somme de cette série entière.}
\Def{Rayon de convergence}{Soit $\sum a_nz^n$ une série entière. On appelle rayon de cette série $R = \sup\{\vert z\vert | z\in\C, (a_nz^n)\text{ est bornée}\}$ \par Par convention, $R=+\infty$ si cet ensemble n'est pas majoré.}
\Thr{Lemme d'Abel}{Soit $(\sum a_nz^n)$ une série entière. Soit $z_0\in\C^*$. Si $(a_nz_0^n)$ est bornée, alors la série $\sum a_nz^n$ converge absolument pour $z\in\C,\vert z\vert < \vert z_0\vert$}
\Thr{Relations de comparaison}{Soient $\sum a_nz^n$ et $\sum b_nz^n$ des séries entières de rayons $R_a$ et $R_b$ :\begin{itemize}
\item Si $a_n = o(b_n)$ alors $R_a\geq R_b$
\item Si $a_n = \mathcal{O}(b_n)$ alors $R_a\geq R_b$
\item Si $a_n\sim b_n$ alors $R_a=R_b$
\end{itemize}}
\Thr{Critère de D'Alembert}{Si $(a_n)$ ne s'annule pas à partir d'un certain rang :
\par si $\left\vert\frac{a_{n+1}}{a_n}\right\vert\to l$ avec $l\in\bar{\R}$, alors $R_a = \frac{1}{l}$
\par (On prend la convention de $\frac{1}{+\infty}=0$ et que $\frac{1}{0}=+\infty$)}
\Thr{Produit de Cauchy}{Pour $\sum a_nz^n$ et $\sum b_nz^n$ deux séries entières de rayons respectifs $R_a$ et $R_b$ \par On pose pour $n\in\N$, $c_n = \sum\limits_{k=0}^na_kb_{n-k}$ \par La série entière $\sum c_nz^n$ est de rayon de convergence $R\geq\min(R_a,R_b)$ et $\forall z\in\C$ tel que $\vert z\vert < \min(R_a,R_b), \sum\limits_{n=0}^{+\infty}c_nz^n = \sum\limits_{n=0}^{+\infty}a_nz^n\sum\limits_{n=0}^{+\infty}b_nz^n$}
\Thr{Continuité}{La somme d'une série entière est continue sur son disque ouvert de convergence}
\Thr{}{Soit $(a_n)\in\C^\N$, alors les séries entières $\sum a_nz^n$ et $\sum na_nz^n$ ont même rayon.}
\Thr{Corollaire}{La somme d'une série entière est $\mathcal{C}^\infty$ sur son intervalle ouvert de convergence
\par Les dérivées s'obtiennent par dérivation terme à terme}

\Thr{Convergence radiale}{Soit $(a_n)\in\C^\N$, $(\sum a_nz^n)$ de rayon $R>0$ \par Si $\sum a_nR^n$ converge alors : $\sum\limits_{n=0}^{+\infty}a_nx^n\to_{x\to R}\sum\limits_{n=0}^{+\infty}a_nR^n$ pour $x\in]-R,R[$
\par Plus précisément, si $f$ définie en $R$ en tant que somme de série entière, alors $f$ continue sur $\mathcal{D}_f$}


\section{Analyse - intégration}
\subsection{Intégration - intégrales impropres}
\Def{}{Soit $f:[a,b[\to\K$ une fonction continue par morceaux avec $b\in\R\cup\{+\infty\}$
\par Notons $F$ la fonction :
\par $$F :\left\{\begin{array}{rcl}[a,b[ & \to & \K \\ x & \mapsto & \int_a^x f\end{array}\right.$$
\par On dit que $\int_a^bf$ est convergente si $F(x)$ a une limite finie quand $x$ tend vers $b$.
\par Dans ce cas, on note :
\par $$\int_a^bf=\lim\limits_{x\to b}\int_a^xf$$
\par Dans le cas contraire, on dit que $\int_a^bf$ est divergente.
\par Etudier la nature de $\int_a^bf$, c'est étudier si l'intégrale est convergente ou divergente.}
\Def{}{Soit $I$ un intervalle, $f$ continue par morceaux sur $I$ à valeurs dans $\K$ \par On dit que $\int_If$ est absolument convergente si $\int_I\vert f\vert$ converge.}

\Thr{}{Soit $a\in \R$, $b\in\R\cup\{+\infty\}$ avec $a<b$. Soit $f\in\mathcal{CM}([a,b[, \R)$
\par Si $f$ est positive, l'intégrale $\int_a^bf$ converge si, et seulement si, $x\mapsto\int_a^xf$ est majorée.}
\Thr{}{Soit $f$ continue par morceaux sur un intervalle $I$.
\par Si $\int_If$ est absolument convergente alors $\int_If$ est convergente.}

\Thr{}{Soit $a\in\R, b\in\R\cup\{+\infty\}$ avec $a<b$
\par Soit $f,g\in\mathcal{CM}([a,b[,\R)$ deux fonctions à valeurs positives telles que $0\leq f\leq g$:\begin{itemize}
\item Si $\int_a^bg$ converge alors $\int_a^bf$ converge.
\item Si $\int_a^bf$ diverge alors $\int_a^bg$ diverge.
\end{itemize}}
\Thr{Corollaire}{Soit $a\in\R, b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f,g\in\mathcal{CM}([a,b[,\R)$ deux fonctions à valeurs positives telles que $f=_bo(g)$ ou $f=_b\mathcal{O}(g)$ :\begin{itemize}
\item Si $\int_a^bg$ converge alors $\int_a^bf$ converge.
\item Si $\int_a^bf$ diverge alors $\int_a^bg$ diverge.
\end{itemize}}
\Thr{}{$\int_1^{+\infty}\frac{dt}{t^\alpha}$ converge si, et seulement si, $\alpha>1$}
\Thr{}{$\int_0^{+\infty}e^{-\lambda t}dt$ converge si, et seulement si, $\lambda>0$}
\Thr{}{$\int_0^1\frac{dt}{t^\alpha}$ converge si, et seulement si, $\alpha<1$}
\Thr{}{$\int_0^1ln(t)dt$ converge}
\Thr{}{Si $a<b$ : \par $\int_a^b\frac{dt}{\vert t-a\vert^\alpha}$ converge si, et seulement si, $\alpha<1$}
\Thr{}{Si $a>b$ : \par $\int_b^a\frac{dt}{\vert b-t\vert^\alpha}$ converge si, et seulement si, $\alpha<1$}


\subsection{Propriétés des intégrales}
\Thr{Sommes de Riemann}{Si $f$ est continue sur $[a,b]$, alors :
\par $$\frac{b-a}{n}\sum\limits_{k=0}^{n-1}f\left(a + k\frac{b-a}{n}\right)\to_{n\to+\infty}\int_a^bf(t)dt$$}
\Thr{Propriétés des intégrales impropres}{\begin{itemize}
\item Linéarité : si $I$ un intervalle, $f,g$ continues par morceaux sur $I$ d'intégrales convergentes sur $I$, $\lambda\in\K$, alors : \par \begin{center}$\int_If + \lambda g$ converge et $\int_If+\lambda g = \int_I f+\lambda\int_Ig$ \end{center}
\item Positivité : si $f$ continue par morceaux sur $I$ réelle positive, d'intégrale sur $I$ convergente, alors $\int_If\geq 0$
\item Croissance : si $f$ et $g$ sont continues par morceaux sur $I$i réelles positives d'intégrales sur $I$ convergentes avec $f\leq g$, alors $\int_If\leq \int_Ig$
\item Inégalité triangulaire : si $f$ continue par morceaux sur $I$ intégrable sur $I$, alors $\left\vert\int_If\right\vert \leq\int_I\vert f\vert$ 
\item Positivité améliorée : si $I$ un intervalle, $f$ continue réelle positive sur $I$ d'intégrale sur $I$ convergente, alors $\int_If = 0 \Rightarrow \forall x\in I, f(x)=0$
\item Relation de Chasles : Soit $I$ un intervalle, soit $f\in\mathcal{CM}(I,\K)$. Soit $a,b,c$ dans l'adhérence de $I$ dans $\R\cup\{+\infty\}$
\par Si $\int_If$ converge, alors $\int_a^cf, \int_c^bf$ et $\int_a^bf$ convergent et
\par $$\int_a^bf=\int_a^cf+\int_c^bf$$
\item Intégration par parties :Avec $f,g$ $\mathcal{C}^1$ sur $]a,b[$,  si $fg$ a une limite en $a^+$ et en $b^-$ alors $\int_a^bf(t)g'(t)dt$ et $\int_a^bf'(t)g'(t)$ sont de même nature et en cas de convergence :
\par $$\int_a^bf(t)g'(t) = [f(t)g(t)]_a^b -\int_a^bf'(t)g(t)dt$$
\item Changement de variables : Soient $a,b,\alpha, \beta$ tels que $-\infty\leq a<b\leq +\infty, -\infty\leq\alpha<\beta\leq+\infty$
\par Soit $f\in]a,b[\to \K$ une fonction continue.
\par Soit $\varphi:]\alpha,\beta[\to]a,b[$ une fonction bijective, strictement croissante et de classe $\mathcal{C}^1$
\par Les intégrales $\int_a^bf(t)dt$ et $\int_\alpha^\beta(f\circ\varphi)(u)\varphi'(u)du$ sont de même nature, et en cas de convergence
\par $$\int_a^bf(t)dt=\int_\alpha^\beta(f\circ\varphi)(u)\varphi'(u)du$$
\end{itemize}}
\Thr{Intégration des ordres de grandeur}{Soit $a\in\R$ et $b\in\R\cup\{+\infty\}$ avec $a<b$ \par Soit $f\in\mathcal{CM}([a,b[,\K)$ \par Soit $\varphi\in\mathcal{CM}([a,b[,\R)$ une fonction positive sur $[a,b[$\begin{itemize}
\item Si $\varphi$ est intégrable :\begin{itemize}
	\item Si $f=_b\mathcal{O}(\varphi)$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf=_b\mathcal{O}(\int_x^b\varphi)$
	\item Si $f=_bo(\varphi)$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf=_bo(\int_x^b\varphi)$
	\item Si $f\sim_b\varphi$ alors $f$ est intégrable sur $[a,b[$ et $\int_x^bf\sim_b\int_x^b\varphi$
\end{itemize}
\item Si $\varphi$ n'est pas intégrable :\begin{itemize}
	\item Si $f=_b\mathcal{O}(\varphi)$ alors $\int_a^xf=_b\mathcal{O}(\int_a^x\varphi)$
	\item Si $f=_bo(\varphi)$ alors $\int_a^xf=_bo(\int_a^x\varphi)$
	\item Si $f\sim_b\varphi$ alors $f$ n'est pas intégrable sur $[a,b[$ et $\int_a^xf\sim_b\int_a^x\varphi$
\end{itemize}
\end{itemize}}


\subsection{Intégration - théorèmes de Lesbesgues}
\Thr{de convergence dominée}{Soit $(f_n)$ une suite de fonctions continues par morceaux de $I$ intervalle de $\R$ dans $\K$. On suppose que :\begin{itemize}
\item La suite $(f_n)$ converge simplement sur $I$ vers une fonction $f$ continue par morceaux
\item Il existe une fonction $\varphi$ positive et intégrable sur $I$ telle que
\par\begin{center}$\forall n\in\N, \vert f_n\vert\leq\varphi$ (hypothèse de domination)\end{center}
\end{itemize}
Alors les fonctions $f_n$ pour $n\in\N$ et la fonction $f$ sont intégrables sur $I$ et
\par$$\int_If_n\to\int_If$$}
\Thr{Intégration terme à terme positive}{Soit $I$ une intervalle, soit $(u_n)$ une suite de fonctions définies de $I$ dans $\R_+^*$. On suppose que :\begin{itemize}
\item Pour tout $n\in\N$, $u_n$ est continue par morceaux et intégrable sur $I$
\item $(\sum u_n)$ converge simplement
\item $\sum\limits_{n=0}^{+\infty}u_n$ est continue par morceaux sur $I$.
\end{itemize} 
Alors :
\par $$\int_I\sum\limits_{n=0}^{+\infty}u_n = \sum\limits_{n=0}^{+\infty}\int_I u_n$$}
\Thr{Intégration terme à terme}{Soit $I$ un intervalle \par Soit $(u_n)$ une suite de fonctions à valeur dans $\K$. On suppose que :\begin{itemize}
\item Pour tout entier $n\in\N$, $u_n$ est continue par morceaux et intégrable sur $I$
\item La série $\sum u_n$ converge simplement et $\sum\limits_{n=0}^{+\infty}u_n$ est continue par morceaux sur $I$
\item La série $\sum\int_I\vert u_n\vert$ converge
\end{itemize}
Alors $\sum\limits_{n=0}^{+\infty}u_n$ est intégrable sur $I$ et
\par $$\int_I\sum\limits_{n=0}^{+\infty}u_n=\sum\limits_{n=0}^{+\infty}\int_Iu_n$$}
\Thr{échange des limites non-discrètes}{Soient $I$, $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$. Soit $\lambda_0$ dans l'adhérence de $J$ ($\in\bar{R}$). On suppose que :\begin{itemize}
\item pour tout $\lambda\in J$, la fonction $t\mapsto f(\lambda,t)$ est continue par morceaux sur $I$
\item il existe une fonction $l$ continue par morceaux de $I$ dans $\K$ telle que pour tout $t\in I, \lim_{\lambda\to \lambda_0}f(\lambda,t)=l(t)$
\item Il existe une fonction $\varphi$ continue par morceaux positive et intégrable sur $I$ telle que :
\par \begin{center}$\forall (\lambda, t)\in J\times I, \vert f(\lambda, t)\vert\leq\varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
Alors les fonctions $t\mapsto f(\lambda, t)$ (pour tout $\lambda\in J$) et la fonction $l$ sont intégrables sur $I$ et :
\par $$\lim_{\lambda\to\lambda_0}\int_If(\lambda, t)dt = \int_I l(t)dt$$}
\Thr{Autre formulation}{Soit $I$ et $J$ deux intervalles de $\R$, $(f_\lambda)_{\lambda\in J}$ une famille de fonctionns définie sur $J$ dans $\K$. Soit $\lambda_0$ dans l'adhérence de $J$ (dans $\bar{\R}$). On suppose que :\begin{itemize}
\item pour tout $\lambda\in J$, la fonction $f_\lambda$ est continue par morceaux sur $I$
\item il existe une fonction $l$ continue par morceaux de $I$ dans $\K$ telle que pour tout $t\in I$, $\lim\limits_{\lambda\to\lambda_0}f_\lambda(t)=l(t)$
\item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que
\par \begin{center}$\forall (\lambda, t)\in J\times I, \vert f_\lambda(t)\vert\leq \varphi(t)$ (hypothèse de domination)\end{center}
\end{itemize}
Alors les fonctions $f_\lambda$ (pour $\lambda\in J$) et $l$ sont intégrables sur $I$ et
\par $$\lim\limits_{\lambda\to\lambda_0}\int_If_\lambda=\int_Il$$}

\Thr{Continuité dominée}{Soit $A$ une partie d'un EVN de dimension finie, $I$ un intervalle de $\R$, $f$ une fonction définie sur $A\times I$ à valeurs dans $\R$. On suppose que :\begin{itemize}
\item pour tout $x\in A$, la fonction $t\mapsto f(x,t)$ est continue par morceaux sur $I$
\item pour tout $t\in I$, la fonction $x\mapsto f(x,t)$ est continue sur $A$
\item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que
\par \begin{center}$\forall (x,t)\in A\times I, \vert f(x,t)\vert\leq \varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
Alors, pour tout $x\in A$, la fonction $t\mapsto f(x,t)$ est intégrable sur $I$ et la fonction
\par \begin{center} $g:\left\{\begin{array}{rcl}A & \to & \K \\ x & \mapsto & \int_If(x,t)dt\end{array}\right.$\end{center} \par est continue.}
\Thr{Extension}{Si l'hypothèse de domination est satisfaite au voisinage d'un point $a$ de $A$, on peut en conclure la continuité de $x\mapsto \int_If(x,t)dt$ en $a$
\par Si $A$ est un intervalle de $\R$, et que l'hypothèse de domination est satisfaite sur tout segment de $A$, alors
\par \begin{center} $g:\left\{\begin{array}{rcl}A&\to&\K\\x&\mapsto&\int_If(x,t)dt\end{array}\right.$ est continue\end{center}}
\Thr{Dérivabilité}{Soit $I$ et $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$. On suppose que :\begin{itemize}
\item pour tout $x\in J$, la fonction $t\mapsto f(x,t)$ est continue par morceaux et dérivable sur $I$
\item la fonction $f$ admet sur $J\times I$ une dérivée parielle par rapport à la première variable, $\dfrac{\partial f}{\partial x}$
\item la fonction $\dfrac{\partial f}{\partial x}$ verifie les hypothèses du théorème 36 :\begin{itemize}
	\item pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est continue par morceaux sur $I$
	\item pour tout $t\in I$, la fonction $x\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est continue sur $J$
	\item il existe une fonction $\varphi$ continue par morceaux, positive et intégrable sur $I$ telle que
	\par \begin{center}$\forall (x,t)\in J\times I, \left\vert\dfrac{\partial f}{\partial x}(x,t)\right\vert\leq\varphi(t)$ (hypothèse de domination) \end{center}
\end{itemize}
\end{itemize}
Alors pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial f}{\partial x}(x,t)$ est intégrable sur $I$,
\par la fonction $g:x\mapsto \int_If(x,t)dt$ est de classe $\mathcal{C}^1$ sur $J$ et vérifie :
\par $$\forall x\in J, g'(x)=\int_I\dfrac{\partial f}{\partial x}(x,t)dt$$}
\Thr{Classe d'une intégrale à paramètre}{Soit $I$ et $J$ deux intervalles de $\R$, $f$ une fonction définie sur $J\times I$ à valeurs dans $\K$ et $k\in\N^*$. On suppose que :\begin{itemize}
\item pour tout $j\in\llbracket 0,k\rrbracket$, la fonction $f$ admet sur $J\times I$ une dérivée partielle d'ordre $j$ par rapport à la première variable, $\dfrac{\partial^j f}{\partial x^j}$
\item pour tout $j\in\llbracket 0,k-1\rrbracket$, pour tout $x\in J$, la fonction $\dfrac{\partial^j f}{\partial x^j}(x,t)$ est continue par morceaux et intégrable sur $I$
\item pour tout $t\in I$, pour tout $j\in\llbracket 0,k\rrbracket$, $x\mapsto \dfrac{\partial^j f}{\partial x^j}(x,t)$ est continue sur $J$
\item pour tout segment $K$ inclus dans $J$, il existe une fonction $\varphi_K$ continue par morceaux, positive et intégrable sur $I$ telle que
\par \begin{center}$\forall (x,t)\in K\times I, \left\vert\dfrac{\partial^k f}{\partial x^k}(x,t)\right\vert\leq \varphi_K(t)$ (hypothèse de domination sur tout segment)\end{center}
\end{itemize}
Alors, pour tout $x\in J$, la fonction $t\mapsto \dfrac{\partial^kf}{\partial x^k}(x,t)$ est intégrable sur $I$, la fonction $g:x\mapsto\int_If(x,t)dt$ est de classe $\mathcal{C}^k$ sur $J$ et vérifie
\par $$\forall x\in J, g^{(k)}(x)=\int_I\dfrac{\partial^k f}{\partial x^k}(x,t)dt$$}


\newpage
\section{Analyse - différentiation}
\subsection{Les équations différentielles}
\Def{Premier ordre}{Soit $I$ un intervalle, soit $E$ un $\K$-ev de dimension finie.
\par Soit $a$ une application continue de $I$ dans $\mathcal{L}(E)$
\par Soit $b$ une application continue de $I$ dans $E$
\par On appelle $y' + a\cdot y = b$ équation différentielle linéaire du premier ordre normalisée.
\par Ses solutions sont les fonctions $y\in\mathcal{D}(I, E)$ vérifiant $\forall t\in I, y'(t)+a(t)\cdot y(t)=b(t)$}
\Def{Traduction matricielle}{Avec les notations ci-dessus, on fixe une base $\mathcal{B}$ de $E$.
\par $\forall t\in I, Y(t) = Mat_{\mathcal{B}}y(t) \Rightarrow Y'(t) = Mat_\mathcal{B}y'(t)\in\mathcal{M}_{n,1}(\K)$
\par $A(t) = Mat_\mathcal{B}a(t)\in\mathcal{M}_n(\K)$
\par $B(t) = Mat_\mathcal{B}b(t)\in\mathcal{M}_{n,1}(\K)$
\par L'équation devient :
\par $$Y'(t) = A(t)Y(t) +B(t)$$}
\Def{Equation homogène associée}{Avec $(E_q) : y' + a\cdot y = b$
\par (où $a\in\mathcal{C}(I, \mathcal{L}(E))$ et $b\in\mathcal{C}(I,E)$)
\par L'équation homogène associée est :
\par $$(H): y' + a\cdot y = 0$$}
\Thr{Résolution de l'équation différentielle linéaire normalisée à coefficients constants}{Notons $\forall t\in I,y'(t) = a\cdot y(t)$, où $a\in\mathcal{L}(E)$
\par L'ensemble des solutions de cette équation est : $\{t\mapsto \exp(t a)\cdot x\vert x\in E\}$
\par plus précisément, la solution du problème de Cauchy $\left\{\begin{array}{l} y' = a\cdot y \\ y(t_0) = x_0\end{array}\right.$ est $t\mapsto \exp((t-t_0)a)\cdot x_0$
}
\Thr{Superposition}{$(E_{q_1})$ et $(E_{q_2})$ deux équations de même équation homogène associé et $\lambda\in\K$ :
\par $(E_{q_1}) : y' + a(t)\cdot y = b_1$
\par $(E_{q_2}) : y' + a(t)\cdot y = b_2$
\par $$(E_{q_+}) : y' + a(t)\cdot y = b_1+b_2$$
\par $$(E_{q_\lambda}) : y' + a(t)\cdot y = \lambda b_1$$
\par Si $y_1\in S_{E_{q_1}}$ et $y_2\in S_{E_{q_2}}$, alors :
\par $$ y_1+y_2\in S_{E_{q_+}}$$
\par et$$ \lambda y_1 \in S_{E_{q_\lambda}}$$}
\Thr{Cauchy-Lipschitz linéaire}{Soit $I$ un intervalle de $\R$
\par Soit $E$ un $\K$-ev de dimension finie
\par Soit $(E_q)$ une équation différentielle linéaire normalisée $y'+a\cdot y = b$
\par Soit $t_0\in I$, $y_0\in E$, alors :
\par Il existe une unique solution $f$ de $(E_q)$ vérifiant :
\par $$f(t_0)=y_0$$}
\Thr{Corollaire}{Si $I$ est un intervalle et $(H) : y' = a\cdot y$ une équation différentielle linéaire \textbf{normalisée}
\par L'ensemble des solutions de $(H)$ sur $I$ à valeurs dans $E$ est un $\K$-ev de dimension $\dim E$}
\Thr{Variation des constantes}{Soit $I$ un intervalle, $(E_q)$ une équation différentielle linéaire normalisée $y'=a\cdot y + b$
\par $(H)$ l'équation homogène associée.
\par Si $(u_1,...,u_p)$ est une base de $S_H$
\par Alors $(E_q)$ possède une solution particulière de la forme $t\mapsto =\lambda_1(t) +...+\lambda_p(t)u_p(t)$
\par où $\lambda_1,...,\lambda_p$ sont des fonctions dérivables à valeurs dans $\K$}


\subsection{Calcul différentiel - différentiabilité et classe}
\Def{}{Soit $f$ un efonction définie d'un ouvert $U$ dans $F$ et $a$ un point de $U$.
\par On dit que $f$ est différentiable au point $a$ s'il existe une application $u\in\mathcal{L}(E,F)$ telle qu'au voisinage de $0$ :
\par $$f(a+h) = f(a) + u(h) + o(h)$$
\par Dans ce cas, une telle application linéaire $u$ est unique, on la note $df(a)$ et on l'appelle différentielle de $f$ en $a$.
\par On l'appelle aussi application linéaire tangente à $f$ en $a$.}
\Def{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$. Si $f$ est différentiable en tout point de $U$, on dit que $f$ est différentiable sur $U$ et on appelle différentielle de $f$ sur $U$ l'application :
\par $$df:\left\{\begin{array}{rcl}U & \to & \mathcal{L}(E,F) \\ a & \mapsto & df(a)\end{array}\right.$$}
\Thr{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
Si $f$ est différentiable en $a$, alors $f$ est continue en $a$.}
\Thr{}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
\par Si $f$ est différentiable en $a$, alors $f$ est dérivable en $a$ selon tout vecteur $v\in E$ et :
\par $$D_vf(a) = df(a)\cdot v$$}
\Thr{Corollaire}{Soit $\mathcal{B}=(e_1,..., e_n)$ une base de $E$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ et $a\in U$.
\par Si $f$ est différentiable en $a$, alors $f$ admet des dérivées partielles (dans la base $\mathcal{B}$) et pour $v=\sum\limits_{i=1}^nv_ie_i\in E$ :
\par $$D_vf(a) = df(a)\cdot f = \sum\limits_{i=1}^nv_i\partial_if(a)$$}
\Def{Matrice Jacobienne}{Si $E=\R^m$ et $F=\R^n$. Soit $f$ une fonction définie d'un ouvert $U$ dans $F$ différentiable sur $U$.
\par Soit $a\in U$. La matrice, dans les bases canoniques, de l'application linéaire $df(a)$ est :
\par $$J_f(a) = \begin{pmatrix} \dfrac{\partial f_1}{\partial x_1}(a) & \dfrac{\partial f_1}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_1}{\partial x_m}(a)
\\ \dfrac{\partial f_2}{\partial x_1}(a) & \dfrac{\partial f_2}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_2}{\partial x_m}(a)
\\ . & . & . & .
\\ . & . & . & .
\\ . & . & . & .
\\ \dfrac{\partial f_n}{\partial x_1}(a) & \dfrac{\partial f_n}{\partial x_2}(a) & \cdot\cdot\cdot & \dfrac{\partial f_n}{\partial x_m}(a)\end{pmatrix}$$
\par et est appelée matrice jacobienne de $f$ en $a$.}
\Thr{Différentielle d'une combinaison linéaire}{Soit $f$ et $g$ deux fonctions définies d'un ouvert $U$ dans $F$, différentiables sur $U$. Soit $\lambda, \mu$ deux réels.
\par La fonction $\lambda f +\mu g$ est différentiable sur $U$ et :
\par $$d(\lambda f+\mu g) = \lambda df+\mu dg$$}
\Thr{Différentielle d'une application bilinéaire}{Soit $E, F, G, H$ quatre espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$. Soit $f$ (respectivement $g$) une fonction définie de $U$ dans $F$ (respectivement $G$) et différentiable sur $U$.
\par Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par La fonction $B(f, g):x\mapsto B(f(x), g(x))$ est différentiable sur $U$ et :
\par $$d(B(f, g)) = B(df, g)+B(f, dg)$$}
\Thr{Différentielle d'une composée}{Soit $E,F, G$ trois espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$, différentiable sur $U$. Soit $g$ une fonction définie de $V$ dans $G$, avec $V$ un ouvert de $F$ contenant $f(U)$, différentiable sur $V$.
\par La fonction $g\circ f$ est différentiable sur $U$ et, pour tout $a\in U$,
\par $$d(g\circ f)(a) = dg(f(a))\circ df(a)$$}
\Thr{Règle de la châine}{Soit $m,p,n$ des entiers naturels non nuls.
\par Soit $f$ une application différentiable sur $U$ un ouvert de $\R^m$ et à valeurs dans $\R^p$.
\par Soit $g$ une application différentiable sur $V$ un ouvert de $\R^p$ contenant $f(U)$ et à valeurs dans $\R^n$.
\par On note $f_1,..., f_p$ les fonctions composantes de $f$ et on pose $h = g\circ f$.
\par On a donc le schéma suivant :
\par $$\begin{array}{ccccc} \R^m & \to^f & \R^p & \to^g & \R^n \\ (x_1,...,x_m) & \mapsto & (f_1,..., f_p) & \mapsto & (g_1,..., g_n)\end{array}$$
\par Soit $x=(x_1,..., x_m)\in U$. Pour $j\in \llbracket 1, m\rrbracket$ et $i\in\llbracket 1,n\rrbracket$ :
\par $$\dfrac{\partial (g\circ f)_i}{\partial x_j}(x) = \sum\limits_{k=1}^p\dfrac{\partial g_i}{\partial f_k}(f(x))\cdot\dfrac{\partial f_k}{\partial x_j}(x)$$
\par $$\partial_jh(x)=\sum\limits_{k=1}^p\partial_kg(f(x))\partial_jf_k(x)$$
\par Ou encore, avec une notation plus abusive :
\par $$\dfrac{\partial g_i}{\partial x_j}=\sum\limits_{i=1}^p\dfrac{\partial f_k}{\partial x_j}\dfrac{\partial g_i}{\partial f_k}$$}
\Def{}{Une application $f$ d'un ouvert $U$ dans $F$ est dite de de classe $\mathcal{C}^1$ si elle est différentiable sur $U$ et si $df$ est continue sur $U$
\par (i.e. $\begin{array}{rcl} U & \to & \mathcal{L}(E,F) \\ x & \mapsto & df(x)\end{array}$ continue)}
\Thr{de Schwarz}{Soit $\mathcal{B}$ une base de $E$ et soit $f$ définie d'un ouvert $U$ dans $F$.
\par Si $f$ est de classe $\mathcal{C}^2$ alors :
\par $$\forall i,j\in\llbracket 1, n\rrbracket^2, \partial_i\partial_jf = \partial_j\partial_if$$}
\Thr{}{Soit $\mathcal{B}$ une base de $E$. Soit $f$ une application définie d'un ouvert $U$ dans $F$.
\par L'application $f$ est de classe $\mathcal{C}^1$ sur $U$ si, et seulement si, les dérivées partielles relativement à la base $\mathcal{B}$ existent en tout point de $U$ et sont continues sur $U$.}
\Thr{Classe d'une combinaison linéaire}{Soit $f$ et $g$ deux fonctions de $U$ dans $F$ de classe $\mathcal{C}^k$. Soit $\lambda, \mu$ deux réels.
\par La fonction $\lambda f+\mu g$ est de classe $\mathcal{C}^k$ sur $U$.}
\Thr{Classe d'une application bilinéaire}{Soit $E,F,G,H$ quatre espaces vectoriels normés de dimension finie et $U$ un ouver tde $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^k$.
\par Soit $g$ une fonction définie de $U$ dans $G$ de classe $\mathcal{C}^k$.
\par Soit $B$ une application bilinéaire définie de $F\times G$ dans $H$.
\par La fonction $B(f, g)$ est de classe $\mathcal{C}^k$ sur $U$.}
\Thr{Classe d'un produit scalaire}{Supposons que $F$ est un espace vectoriel euclidien. Soit $f$ et $g$ deux applications de classe $\mathcal{C}^1$ de $U$ dans $F$.
\par La fonction $\left\{\begin{array}{rcl} U & \to & \R \\ t & \mapsto & \langle f(t), g(t)\rangle\end{array}\right.$ est $\mathcal{C}^1$}
\Thr{Classe d'un produit}{Soit $f$ et $g$ deux applications de classe $\mathcal{C}^1$ d'un ouvert $U$ dans $\R$. La fonction $fg$ est $\mathcal{C}^1$.}
\Thr{Classe d'une composée}{Soit $E, F,G$ trois espaces vectoriels normés de dimension finie et $U$ un ouvert de $E$.
\par Soit $f$ une fonction définie de $U$ dans $F$ de classe $\mathcal{C}^k$ sur $U$.
\par Soit $g$ une fonction définie de $V$ un ouvert de $F$ contenant $f(U)$ dans $G$ et de classe $\mathcal{C}^k$ sur $V$.
\par La fonction $g\circ f$ est de classe $\mathcal{C}^k$ sur $U$.}
\Thr{Dérivée le long d'un arc}{Soit $f$ une fonction définie d'un ouvert $U$ dans $F$. Soit $\gamma$ une application définie sur un intervalle d'intérieur non-vide $I$ de $\R$ et à valeurs dans $U$.
\par Si $\gamma$ est dérivable en $t$ et si $f$ est différentiable en $\gamma(t)$, alors $f\circ\gamma$ est dérivable en $t$ et :
\par $$(f\circ\gamma)'(t) = df(\gamma(t))\cdot\gamma'(t)$$}
\Thr{Intégrale le long d'un arc}{Si $f$ est une application de classe $\mathcal{C}^1$ d'un ouvert $U$ dans $F$, si $\gamma$ est une application de classe $\mathcal{C}^1$ de $[0,1]$ dans $\Omega$, si $\gamma(0)=a$, $\gamma(1)=b$, alors :
\par $$f(b)-f(a) = \int_0^1df(\gamma(t))\cdot\gamma'(t)dt$$}


\subsection{Calcul différentiel - optimisation}
\Def{}{Soit $f$ une fonction définie et différentiable sur un ouvert $U$ et à valeurs dans $\R$. Soit $a\in U$.
\par On appelle gradient de $f$ en $a$ et on note $\nabla f(a)$ l'unique vecteur de $E$ tel que :
\par $$\forall h\in E, df(a)\cdot h=\langle \nabla f(a), h\rangle$$}
\Thr{Interprétation géométrique du gradient}{Pour $h$ un vecteur de $E$, $D_hf(a)= df(a)\cdot h = \langle \nabla f(a), h\rangle$.
\par Si $\nabla f(a)\neq 0$, il est colinéaire et de même sens que le vecteur unitaire selon lequel la dérivée de $f$ en $a$ est maximale.}
\Thr{Formule du gradient}{Soit $f$ une fonction définie et différentiable sur un ouvert $U$ et à valeurs dans $\R$. Soit $a\in U$.
\par Dans $(e_1,..., e_n)$ une base orthonormale de $E$, $\nabla f(a)$ s'écrit $\nabla f(a) = \sum\limits_{i=1}^n\partial_if(a)\cdot e_i$}
\Thr{Constance sur un connexe}{Si $f$ est une application d'un ouvert $U$ dans $F$.
\par Si $U$ est connexe par arcs, la fonction $f$ est constante sur $U$ si, et seulement si, elle est différentiable sur $U$ et si $df=0$}
\Def{}{Si $X$ est une partie de $E$ et $x$ un point de $X$, un vecteur $v$ de $E$ est tangent à $X$ en $x$ s'il existe $\varepsilon>0$ et un arc $\gamma$ défini sur $]-\varepsilon, \varepsilon[$, dérivable en $0$, à valeurs dans $X$, tels que $\gamma(0)=x$ et $\gamma'(0)=v$
\par On note $T_xX$ l'ensemble des vecteurs tangents à $X$ en $x$. C'est un espace vectoriel.}
\Thr{}{Soit $E$ un espace vectoriel euclidien. Soit $f$ une fonction définie d'un ouvert $U$ dans $\R$, de classe $\mathcal{C}^1$. Soit $X$ une ligne de niveau de $f$. Soit $x_0\in X$ de $X$.
\par Si $df(x_0)\neq 0$, alors :
\par $$T_{x_0}X =\ker(df(x_0))=(\nabla f(x_0))^\perp$$}
\Thr{}{Soit $f$ une fonction définie sur un ouvert $U$ et à valeurs dans $\R$ et $a\in U$.
\par Si $f$ admet un extremum local en $a$ et si $f$ est différentiable en $a$, alors $df(a)=0$.
\par (i.e. $a$ est un point critique de $f$)}
\Thr{optimisation sous une contrainte}{Si $f$ et $g$ sont des fonctions numériques définies et de classe $\mathcal{1}^1$ sur l'ouvert $\Omega$ de $E$, si $X$ est l'ensemble des zéros de $g$, si $x\in X$ et $dg(x)\neq 0$ et si la restriction de $f$ à $X$ admet un extremum local en $x$, alors $df(x)$ est colinéaire à $dg(x)$.}
\Pre{Ici, on a que $T_xX=\ker dg(x)$ (théorème admis)
\par $df(x)$ et $dg(x)$ sont des formes linéaires telles que $\ker dg(x)\subset \ker df(x)$
\par Et donc $df(x) = \lambda dg(x)$
\par $X = g^{-1}(\{0\})$, $dg(x)\neq 0$ et $f_{|X}$ admet une différentielle en $x$
\par Ici, $T_xX = \ker dg(x)$ et de plus, $\ker dg(x)\subset\ker df(x)$ par le théorème précédent, comme $f_{|X}$ admet un extrémum en $x$
\par Si $df(x)=0$ : $df(x) = 0dg(x)$
\par Sinon : $\ker df(x)$ est un hyperplan (parce que $df(x)$ est une forme linéaire), donc $\ker df(x) = \ker dg(x)$ puisque $\ker dg(x)$ est de dimension 1 en tant que gradient d'un vecteur}
On a quelques résultats très utiles sur une fonction $\mathcal{C}^2$.
\Def{}{Soit $f$ une fonction de classe $\mathcal{C}^2$ sur un ouvert $U$ de $\R^n$ euclidien, à valeurs réelles. Soit $x\in U$.
\par La matrice hessienne de $f$ en $x$ est la matrice symétrique :
\par $$H_f(x) = (\partial^2 f_{i,j}(x))_{(i,j)\in\llbracket1,n\rrbracket^2}$$}
\Thr{Formule de Taylor-Young à l'ordre 2}{Soit $f$ une fonction de classe $\mathcal{C}^2$ sur un ouvert $U$ de $\R^n$ euclidien, à valeurs réelles. Soit $x\in U$.
\par $$f(x+h)=_{h\to 0} f(x) + \langle \nabla f(x), h\rangle + \frac{1}{2}\langle H_f(x)\cdot h, h\rangle + o(\Vert h\Vert^2)$$
\par $$f(x+h)=_{h\to 0} f(x) + \nabla f(x)^\top h + \frac{1}{2}h^\top H_f(x)\cdot h + o(\Vert h\Vert^2)$$}
\Thr{Interprétation de la Hessienne}{Si $f:U\subset E\to f$ est $\mathcal{C}^2$ avec $x_0$ un point critique de $f$.\begin{itemize}
\item Si $H_f(x_0)\in\mathcal{S}_p^{++}(\R)$, $f$ a un minimum local.
\item Si $-H_f(x_0)\in\mathcal{S}_p^{++}(\R)$, $f$ a un maximum local.
\item Si $H_f(x_0)$ possède une valeur propre strictement négative ($H_f\notin\mathcal{S}_p^+(\R)$), alors $f$ n'a pas de minimum en $x_0$.
\item Si $H_f(x_0)$ possède une valeur propre strictement positive ($-H_f\notin\mathcal{S}_p^+(\R)$), alors $f$ n'a pas de maximum en $x_0$.
\end{itemize}}


\subsection{Hors programme - Différentielle d'un inverse}
\Thr{Dans R}{Si $f$ dérivable bijective d'un intervalle $I$ sur un intervalle $J$, si $f'$ ne s'annule pas alors $f^{-1}$ est dérivable et :
\par $$(f^{-1})' = \dfrac{1}{f'\circ f^{-1}}$$}
\Thr{Dans le cas général}{Si $f$ est différentiable bijective d'un ouvert $U$ sur un ouvert $V$, et si $f^{-1}$ est différentiable
\par Alors $\dim E=\dim F$ et :
\par $$\begin{cases}\forall x\in U, df^{-1}(f(x)) = df(x)^{-1} \\ \forall x\in V, df^{-1}(x) = df(f^{-1}(x))\end{cases}$$}
\Thr{}{Si $f$ est différentiable bijective d'un ouvert $U$ sur un ouvert $V$
\par Soit $x\in U$ tel que $df(x)$ est bijective.
\par Alors $f^{-1}$ est différentiable en $f(x)$ et :
\par $$df^{-1}(f(x)) = (df(x))^{-1}$$}


\newpage
\section{Algèbre}
\subsection{Espaces vectoriels normés}
Ce chapitre a beaucoup de définitions qui lui sont propres.
\Def{Norme}{Soit $E$ un $\K$-ev. Soit $\varphi$ une application de $E$ dans $\R_+$. On dit que $\varphi$ est une norme si elle vérifie:\begin{enumerate}
\item $\forall u\in E, \varphi(u) = 0\Rightarrow u=0$ (on dit que l'application est définie)
\item homogénéité : $\forall\lambda\in\K, \forall u\in E,\varphi(\lambda u) =\vert \lambda\vert \varphi(u)$
\item inégalité triangulaire : $\forall u, v\in E, \varphi(u+v) \leq \varphi(u)+\varphi(v)$
\end{enumerate}}
\Def{Distance}{Soit $E$ un EVN. On appelle distance associée à la norme sur $E$ l'application :
\par $$d:\left\{\begin{array}{rcl} E & \to & \R_+ \\ (x,y) & \mapsto & \Vert x-y\Vert \end{array}\right.$$
\par Une distance définit un espace métrique.}
\Def{Normes équivalentes}{Soit $E$ un espace vectoriel, et $\Vert.\Vert_1, \Vert.\Vert_2$ deux normes sur $E$. On dit que $\Vert.\Vert_1$ et $\Vert.\Vert_2$ sont équivalentes si : $\exists \alpha,\beta\in\R_+^*,\forall x\in E, \alpha\Vert x\Vert_1\leq \Vert x\Vert_2\leq \beta\Vert x\Vert_1$, ou si $x\neq0$ : $\alpha\leq\dfrac{\Vert x\Vert_2}{\Vert x\Vert_2}\leq\beta$}
\begin{Rem}
Toutes les notions topologiques qui suivent sont invariantes par changement de normes équivalentes. En dimension finie, on a même que ce sont des notions qui ne dépendent pas d'une norme.
\end{Rem}
\Def{Point intérieur}{Soit $A\subset E$ et $a\in A$, on dit que $a$ est intérieur à $A$ si $\exists\alpha\in\R_+^*,\mathcal{B}(a,\alpha)\subset A$}
\Def{Intérieur}{Soit $A\subset E$, on appelle intérieur de $A$ l'ensemble des points intérieurs de $A$, noté $\overset{\circ}{A}$ ; on a donc $\overset{\circ}{A}\subset A$}
\Def{Ouvert}{Soit $A\subset E$, on dit que $A$ est ouvert si tous les points de $A$ sont intérieurs (ou si $A$ contient un voisinage de chacun de ses points), ce qui signifie $A\subset\overset{\circ}{A}$}
\Def{Point adhérent}{Soit $A\subset E$ et $x\in E$, on dit que $x$ est adhérent à $A$ si : $\forall\alpha\in\R_+^*, \mathcal{B}(x,\alpha)\cap A\neq\emptyset$}
\Def{Adhérence}{Soit $A\subset E$, on appelle adhérence de $A$ l'ensemble des points adhérents à $A$, noté $\bar{A}$. On notera que $A\subset\bar{A}$}
\Def{Fermé}{Si $A\subset E$, $A$ est fermé si $\bar{A}=A$ (donc que $\bar{A}\subset A$), donc que $A$ contient tous les points qui lui sont adhérents.}
\Def{Compact}{Soit $E$ un EVN, soit $A\subset E$. On dit que $A$ est compact si de toute suite de $A$, on peut extraire une suite convergente dans $A$.}



\Thr{Deuxième forme de l'inégalité triangulaire}{Soit $E$ un EVN, $\forall x,y\in E, \vert\Vert x\Vert-\Vert y\Vert\vert\leq\Vert x\pm y\Vert \leq \Vert x\Vert + \Vert y\Vert$}
\Thr{L'équivalence des normes}{Toutes les normes sont équivalentes en dimension finie}

\Thr{Réunion et intersection d'ouverts}{Avec $E$ un EVN, $(O_i)_{i\in I}$ une famille d'ouverts de $E$, on a que :\begin{itemize}
\item $\cup_{i\in I}O_i$ est un ouvert
\item $\cap_{i\in I}O_i$ est un ouvert à condition que $I$ soit fini\end{itemize}}
\Thr{Caractérisation séquentielle de l'adhérence}{Soient $A\subset E,x\in E$, alors $x$ est adhérent à $A$ si, et seulement si, $\exists (a_n)\in A^\N, (a_n)\to x$}
\Thr{Caractérisation séquentielle des fermés}{Soit $A\subset E$, $A$ est fermé si, et seulement si, pour toute suite d'éléments de $A$ convergente vers $l$, $l\in A$. ie $\forall (a_n)\in A^\N, (a_n)\to l\Rightarrow l\in A$}
\Thr{Complémentarité d'un ouvert}{Soit $E$ un EVN et $A\subset E$, alors $A$ est fermé si, et seulement si, $\mathcal{C}_EA$ est ouvert.}
\Thr{Compacts}{Dans un espace de dimension finie $E$, les compacts sont les fermés bornés.}


\subsection{Limites dans un EVN}
\Def{Limite}{Soient $E,F$ deux EVN, et $A\subset E$. Soit $f\in\mathcal{F}(A,F)$, $x_0\in\bar{A}$. Soit $l\in F$. On dit que $f$ converge vers $l$ en $x_0$ si :
\par $$\forall\varepsilon\in\R_+^*,\exists\alpha\in\R_+^*,\forall x\in\mathcal{B}(x_0,\alpha)\cap A, f(x)\in\mathcal{B}(l,\varepsilon)$$
\par On notera $f\to_{x_0} l$ ou $f(x)\to_{x\to x_0} l$ (notation plus abusive) ou $\lim f = l$ (notation plus adaptée à une conclusion) et $\lim\limits_{x\to x_0}f(x)=l$.}
\Def{Continuité}{Soit $f\in\mathcal{F}(A,F)$ avec $A\subset E$ et $E,F$ deux EVN. Soit $a\in A$, on dit que $f$ est continue en $a$ si $f\to_a f(a)$.}
\Def{Uniforme continuité}{Soit $f\in\mathcal{F}(A,F)$ où $A\subset E$ avec $E,F$ deux EVN. On dit que $f$ est uniformément continue si :
\par $$\forall\varepsilon\in\R_+^*,\exists\eta\in\R_+^*,\forall x,y\in A,\Vert x-y\Vert<\eta\Rightarrow\Vert f(x)-f(y)<\varepsilon\Vert$$}
\Def{Fonctions lipschitziennes}{Soit $f\in\mathcal{F}(A,F)$, où $A\subset E$ avec $E,F$ EVN. Soit $k\in\R_+^*$. On dit que $f$ est $k-lipschitzienne$ si :
\par $$\forall x,y\in A,\Vert f(x)-f(y)\Vert\leq k\Vert x-y\Vert$$}
\Def{Norme triple}{Soit $E,F$ deux EVN, on appelle $\mathcal{L}_C(E,F$) l'ensemble des applications linéaires continues de $E$ dans $F$.
\par Alors $\mathcal{L}_C(E,F)$ est un espace vectoriel normé pour la norme 
\par $$\varphi\mapsto \vert\Vert\varphi\Vert\vert=\sup\limits_{x\in E, x\neq 0} \frac{\Vert \varphi(x)\Vert_F}{\Vert x\Vert_E} = \sup\limits_{x\in E, \Vert x\Vert=1}\Vert \varphi(x)\Vert$$}
\Thr{Union d'applications d'extraction}{Si $\varphi, \psi$ sont deux applications de $\N$ dans $\N$ strictement croissantes vérifiant $\varphi(\N)\cup\varphi(\N)=\N$, si $(u_{\varphi(n)})$ et $(u_{\psi(n)})$ convergent vers $l$, alors $u_n$ est convergente de limite $l$.}
\Thr{Bolzano-Weierstrass}{De toute suite bornée dans un $\K$-ev de dimension finie on peut extraite une suite convergente.}

\Def{Convergence}{Soit $E$ un EVN sur $\K$, soit $(u_n)\in E^\N$. Soit $l\in E$. On dit que $(u_n)$ converge vers $l$ si $(\Vert u_n-l\Vert)\to 0$
\par On peut aussi écrire :
\par $$\forall\varepsilon\in\R_+^*, \exists n_0\in\N,\forall n\in\N, n\geq n_0\Rightarrow u_n\in\mathcal{B}(l, \varepsilon)$$
\par On parle de suites convergentes et de limites (notées $\lim (u_n)=l$ et $(u_n)\to l$) dans un EVN}
\Thr{Convergence des suites extraites}{Si une suite $(u_n)\in E^\N$ converge $l$ alors toute suite extraite de $(u_n)$ converge vers $l$.}

\Thr{Caractérisation séquentielle de la limite}{Avec $f\in\mathcal{F}(A,F)$ avec $A\subset E$ et $E,F$ deux EVN. Avec $x_0\in\bar{A}$ et $l\in F$, alors :
\par$$f\rightarrow_{x_0}l\Leftrightarrow \forall (u_n)\in A^\N, (u_n)\to x_0, (f(u_n))\to l$$}
\Thr{Images réciproques}{Soit $f\in\mathcal{F}(A,F)$ continue, alors l'image réciproque d'un ouvert de $F$ par $f$ est un ouvert relatif de $A$
\par L'image réciproque d'un fermé de $F$ par $f$ est un fermé relatif de $A$.}
\Thr{Théorème de Heine}{Toute fonction continue sur un compact est uniformément continue.}
\Thr{Bornes atteintes}{L'image d'un compact par une application continue est un compact.}
\Thr{Valeurs intermédiaires}{L'image d'un connexe par arcs par une application continue est connexe par arcs.}

\Thr{Critère de continuité des applications linéaires}{Soit $E, F$ des $\K$-EVN et $f\in\mathcal{L}(E,F)$. $f$ est continue si, et seulement si, elle vérifie l'une des propriétés équivalentes suivantes :\begin{enumerate}
\item $f$ est continue en $0$ ;
\item $\exists k\in\R_+^*,\forall x\in E, \Vert f(x)\Vert_F\leq k\Vert x\Vert_E$ ;
\item $f$ est lipschitzienne.\end{enumerate}}
\begin{Rem}
En dimension finie, toute application linéaire est continue, par continuité des projecteurs (les applications linéaires sont des polynômes de degré au plus 1 sur les coordonnées).
\par Si $\varphi$ est linéaire et injective en dimension finie, on a que $\Vert\varphi\Vert$ est une norme, qu'on appelle la norme $\varphi$.
\end{Rem}
\Thr{Sous-multiplicativité}{Soit $E,F,G$ des EVN, soit $f\in\mathcal{L}_C(E,F)$ et $g\in\mathcal{L}_C(F,G)$, alors $\vert\Vert g\circ f\Vert\vert\leq \vert\Vert g\Vert\vert\vert \Vert f\Vert\vert$}


\subsection{Réduction}
\Def{Valeur propre}{Soit $E$ un $\K$-ev et $f\in\mathcal{L}(E)$. Soit $\lambda \in\K$. \par On dit que $\lambda$ est une valeur propre de $f$ si : $f-\lambda id$ n'est pas injective. \par Ce qui correspond à $\ker (f-\lambda id)\neq\{0\}$ \par Ce qui correspond à $\exists x\in E, x\neq 0, (f-\lambda id)(x)=0$ \par Ce qui correspond à $\exists x\in E,x\neq 0, f(x)-\lambda x=0$}
\Def{Spectre d'un endomorphisme}{Soit $E$ un $\K$-ev et $f\in \mathcal{L}(E)$. On appelle spectre de $f$ l'ensemble des valeurs propres de $f$, noté $S_p(f)$. \par On peut étendre la notion aux matrices.}
\Def{Espace propre}{Soit $E$ un $\K$-ev, $f\in\mathcal{L}(E)$ et $\lambda\in S_p(f)$. On appelle espace propre associé à $\lambda$ l'ensemble $\ker (f-\lambda id)$, qu'on note $E_\lambda(f)$ \par C'est un sev de $E$ non-réduit à $0$. \par On étend la notion aux matrices de $\mathcal{M}_n(\K)$. Pour $A\in\mathcal{M}_n(\K)$ et $\lambda\in S_p(A)$, alors $E_\lambda(A) = \ker (A-\lambda I_n)$ est un sev de $\mathcal{M}_{n,1}(\K)$.}
\Def{Polynôme annulateur}{Soit $P\in\K[X]$ et $f\in\mathcal{L}(E)$. On dit que $P$ est un polynôme annulateur de $f$ si $P(f)=0$}
\Def{Polynôme minimal}{Soit $f\in\mathcal{L}(E)$. On appelle polynôme minimal de $f$ un polynôme non-nul unitaire annulateur de $f$ de degré minimal, qu'on note $\pi_f$ ou $\mu_f$ (cette notation existe mais elle est très rare).}
\Def{Polynôme caractéristique}{Avec $f\in\mathcal{L}(E)$, $\begin{array}{rcl} \K & \to & \K \\ \lambda & \mapsto & \det (f-\lambda id)\end{array}$ est polynomiale. Le polynôme associé est de degré $n$ unitaire.
\par On appelle ce polynôme le polynôme caractéristique de $f$, noté $\chi_f$}

\Thr{}{Une somme finie de sous-espaces propres d'un endomorphisme associés à des valeurs propres distinctes est directe.\par Ce qui correspond à : si $\lambda_1,...,\lambda_p$ sont des valeurs propres distinctes de $f\in\mathcal{L}(E)$, alors $E_1+...+E_p$ est une somme directe.}
\Thr{}{Soit $E$ un $\K$-ev de dimension finie. Soit $f\in\mathcal{L}(E)$. Soit $\lambda$ une valeur propre de $f$ d'ordre $\alpha$
\par Alors $\dim E_\lambda(f)\leq\alpha$}
\Thr{}{Un endomorphisme d'un $\K$-ev $E$ de dimension finie est trigonalisable si, et seulement si, son polynôme caractéristique est scindé.}
\Thr{}{Soit $E$ un $\K$-ev de dimension finie $n$, $f\in\mathcal{L}(E)$. Alors $f$ est nilpotente si, et seulement si, $\chi_f = X^n$ et $\chi_f=X^n$ si, et seulement si, il existe une base $B$ de $E$ dans laquelle $Mat_B(f)$ est triangulaire avec des 0 sur la diagonale.}
\Thr{Opérations sur les polynômes d'endomorphismes}{Pour $P,Q\in\K[X]$, $\lambda\in\K$ et $f\in\mathcal{L}(E)$, on a :\begin{itemize}
\item $(P+Q)(f) = P(f) + Q(f)$
\item $(\lambda\cdot P)(f) = \lambda P(f)$
\item $(P\times Q)(f) = P(f)\circ Q(f)=Q(f)\circ P(f)$
\item $(P\circ Q)(f) = P(Q(f))$
\end{itemize}}
\Thr{Cayley-Hamilton}{Avec $E$ un $\K$-ev de dimension finie, $f\in\mathcal{L}(E)$, alors $\chi_f$ est un polynôme annulateur de $f$.}
\Thr{}{Avec $E$ un $\K$-ev de dimension finie, $f\in\mathcal{L}(E)$. $f$ est diagonalisable si, et seulement si, $\pi_f$ est scindé à racines simples
\par Ce qui correspond à ce qu'il existe un polynôme annulateur non-nul scindé à racines simples de $f$.}


\subsection{Espaces préhilbertiens}
\Def{}{Soit $E$ un $\R$-ev, on dit qu'une application $\varphi:E\times E\to\R$ est :\begin{itemize}
\item une forme bilinéaire si : $\forall x\in E, y\mapsto \varphi(x,y)=\varphi(x, .)$ est linéaire et $\varphi(., x)$ est linéaire.
\item symétrique si : $$\forall x,y\in E, \varphi(x,y)=\varphi(y,x)$$
\item positive si : $$\forall x\in E, \varphi(x, x)\geq 0$$
\item définie si : $$\forall x\in E, \varphi(x, x)=0\Rightarrow x=0$$
\end{itemize}
Une forme bilinéaire symétrique définie positive est un produit scalaire.}
\Thr{Identités polaires}{$\varphi$ une forme bilinéaire symétrique.
\par \begin{align*}\forall x, y\in E, \varphi(x, y) &=\frac{1}{2}(\varphi(x+y, x+y) - \varphi(x, x) - \varphi(y,y))\\
& = \frac{1}{4}(\varphi(x+y, x+y) - \varphi(x-y, x-y))\end{align*}
\par $\varphi$ est donc entièrement caractérisée par l'application $u\mapsto \varphi(u,u)$}
\Thr{Cauchy-Schwarz}{$E$ un $\R$-v et $\varphi$ une forme bilinéaire symétrique positive
\par Alors $\forall x,y\in E, \vert\varphi(x,y)\vert\leq \varphi(x, x)\varphi(y,y)$
\par Dans un espace préhilbertien réel $E$, 
\par $$\forall x,y\in E, \vert\langle x,y\rangle \vert \leq \Vert x\Vert\Vert y\Vert$$
\par avec le cas d'égalité si, et seulement si, $x$ et $y$ sont colinéaires.}
\Thr{Inégalité triangulaire}{Soient $x, y\in E$, alors :
\par $$\Vert x+y\Vert^2=\Vert x\Vert^2+\Vert y\Vert^2+2\langle x,y\rangle$$
\par $$(\Vert x\Vert+\Vert y\Vert)^2=\Vert x\Vert^2+\Vert y\Vert^2 + 2\Vert x\Vert\Vert y\Vert$$
\par Donc par Cauchy-Schwarz, $\vert\langle x, y\rangle\vert\leq \Vert x\Vert\Vert y\Vert$ et on en déduit l'inégalité triangulaire :
\par $$\Vert x+y\Vert\leq \Vert x\Vert+\Vert y\Vert$$
\par Avec cas d'égalité si $x$ et $y$ sont colinéaires et de même sens.}
\Def{Orthogonalité}{\begin{itemize}
\item pour $u,v\in E$, $u$ et $v$ sont orthogonaux, noté $u\perp v$, si $\langle u, v\rangle = 0$
\item deux sev $F$ et $G$ de $E$ sont orthogonaux si $\forall x\in F, \forall y\in G, \langle x, y\rangle = 0$
\item pour $A\subset E$, l'orthogonal de $A$ est l'ensemble $A^\perp = \{x\in E\vert \forall a\in a, \langle a, x\rangle=0\}$
\end{itemize}}
\Thr{Propriétés des orthogonaux}{\begin{itemize}
\item Pour $A, B$ inclus dans $E$, $A\subset B\Rightarrow B^\perp \subset A^\perp$ 
\item $$\forall A\subset E, A^\perp = vect(A)^\perp$$
\item $A^\perp$ est un sev de $E$
\item $F, G$ sev de $E$ :
\par $$ F^\perp\cap G^\perp = (F+G)^\perp$$
\par $$F^\perp + G^\perp \subset (F\cap G)^\perp$$ (réciproque fausse)
\item $$F\subset (F^\perp)^\perp$$ 
\end{itemize}}
\Def{Familles orthogonales}{Soit $(x_i)_{i\in I}$ une famille de vecteurs d'un espace préhilbertien $E$
\par $(x_i)$ est une famille orthogonale si :
\par $$\forall i,\in I, i\neq j \Rightarrow \langle x_i, x_j\rangle$$
\par $(x_i)$ est une famille orthonormale si :
\par $(x_i)_{i\in I}$ est orthogonale et $\forall i\in I, \Vert x_i\Vert = 1$
\par Toute famille orthogonale est libre (s'il n'y a pas de vecteurs nuls dedans).}
\Thr{}{Soit $E$ un espace euclidien et $F$ un sev de $E$
\par Alors $$F\oplus F^\perp = E$$
\par (ie : $F^\perp$ est un supplémentaire de $F$)}
\Thr{Extension}{Soit $E$ préhilbertien réel et $F$ un sev de $E$ de dimension finie. Alors :
\par $$F\oplus F^\perp = E$$}
\Thr{Méthode de Schmidt}{Cette méthode permet de "redresser une boîte à chaussures écrasée", c'est à dire construire une base orthogonale.
\par (on se sert du fait que tout espace euclidien possède une BON, une base orthogonale où tous les vecteurs ont même norme)
\par Soit $E$ un espace préhilbertien, $(u_i)_{i\in I}$ famille libre de $E$ avec $I\subset \N$
\par Alors on peut construire par récurrence une famille morthogonale $(v_n)_{n\in I}$ qui vérifie :
\par $$\forall n\in I, Vect(u_0,..., u_n) =Vect(v_0,..., v_n)$$
\par $(v_n)$ est définie par la relation :
\par $\begin{cases} v_0 = u_0 \\ v_{n+1} = u_{n+1} - \sum\limits_{i=1}^n\frac{\langle u_{n+1}, v_i\rangle}{\Vert v_i\Vert^2}v_i \end{cases}$
\par Pour rendre cette famille orthonormale, il suffit de prendre la famille et de diviser chaque vecteur par sa norme}
\Def{}{Dans tous les cas où $F\oplus F^\perp = E$, on peut définir :\begin{itemize}
\item la projection orthogonale sur $F$ (la projection sur $F$ parallèlement à $F^\perp$)
\item la symétrie orthogonale par rapport à $F$
\end{itemize}}
\Thr{de la meilleure approximation}{Soit $E$ un espace préhilbertien et $F$ un sev de dimension finie.
\par Alors pour tout $x$ de $E$,
\par \begin{center} $d(x,F)$ est atteinte en un unique vecteur de $F$, le projeté orthogonal de $x$ sur $F$\end{center}}
\Thr{Calcul pratique du projeté}{$F$ un sev de $E$ de dimension finie $p$, $(u_1, ..., u_p)$ une base de $F$
\par $x\in E$, $p(x)$ son projeté orthogonal sur $F$
\par Donc $(px)$ est entièrement défini par les équations :
\par $$(1)\left\{\begin{array}{l} p(x)\in F \\ x-p(x)\in F^\perp\end{array}\right.$$}
\Thr{de représentation de Reese (cas euclidien)}{Soit $E$ un espace euclidien, l'application :
\par $\psi\left\{\begin{array}{rcl} E & \to & E^* \\a & \mapsto \varphi_a:\left\{\begin{array}{rcl} E & \to & \K\\ x & \mapsto & \langle a, x\rangle\end{array}\right.\end{array}\right.$ est un isomorphisme}
\Def{Adjoint}{Soit $E$ un espace euclidien et $u\in\mathcal{L}(E)$.
\par On appelle adjoint de $u$ l'application de $E$ dans $E$ $u^\star$ telle que :
\par $$\forall x,y\in E\times E, \langle u(x), y \rangle = \langle x, u^\star(y)\rangle$$}
\Thr{Propriétés de l'ajdoint}{Soit $E$ euclidien, $u,v\in\mathcal{L}(E)$.\begin{itemize}
\item $$u^\star\in\mathcal{L}(E)$$
\item $$(u^\star)^\star = u$$
\item $$(u\circ v)^\star =v^\star \circ u^\star$$
\item $u\to u^\star$ est linéaire, ie $(u+v)^\star = u^\star + v^\star$ et $\forall \lambda\in\R, (\lambda u)^\star = \lambda u^\star$
\end{itemize}}
\Thr{}{Soit $E$ un espace euclidien. Soit $B$ une BON de $E$. Soit $u\in\mathcal{L}(E)$
\par On note $A = Mat_B(u)$
\par Alors $Mat_B(u^\star)= A^T$}
\Thr{}{Soit $E$ un espace euclidien, $u\in\mathcal{L}(E)$, $F$ un sev de $E$.
\par Si $F$ est stable par $u$ alors $F^\perp$ est stable par $u^\star$}
\Def{}{Soit $E$ un espace euclidien et $f\in\mathcal{L}(E)$. On dit que $f$ est une isométrie si
\par $$\forall x\in E, \Vert f(x)\Vert = \Vert x\Vert$$}
\Thr{Caractérisation}{Soit $f\in\mathcal{L}(E)$ avec $E$ euclidien muni d'une base $\mathcal{B}$ orthonormée, $f$ est une isométrie si, et seulement si, elle vérifie l'une des propriétés suivantes :\begin{enumerate}
\item $$\forall x\in E, \Vert f(x)\Vert=\Vert x\Vert$$
\item $$\forall x,y\in E, \langle f(x), f(y)\rangle = \langle x, y\rangle$$
\item $f(\mathcal{B})$ est une base orthonormée.
\item $f\in\mathcal{GL}(E)$ et $f^\star=f^{-1}$
\end{enumerate}}
\Thr{}{Soit $E$ un espace euclidien, $u$ une isométrie. Alors :
\par $$(\det u)\in \{-1, 1\}$$
\par On dit que $u$ est une isométrie directe si $\det(u)=1$, indirecte sinon.}
\Def{}{On dit qu'une matrice $M\in\mathcal{M}_n(\K)$ est orthogonale si elle vérifie l'une des propriétés équivalentes suivantes :\begin{enumerate}
\item $$M^TM = I$$
\item $$MM^T = I$$
\item $M\in\mathcal{GL}_n(\R)$ et $M^T=M^{-1}$
\item Les vecteurs colonnes de $M$ forment une base orthonormale pour le produit scalaire canonique de $\mathcal{M}_{n,1}(\R)$
\item Les vecteurs lignes de $M$ forment une base orthonormale pour le produit scalaire canonique de $\mathcal{M}_{1,n}(\R)$
\end{enumerate}}
\Thr{}{La matrice de passage entre deux BON est une matrice orthogonale.
\par Le déterminant d'une matrice orthogonale est égal à $1$ ou $-1$.
\par On a ces équivalences pour $u\in\mathcal{L}(E)$:\begin{itemize}
\item $u$ est une isométrie
\item la matrice associée à $u$ dans une BON est orthogonale
\item il existe une BON dans laquelle la matrice associée à $u$ est orthogonale
\end{itemize}}
\Thr{}{Soit $E$ un espace euclidien et $u\in\mathcal{O}(E)$ alors il existe une base orthonormale $\mathcal{B}$ telle que $Mat_B(u)$ est diagonale par bloc et chaque bloc est de la forme :
\par $$\begin{matrix} [1] \\ [-1] \\ [R_\theta]\end{matrix}$$
\par pour $\theta\in\R$
\par ie chaque bloc est soit $I$, soit $-I$, soit une rotation.}
\Thr{}{Soit $E$ un espace euclidien de dimension $n$, soit $\mathcal{B}$ une \underline{base orthonormale} de $E$, soit $u\in\mathcal{L}(E)$
\par $u$ est auto-adjoint si $Mat_B(u)\in\mathcal{S}_n(\R)$}
\Thr{Théorème spectral}{Tout endomorphisme auto-adjoint d'un espace euclidien est diagonalisable dans une BON.}
\Thr{Théorème spectral matriciel}{Toute matrice symétrique réelle est orthogonalement semblable à une matrice diagonale.}
\Thr{}{Soit $E$ un espace euclidien et $\varphi$ une fbs sur $E$. Il existe un unique endomorphisme auto-adjoint $u$ tel que :
\par $$\forall x,y\in E, \varphi(x,y) = \langle x, u(y)\rangle$$}
\Def{}{Soit $S\in\mathcal{S}_n(\R)$.
\par On dit que :\begin{itemize}
\item $S$ est positive si $(X,Y)\mapsto X^T SY$ est une forme bilinéaire symétrique positive sur $\mathcal{M}_{n,1}(\R)$. On note l'ensemble de ces matrices $\mathcal{S}_n^+(\R)$.
\item $u$ est défini positif si $(X,Y)\mapsto X^TSY$ est un produit scalaire sur $\mathcal{M}_{n,1}(\R)$. On note l'ensemble de ces endomorphismes $\mathcal{S}_n^{++}(\R)$.
\end{itemize}}
\Thr{}{Soit $E$ un espace euclidien et $u\in\mathcal{S}(E)$ alors :\begin{itemize}
\item $u$ est positif si, et seulement si, $S_p(u)\subset\R_+$
\item $u$ est défini positif si, et seulement si, $S_p(u)\subset\R_+^*$
\end{itemize}}



\subsection{Classification des matrices orthogonales du plan}
($\mathcal{O}_2(\R)$ l'ensemble des matrices orthogonales de $\mathcal{M}_2(\R)$)
\par $\mathcal{O}_2(\R)=\{R_\theta\vert\theta\in\R\}\cup\{S_\theta\vert\theta\in\R\}$
\par $R_\theta = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$
\par $S_\theta = \begin{pmatrix} \cos\theta & \sin\theta \\ \sin\theta & -\cos\theta\end{pmatrix}$
\par $\mathcal{SO}_2(\R) = \mathcal{O}_2^+(\R) = \{R_\theta\vert\theta\in\R\}$ (ensemble des rotations d'angle $\theta$, ensemble des matrices de $\mathcal{O}_2(\R)$ de déterminant 1)
\par $\mathcal{O}_2^-(\R)=\{S_\theta\vert\theta\in\R\}$ (ensemble des matrices de $\mathcal{O}_2(\R)$ de déterminant $-1$)
\Thr{}{$\forall(\theta,\varphi)\in\R^2$ :\begin{itemize}
\item $$R_\theta R_\varphi = R_{\theta+\varphi}$$
\item $$R_\theta^{-1} = R_{-\theta} =R_\theta^T$$
\item $\mathcal{SO}_2(\R)$ est un groupe commutatif, en particulier :
\par $$\forall \theta,\varphi\in\R, R_\theta R_\varphi R_\theta^{-1}=R_\varphi$$
\par Dans ce cas, $R_\theta$ est la matrice de passage d'une BOND dans une BOND.
\end{itemize}}
\Pre{Se prouvent par du calcul matriciel, et un peu de trigonométrie.}
$S_\theta$ est une symétrie orthogonale par rapport à une droite
\par $S_\theta\times \begin{pmatrix}\cos(\frac{\theta}{2})\\\sin(\frac{\theta}{2})\end{pmatrix} = \begin{pmatrix}\cos(\frac{\theta}{2})\\ \sin\frac{\theta}{2}\end{pmatrix}$
\par $S_\theta\times \begin{pmatrix}-\sin(\frac{\theta}{2})\\\cos(\frac{\theta}{2})\end{pmatrix} = -\begin{pmatrix}-\sin(\frac{\theta}{2})\\ \cos\frac{\theta}{2}\end{pmatrix}$
\par Donc $S_p(S_\theta) = \{-1,1\}$ Et $E_1(S_\theta)$ est orthogonal à $E_{-1}(S_\theta)$

\subsection{Classification des matrices orthogonales d'un espace euclidien orienté de dimension 3}
Pour $u\in\mathcal{O}(E_3)$, on note $F = \ker (u-id)$
\par Si $\dim F = 3$ : alors $u = id$, donc $u$ est nécessairement dans $\mathcal{O}_+(E_3)$
\par Si $\dim F=2$, alors $\dim F^\perp=1$, l'endomorphisme induit par $u$ sur $F^\perp$ est égal à $id_{F^\perp}$ ou $-id_{F^\perp}$
\par Si $u_{F^\perp}$ était égal à $id_{F^\perp}$, alors $u$ serait l'identité. Donc nécessairement, $u_{F^\perp} = -id_{F^\perp}$
\par Donc $u$ est la symétrie orthogonale par rapport à $F$
\par Si $\dim F=1$, alors $\dim F^\perp = 2$. Notons $v$ l'endomorphisme induit par $u$ sur $F^\perp$.
\par $v$ est donc soit une symétrie orthogonale par rapport à une droite, soit une rotation du plan. Si $v$ était une symétrie orthogonale, on aurait $\dim F = 2$, donc $v$ est une rotation du plan $F^\perp$
\par Donc si on choisit $w$ un vecteur qui oriente $F$ et qu'on choisit $u, v$ dans $F^\perp$ tels que $(u,v)$ soit une base orientée de $F^\perp$ et que $(w, u, v)$ soit une base directe de $E_3$, on a alors que $u$ est la rotation d'axe $w$ orienté par $w$ d'angle $\theta$
\par Si $\dim F=0$, alors ça veut dire que $1$ n'est pas valeur propre, donc $-1$ est valeur propre. $u$ est donc la composée d'une symétrie orthogonale et d'une rotation.
\begin{Exe}
Prenons $u\in\mathcal{L}(E_3)$ avec $\mathcal{B}$ une BOND de $E_3$ telle que $Mat_\mathcal{B}(u) = \frac{1}{3}\begin{pmatrix} 2 & 1 & 2 \\ 2 & -2 & -1 \\ 1 & 2 & -2\end{pmatrix}$
\par On peut faire les produits scalaires des colonnes les unes avec les autres, et on obtiendra que les produits scalaires sont nuls. Donc $A\in\mathcal{O}_3(\R)$, soit $u\in\mathcal{O}(E_3)$
\par Le calcul du déterminant de $A$ donne $1$, donc $u\in\mathcal{SO}(E_3)$, et $u\neq id$. Donc $u$ est une rotation d'axe d'angle $\theta$ orientée par $w$.
\par $w$ est solution de $(id-u)(w)=0$. On échelonne le système, et on obtient que $w$ est solution de $\left\{\begin{array}{ccccl} x & -y & -2z & = & 0 \\ & 3y & -3z & = & 0 \\ & -3y & +3z & = & 0\end{array}\right.$
\par Choisissons $w = 3e_1+e_2+e_3$, qu'on norme en $e_1' = \frac{1}{\Vert w\Vert}w=\frac{1}{\sqrt{11}}w$
\par Pour le choix du deuxième élément de la base, on a juste besoin d'un élément de $F^\perp$, donc on peut choisir le vecteur normé qu'on veut. On prend $e_2' = \frac{1}{\sqrt{10}}(e_1-3e_2)$
\par On n'a plus de choix pour le troisième élément de la base cependant. Pour déterminer un vecteur qui soit orthogonal aux deux précédents, on peut utiliser le produit vectoriel, et on a $e_3' = \frac{1}{\sqrt{10}\sqrt{11}}(3e_1+e_2-10e_3)$
\par On note $\mathcal{B}' = (e_1', e_2', e_3')$
\par On a alors $Mat_{\mathcal{B}'}(u) = \begin{pmatrix} 1 & 0 & 0 \\ 0 & \cos\theta & -\sin\theta \\ 0 & \sin\theta & \cos\theta\end{pmatrix}$
\par (La matrice de passage de $\mathcal{B}$ dans $\mathcal{B}'$ est la matrice d'une BOND dans une BOND, donc son inverse est égal à sa transposée.)
\par On a que $tr(A)=tr(A')$, donc $\frac{-2}{3} = 1 + 2\cos\theta$.
\par D'où $\cos\theta = \frac{-5}{6}$
\par Et donc $\theta=\pm\arccos\left(\frac{-5}{6}\right)$
\par Pour déterminer le signe, on fait le produit mixe de $w, e_2', r_{w, \theta}(e_2')$, qui vaut $\Vert w\Vert\sin \theta$ dans la base $\mathcal{B}'$.
\par Dans la base $\mathcal{B}$ d'origine, on a que leur produit mixte vaut $\left\vert\begin{matrix} 3 & \frac{1}{\sqrt{10}} & \frac{-5}{3\sqrt{10}} \\ 1 & \frac{-3}{\sqrt{10}} & \frac{8}{3\sqrt{10}} \\ 1 & 0 & \frac{-5}{3\sqrt{10}}\end{matrix}\right\vert>0$
\par Donc $\sin\theta>0$
\par Donc $\theta = +\arccos\left(\frac{-5}{6}\right)$
\end{Exe}


\subsection{Groupes et anneaux}
\Def{}{Un groupe est monogène s'il est engendré par un élément.
\par i.e. $G$ est monogène si $\exists a\in G, G = \{a^n\vert n\in\Z\}\text{ ou }\{na\vert n\in\Z\}$}
\Def{}{$G$ est cyclique si $G$ est monogène fini.}
\Def{Ordre}{Soit $G$ un groupe
\par Soit $a\in G$
\par On dit que $a$ est d'ordre fini si $\langle a\rangle$ est de cardinal fini (cyclique).
\par On dit alors que l'ordre de $a$ est $\mathrm{card}\langle a\rangle$}
\Thr{}{Soit $G$ un groupe fini et $a\in G$\begin{itemize}
\item $$\mathrm{ordre}(a) = \mathrm{card}\langle a \rangle = min\{n\in\N^*\vert a^n = e\}$$
\item $$\forall p\in\Z, a^p = e \Leftrightarrow \mathrm{ordre}(a)\vert p$$
\end{itemize}}
\Thr{Théorème de Lagrange}{Si $G$ est fini et $a\in G$, alors l'ordre de $a$ divise le cardinal de $G$.}
\Thr{}{\begin{itemize}
\item Un groupe monogène infini est isomorphe à $\Z$
\item Un groupe monogène fini $G$ est isomorphe à $\Z/n\Z$ (où $n = \mathrm{card}(G)$)
\end{itemize}}
On étudie ensuite Z/nZ
\Def{Lois de composition interne de Z/nZ}{On définit la loi additive par :
\par $$+:\left\{\begin{array}{rcl} \Z/n\Z \times \Z/n\Z & \to & \Z/n\Z \\ (\overline{x}, \overline{y}) & \mapsto & \overline{x}+\overline{y} = \overline{x+y}\end{array}\right.$$
\par On définit la loi multiplicative par :
\par $$\times:\left\{\begin{array}{rcl} \Z/n\Z \times \Z/n\Z & \to & \Z/n\Z \\ (\overline{x}, \overline{y}) & \mapsto & \overline{x}\times\overline{y} = \overline{xy}\end{array}\right.$$}
On n'a pas l'intégrité de cet anneau si $n$ n'est pas premier.

\Thr{}{Pour $n\in\N$, $n\geq 2$
\par Soit $p\in\Z$\begin{itemize}
\item $\overline{p}$ est un générateur de $(\Z/n\Z,+)$ si, et seulement si, $p\wedge n = 1$
\item $\overline{p}$ est un inversible de $(\Z/n\Z,\times)$ si, et seulement si, $p\wedge n = 1$
\end{itemize}
Donc $\Z/n\Z$ est un corps si, et seulement si, $n$ est premier. On note $\mathbb{F}_n$ ce corps.}
\Thr{Euler-Fermat}{Soit $n\in\N,n\geq 2$ et $a\in(\Z/n\Z)^\times$
\par Alors :
\par $$a^{\varphi(n)}=\overline{1}$$}
\Thr{Lemme chinois}{Si $(n,p)\in(\N\backslash\{0,1\})^2$ avec $n\wedge p=1$,
\par Alors $\Z/np\Z$ est un anneau isomorphe à $\Z/n\Z\times \Z/p\Z$}
\Def{Indicatrice d'Euler}{On note $\varphi(n) = \mathrm{Card}(\Z/n\Z)^\times = \mathrm{Card}\{\in\llbracket 0,n\rrbracket, p\wedge n = 1\}$
\par On appelle $\varphi$ l'indicatrice d'Euler. Elle sert à compter les inversibles de $\Z/n\Z$.}
\Thr{}{Si $n\wedge p = 1$, alors $\varphi(np) =\varphi(n)\varphi(p)$}





\subsection{Idéaux et anneaux}
\Def{Anneau euclidien}{$A$ est un anneau euclidien si :\begin{itemize}
\item $A$ est un anneau commutatif intègre ($AB=0 \Rightarrow A=0\text{ ou } B=0$)
\item $A$ est muni d'une division euclidienne : il existe $\varphi\in\mathcal{F}(A\backslash\{0\},\N)$ telle que $\forall b\in A\backslash\{0\}, \forall a\in A, \exists q,r\in A, a =bq+r$ et $(r=0\text{ ou }\varphi(r)<\varphi(b)$
\end{itemize}}
\Def{Divisibilité}{Soit $A$ un anneau euclidien (ou intègre). Soit $a,b\in A$. \par On dit que $b$ divise $a$ (noté $b|a$) ou que $a$ est un multiple de $b$ si $\exists c\in A, a = bc$}
\Thr{Association}{Soit $A$ un anneau euclidien \par Soit $a,b\in A$, $a$ divise $b$ et $b$ divise $a$ si, et seulement si, il existe un inversible tel que $b = au$}
\Def{Idéal}{Soit $A$ un anneau euclidien. Soit $I\subset A$. \par On dit que $I$ est un idéal si : \begin{itemize}
\item $I$ est un sous-groupe de $(A,+)$
\item $\forall i\in I, \forall a\in A, ia\in I$
\end{itemize}}
\Def{PGCD}{Avec $A$ un anneau euclidien, soit $a,b\in A$, on appelle $pgcd(a,b)$ un générateur de $aA+bA$
\par Dans $\Z$ ou $\K[X]$, on choisit un générateur spécifique, noté $a\wedge b$, et on l'appelle le pgcd.
\par $aA+bA =(a\wedge b)A$}

\Thr{}{Avec $J$ un ensemble quelconque, $(I_j)_{j\in J}$ une famille d'idéaux de $A$, alors $\cap_{j\in J} I_j$ est un idéal}
\Thr{}{Si $I_1$ et $I_2$ sont des idéaux, alors $I_1+I_2$ est un idéal.
\par C'est le plus petit idéal contenant $I_1\cup I_2$.}
\Thr{}{Si $A$ est un anneau euclidien, alors tout idéal de $A$ est principal.
\par Cela signifie que si $I$ est un idéal de $A$, alors $\exists a\in A, I = aA$}
\Thr{Bézout}{$a,b\in A$ sont premiers entre eux si, et seulement si, $\exists (u,v)\in A^2, au+bv=1$}
\Thr{Bézout étendu}{$$\forall a,b\in A, \forall x\in A : \exists u,v\in A, x=au+bv \Leftrightarrow (a\wedge b)\vert x$$}
\Thr{Bézout généralisé}{$(a_1,a_2...,a_n)\in A^n$ sont premiers entre eux si, et seulement si, $\exists (u_1,...,u_n)\in A^n, 1=a_1u_1+a_2u_2+...+a_nu_n$}
\Thr{}{Tout élément de $\Z$ et de $\K[X]$ peut se décomposer en un produit unique d'éléments irréductibles (éléments qui, à un inversible près, ont un seul diviseur) et d'un inversible.}

\Def{Espaces caractéristiques}{Soit $E$ un $\K$-ev de dimension finie, soit $f\in\mathcal{L}(E)$ telle que $\chi_f$ est scindé. \par $\chi_f = \prod\limits_{i=1}^p(x-\lambda_i)^{\alpha_i}$ avec $S_p(f)=\{\lambda_1,...,\lambda_p\}$ et $(\alpha_1,...,\alpha_p)\in\N^{*p}$ \par Alors $E=\bigoplus\limits_{1\leq i\leq p}\ker((f-\lambda_iid)^{\alpha_i})$ par lemme des noyaux.
\par Les sev de $E$, $F_i = \ker((f-\lambda_i)^{\alpha_i})$ sont appelés sous-espaces caractéristiques de $f$.\begin{itemize}
\item les sous-espaces caractéristiques sont stables par $f$ ;
\item $f$ est entièrement caractérisée par $f_1,f_2,...,f_p$ les endomorphismes induis par $f$ sur $F_1,...,F_p$ ;
\item $\forall i\in\llbracket 1,p\rrbracket, (f_i-\lambda_iid)^{\alpha_i}=0$, ou $f_i-\lambda_iid$ est nilpotent, donc $f_i$ a pour unique valeur propre $\lambda_i$ ;
\item les $f_i$ sont trigonalisables (leur polynôme caractéristique est scindé) ;
\item dans une base $\mathcal{B}=(\mathcal{B}_1,...,\mathcal{B}_p)$ où $\forall i\in\llbracket 1,p\rrbracket$, $\mathcal{B}_i$ est une base de diagonalisation de $f_i$, alors $Mat_\mathcal{B}(f)$ est diagonale par blocs, chaque bloc étant triangulaire de taille $\alpha_i\times \alpha_i$ ;
\item pour $i\in\llbracket 1,p\rrbracket$, $\dim F_i = \alpha_i$.
\end{itemize}}
\Thr{Lemme des noyaux}{Soit $E$ un $\K$-ev, $f\in \mathcal{L}(E)$.
\par Soit $P\in \K[X]$ tel que $P=P_1...P_n$ où $P_1,...,P_n$ sont premiers entre eux deux à deux.
\par $\ker P(f) = \bigoplus\limits_{i=1}^n\ker(P_i(f))$}




\chapter{Physique - Formulaires}
\section{Infinitésimaux}
\Def{Déplacements}{En cartésien :
\par $$\Vec{dl} = dx\Vec{e_x}+dy\Vec{e_y}+dz\Vec{e_z}$$
\par En cylindrique :
\par $$\Vec{dl} = dr\Vec{e_r} +rd\theta\Vec{e_\theta}+dz\Vec{e_z}$$
\par En sphérique :
\par $$\Vec{dl} = dr\Vec{e_r} +rd\theta\Vec{e_\theta}+r\sin\theta\Vec{e_\varphi}$$}
\Def{Surfaces}{Plan :
\par $$dS = dxdy$$
\par Cylindre (bord) :
\par $$dS = rd\theta dz$$
\par Cylindre (intérieur) :
\par $$dS = rd\theta dr$$
\par Sphère (bord) :
\par $$dS = rd\theta r\sin\theta d\varphi$$}
\Def{Volumes}{Parallélipipède :
\par $$d\tau = dV = dxdydz$$
\par Morceau de cylindre :
\par $$d\tau = dV = rd\theta drdz$$
\par Morceau de sphère :
\par $$d\tau = dV = r^2\sin\theta drd\theta d\varphi$$}



\section{Identités thermodynamiques}
Pour un système fermé de composition fixe sans travail autre que celui des forces de pression :
\par $$dU = -pdV + TdS$$
\par $$dH = Vdp + TdS$$
\par $$dS = \frac{\delta Q}{T_e}+\delta S_c$$
\par $$dG = VdP -SdT+\sum\limits_i\mu_i dn_i$$


\section{Analyse vectorielle}
\Thr{Stokes(-Ampère)}{$$\oint_\mathcal{C}\Vec{A}\Vec{dl} = \iint_\mathcal{S}(\Rot\Vec{A})\Vec{dS}$$
\par où $\mathcal{C}$ est un contour fermé orienté et $\mathcal{S}$ une surface qui s'appuie sur ce contour, orienté dans son sens direct}
\Thr{(Green-)Ostrogradski}{$$\oiint_\mathcal{S}\Vec{A}\Vec{dS}=\iiint_\mathcal{V}(\Div \Vec{A})dV$$
\par où $\mathcal{S}$ est une surface fermée avec normale sortante qui entoure le volume $\mathcal{V}$}
\Thr{Du gradient}{$$\oiint_\mathcal{S}f\Vec{dS} = \iiint_\mathcal{V}(\Grad f)dV$$}
\Thr{Identités entre opérateurs}{\begin{itemize}
\item $$\Rot(\Rot\Vec{A}) = \Grad(\Div\Vec{A})-\Delta\Vec{A}$$
\item $$\Rot(\Grad f)=\Vec{0}$$
\item $$\Div(\Rot\Vec{A}) =0$$
\end{itemize}}

\Def{Nabla}{Le vecteur :
\par $$\nabla = \left|\begin{matrix}\dfrac{\partial}{\partial x}\\\dfrac{\partial}{\partial y}\\\dfrac{\partial}{\partial z}\end{matrix}\right.$$}
\Def{Gradient}{$$\Grad f = \Phyvex{\dfrac{\partial f}{\partial x}}{\dfrac{\partial f}{\partial y}}{\dfrac{\partial f}{\partial z}}$$
\par En cylindrique :
\par $$\dfrac{\partial f}{\partial r}\Vec{e_r}+\frac{1}{r}\dfrac{\partial f}{\partial\theta}\Vec{e_\theta}+\dfrac{\partial f}{\partial z}$$
\par En sphérique :
\par $$\dfrac{\partial f}{\partial r}\Vec{e_r}+\frac{1}{r}\dfrac{\partial f}{\partial\theta}\Vec{e_\theta}+\frac{1}{r\sin\theta}\dfrac{\partial f}{\partial\varphi}\Vec{e_\varphi}$$
}
\Def{Divergence}{$$\Div \Vec{A} = \dfrac{\partial \Vec{A}}{\partial x} + \dfrac{\partial \Vec{A}}{\partial y} +\dfrac{\partial \Vec{A}}{\partial z}$$
\par En cylindrique :
\par $$\frac{1}{r}\dfrac{\partial rA_r}{\partial r}+\frac{1}{r}\dfrac{\partial A_\theta}{\partial\theta}+\dfrac{\partial A_z}{\partial z}$$
\par En sphérique :
\par $$\frac{1}{r^2}\dfrac{\partial r^2A_r}{\partial r}+\frac{1}{r\sin\theta}\dfrac{\partial \in\theta A_\theta}{\partial \theta}+\frac{1}{r\sin\theta}\dfrac{\partial A_\varphi}{\partial\varphi}$$
}
\Def{Rotationnel}{$$\Rot \Vec{A} = \Phyvex{\dfrac{\partial \Vec{A_z}}{\partial y}-\dfrac{\partial\vec{A_y}}{\partial z}}{\dfrac{\partial \vec{A_x}}{\partial z}-\dfrac{\partial\Vec{A_z}}{\partial x}}{\dfrac{\partial \Vec{A_y}}{\partial x}-\dfrac{\partial \vec{A_x}}{\partial y}}$$
\par En cartésien, on le retrouve avec $\Vec{\nabla}\wedge \Vec{A}$
\par En cylindrique :
\par $$\left(\frac{1}{r}\dfrac{\partial A_z}{\partial\theta}-\dfrac{\partial A_\theta}{\partial z}\right)\Vec{e_r}+\left(\dfrac{\partial A_r}{\partial z}-\dfrac{\partial A_z}{\partial r}\right)\Vec{e_\theta}+\dfrac{1}{r}\left(\dfrac{\partial (rA_\theta)}{\partial r}-\dfrac{\partial A_r}{\partial \theta}\right)\Vec{e_z}$$
\par En sphérique :
\par $$\frac{1}{r\sin\theta}\left(\dfrac{\partial \sin\theta A_\varphi}{\partial\theta}-\dfrac{\partial A_\theta}{\partial\varphi}\right)\Vec{e_x}+\frac{1}{r}\left(\frac{1}{\sin\theta}\dfrac{\partial A_r}{\partial\varphi}-\dfrac{\partial rA_\varphi}{\partial r}\right)\Vec{e_\theta}+\frac{1}{r}\left(\dfrac{\partial rA_\theta}{\partial r}-\dfrac{\partial A_r}{\partial \theta}\right)\Vec{e_\varphi}$$
}
\Def{Laplacien scalaire}{$$\Delta f = \dfrac{\partial^2 f}{\partial x^2} + \dfrac{\partial^2 f}{\partial y^2} +\dfrac{\partial^2 f}{\partial z^2}$$
\par En cylindrique :
\par $$\frac{1}{r}\dfrac{\partial}{\partial r}\left(r\dfrac{\partial f}{\partial r}\right)+\frac{1}{r^2}\dfrac{\partial^2f}{\partial \theta^2}+\dfrac{\partial^2f}{\partial z}$$
\par En sphérique :
\par $$\frac{1}{r^2}\dfrac{\partial}{\partial r}\left(r^2\dfrac{\partial f}{\partial r}\right) +\frac{1}{r^2\sin\theta}\dfrac{\partial}{\partial\theta}\left(\sin\theta\dfrac{\partial f}{\partial\theta}\right)+\frac{1}{r^2\sin^2\theta}\dfrac{\partial^2 f}{\partial \varphi^2}$$
}
\Def{Laplacien vectoriel}{$$\Delta\Vec{A} = \Phyvex{\Delta A_x}{\Delta A_y}{\Delta A_z}$$}



\chapter{Physique - sup}
\section{L'électrocinétique}
\Thr{Loi des noeuds}{La somme des courants entrants dans un noeud est égale à la somme des courants sortants.}
\Thr{Loi des mailles}{La somme des tensions dans une maille est nulle.}
\Thr{Pont diviseur de tension}{Si la tension entre deux dipôles d'impédance $Z_1$ et $Z_2$ vaut $U_e$ et qu'on cherche la tension $U$ aux bornes de $Z_1$, alors :
\par $$U =\frac{Z_1U}{Z_1+Z_2}$$}
\Thr{Pont diviseur de courant}{Avec deux dipôles en parallèle, $Z_1$ dans lequel passe le courant $i_1$ et $Z_2$ dans lequel passe le courant $i_2$, avec $i$ le courant qui arrive dans le noeud qui se sépare dans les branches des deux :
\par $$i_1 =\dfrac{R_2i}{R_1+R_2}$$}
\Def{La résistance}{Un dipôle, qui en convention récepteur a une tension de :
\par $$U = Ri$$
\par Avec $R$ sa résistance en ohms $\Omega$.
\par Son impédance complexe est $R$.
\par Son admittance complexe est $\frac{1}{R}$.}
\Def{Le condensateur}{Un dipôle constitué de deux barres de métal avec un isolant entre les deux, tel que, en convention récepteur :
\par $$i = C\dfrac{d u}{dt}$$
\par $C$ est la capacité du condensateur, en farads F. ($=A.s.V^{-1}=m^{-2}kg^{-1}s^4A^2$)
\par Il y a continuité de la tension dans un condensateur.
\par Sur chaque barre du condensateur, on peut mettre une charge $q$. Une barre est chargée positivement, l'autre l'est négativement.
\par La charge respecte la relation :
\par $$q = Cu$$
\par Son impédance complexe est $\frac{1}{jC\omega}=-\frac{j}{C\omega}$
\par Son admittance complexe est $jC\omega$
\par En basse fréquence, un condensateur se comporte comme un interrupteur ouvert.
\par En haute fréquence, un condensateur se comporte comme un fil.}
\Def{Le solénoïde}{Un dipôle constitué de fils de métal enroulés, tel que, en convention récepteur :
\par $$U = L\dfrac{di}{dt}$$
\par $L$ est son inductance en Henry H. ($V.s.A^{-1}$)
\par Il y a continuité du courant au travers d'un solénoïde.
\par Son impédance complexe est $jL\omega$
\par Son admittance complexe est $\frac{1}{jL\omega}=-\frac{j}{L\omega}$
\par En basse fréquence, un solénoïde se comporte comme un fil.
\par En haute fréquence, un solénoïde se comporte comme un interrupteur ouvert.}
\Def{Fonction de Transfert}{Un filtre est un quadripôle, avec une tension en entrée $U_e$ et une tension en sortie $U_s$. On définit :
\par $$H = \frac{U_s}{U_e}$$
\par qui est appelée la fonction de transfert du filtre. À partir d'elle, on définit aussi :
\par $$G = \vert H\vert$$
\par qui est le gain du filtre. On définit :
\par $$G_{dB} = 20\log(\vert H\Vert)$$
\par qui est le gain en décibels.}
\Def{Ordre des filtres}{}
\Thr{Calcul pratique d'argument}{}



\newpage
\section{Optique}
Snell-Descartes $n_1\sin i_1 = n_2\sin i_2$ (réfraction) et $i_1'=-i_1$ où $i_1$ l'angle d'incidence
\par Condition de réfraction limite : si $n_2<n_1$, on note $i_{lim} =\arcsin\left(\frac{n_2}{n_1}\right)$ et $i_1<i_{lim}$ donne réfraction. Sinon, réflexion totale. $n_1<n_2$ et il y a toujours réfraction.
\par Grandissement : $\gamma = \frac{\overline{A'B'}}{\overline{AB}}$
\par Formule Descartes : $\frac{1}{\overline{OA'}}-\frac{1}{\overline{OA}}=\frac{1}{\overline{OF}}$ 
\par (et aussi $\frac{\overline{A'B'}}{\overline{AB}}=\frac{\overline{OA'}}{\overline{OA}}$)
\par Formule Newton : $\frac{\overline{A'B'}}{\overline{AB}}=\frac{\overline{FO}}{\overline{FA}}=\frac{\overline{F'A'}}{\overline{F'O}}$
\par (et aussi $\overline{FA}\cdot\overline{F'A'}=-\overline{OF}^2$)
\par Association de lentilles : $\frac{1}{f_{tot}}=\frac{1}{f_1}+\frac{1}{f_2}$


\newpage
\section{Mécanique du point}
Dans tout ce chapitre, on se place dans un référentiel galiléen $\mathcal{R}$ et on considère un point $M$ de masse $m$, repéré par un vecteur $\Vec{OM}(t)$, qui subit des forces $(\Vec{F_i})$
\Thr{RFD}{Pour un point de masse $m$ repéré par un vecteur $\Vec{OM(t)}$ en fonction du temps, subissant des forces $\Vec{F_i}$ :
\par $$m\dfrac{d^2\Vec{OM(t)}}{dt^2} = \sum\Vec{F_i}$$
\par On note $\Vec{p} = m\dfrac{d\Vec{OM(t)}}{dt} = m\Vec{v}(t)$ la quantité de mouvement du point, en 
\par On note $E_c=\frac{1}{2}mv^2 =\frac{p^2}{2m}$ l'énergie cinétique d'un point, en joules $J$.}
\Def{Grandeurs relatives à une force}{Une force est représentée par un vecteur, qui a une norme en Newton, ou en $kg.m.s^{-2}$.
\par On définit la puissance d'une force $\Vec{F}$ s'appliquant sur un point qui va à la vitesse $\Vec{v}$ comme :
\par $$\mathcal{P}=\Vec{F}\cdot\Vec{v}$$
\par La puissance est en watts $W=J.s^{-1} = kgm^2s^{-3}$
\par On définit le travail d'une force $\Vec{F}$ de puissance $\mathcal{P}$ comme :
\par $$W = \int_0^\tau\mathcal{P}(t)dt$$
\par Le travail est une énergie, qui s'exprime en joules $J$
\par Si le travail entre deux points $A$ et $B$ peut s'exprimer sous la forme de $E_p(A)-E_p(B)$, alors on dit que la fonction $E_p$ représente l'énergie potentielle de la force. On dit aussi que la force est conservative.
\par Il est à noter que $E_p$ est définie à une constante additive près.
\par Si le travail est positif, on le dit moteur. Sinon, on le dit résistant. S'il est nul, la force ne travaille pas (elle est orthogonale à la trajectoire)}
\Thr{Théorème de la puissance mécanique TPM}{En tout instant :
\par $$\dfrac{d M(t)}{dt} = \sum \mathcal{P}_i$$}
\Thr{Théorème de l'énergie cinétique TEC}{En intégrant la relation précédente, on a :
\par $$\Delta E_c = \sum W_i$$}
\Thr{Conservation de l'énergie mécanique TEM}{Si toutes les forces s'appliquant à $M$ sont conservatives, on peut écrire :
\par $$E_c + \sum E_{p,i} = cste$$
\par Même si elle n'est pas constante, on note $E_m =E_c + \sum E_{p,i} = E_c + E_{ptot}$ l'énergie mécanique du point.
\par Les extréma de $E_{ptot}$ sont appelés des points d'équilibres. Les maxima sont des points instables (un changement d'énergie potentielle va leur faire descendre une pente), et les minima sont des points stables.
\par On les identifie en regardant les points critiques de $E_{ptot}$ puis en calculant sa dérivée seconde (si elle est positive, c'est un minimum, si elle est négative c'est un maximum)}

\Thr{Modélisation des forces usuelles}{\textbf{Tension ressort :} sa masse est négligeable et on a, avec $\Vec{e}_r$ la direction du ressort, $l$ sa longueur, $k$ son coefficient de raiseur et $l_0$ sa longueur au repos : $\Vec{T} = -k(l-l_0)\Vec{e}_r$
\par\textbf{Energie potentielle tension ressort :} $E_{pe}= \dfrac{1}{2}k(l-l_0)^2+cste$
\par\textbf{Pression :} La pression exercée par un fluide sur un plan $S$ solide est $\Vec{F} = PS\Vec{n}$ avec $\Vec{n}$ normal à la surface, dirigé du fluide vers le solide et $P$ pression du fluide. Si un solide est totalement immergé, la force est la poussée d'Archimède, égale à l'opposée du poids du fluide déplacé.
\par\textbf{Frottement fluide :} Quand un objet est en mouvement dans un fluide, on a la force de frottement : $\Vec{F}_f = -h\Vec{v}(M)$ ou $\Vec{F}_f = -h\Vec{v}(M)^2$, avec $h$ une constante.
\par\textbf{Gravitation :} La gravité entre deux points $A$ et $B$ est : $\Vec{F}_{g_{A\to B}} = -G\dfrac{m_Am_B}{AB^2}\Vec{e}_{A\to B}$ avec $G=6.674.10^{-11} m^3.kg^{-1}.s^{-2}$ constante de gravitation universelle.
\par\textbf{Energie potentielle gravitation :} $E_{pg} = -G\dfrac{Mm}{r}+cste$ 
\par\textbf{Force électrique :} La force électrique entre deux points $A$ et $B$ est : $\Vec{F}_{el_{A\to B}} = \dfrac{1}{1\pi\varepsilon_0}\dfrac{q_Aq_B}{AB^2}\Vec{e}_{A\to B}$ avec $\varepsilon_0 = 8.854.10^{-12} F.m^{-1}$ la permittivité du vide.
\par\textbf{Force de Lorentz :} La force induite sur une particule de charge $q$ par un champ électrique $\Vec{E}$ et un champ magnétique $\Vec{B}$ est : $\Vec{F_L} = q\Vec{E} + q\Vec{v}\wedge \Vec{B}$ 
\par\textbf{Poids :} Sur Terre, on a $\Vec{P} = m\Vec{g}$ où $g=9.81 m.s^{-2}$ est la norme du vecteur $\Vec{g}$
\par\textbf{Energie potentielle du poids :} $E_{pp} = mgz+cste$ si $z$ est la position verticale.}

\section{Mécanique du solide}
On se place toujours dans un référentiel galiléen $\mathcal{R}$, mais désormais dans le cas d'un solide $S$ indéformable, en rotation autour d'un axe $\Delta$, sur lequel s'appliquent les forces $(\Vec{F_i})$
\Def{Moment cinétique}{Pour un point $M$, le moment cinétique par rapport à un point $O$ est le vecteur :
\par $$\Vec{L_O}(M) = m\Vec{OM}\wedge\Vec{v(M)}$$
\par Pour un point $M$, le moment cinétique par rapport à l'axe $\Delta$ contenant le point $O$, orienté par $\Vec{e_\Delta}$ est :
\par $$L_\Delta(M)=\Vec{L_0}(M)\cdot\Vec{e\delta} = m[\Vec{OM}\wedge\Vec{v(M)}]\cdot\Vec{e_\Delta}$$
\par C'est donc le projeté du moment cinétique de $M$ par rapport à $O$ sur l'axe $\Delta$
\par On peut définir à partir de ça le moment cinétique d'un système par rapport à un point ou un axe, ça sera la résultante des moments de tous les points du système.}
\Def{Moment d'une force}{Pour une force $\Vec{F}$, son moment vectoriel par rapport au point $O$ est le vecteur :
\par $$\Vec{\mathcal{M}_O}(\Vec{F}) = \Vec{OM}\wedge \Vec{F}$$
\par On peut aussi définit son moment scalaire selon l'axe $\Delta$ passant par $O$ et de vecteur unitaire $\Vec{e_\Delta}$ :
\par $$\mathcal{M}_\Delta(\Vec{F}) = \Vec{\mathcal{M}_O}(\Vec{F})\cdot\Vec{e_\Delta} = [\Vec{OM}\wedge\Vec{F}]\cdot\Vec{e_\Delta}$$}
\Thr{Théorème du moment cinétique}{On a :
\par $$\dfrac{dL_\Delta(M)}{dt} = \sum\mathcal{M}_\Delta(\Vec{F_i})$$
\par Ce qu'on peut écrire en vectoriel par :
\par $$\dfrac{d\Vec{L_O(M)}}{dt} = \sum\Vec{\mathcal{M}_O(\Vec{F_i})}$$}

\Def{Rotation}{Un solide $S$ est en rotation autour d'un axe $\Delta$ si tous ses points ont une trajectoire circulaire centrée sur un point de $\Delta$.
\par Donc tous les points du solide sont à une distance constante les uns des autres, on n'a besoin que d'un seul angle $\theta$ pour tous les repérer dans l'espace.
\par On définit $\omega = \dot{\theta}$ la vitesse angulaire du solide.
\par On définit $\dot{\omega}=\ddot{\theta}$ l'accélération angulaire du solide.
\par Comme $S$ est en rotation, pour un point $M$ de $S$ on peut écrire :
\par $$\Vec{v(M)} = r\omega \Vec{e_\theta}$$
\par $$\Vec{\Gamma(M)} = -r\omega^2\Vec{e_r} + r\dot{\omega}\Vec{e_\theta}$$}
\Def{Moment d'inertie}{Pour $S$ un solide en rotation autour de $\Delta$, on a toujours que :
\par $$L_\Delta(S) = J_\Delta\omega$$
\par Où $J_\Delta$ est une constante, appelée le moment d'inertie du solide.
\par Donc dans la formule du moment cinétique, on aura très simplement :
\par $$\dfrac{dL_\Delta(S)}{dt}=J_\Delta\dot{\omega}$$
\par L'énergie cinétique du solide peut aussi être calculée très facilement, et vaut :
\par $$ E_c(S) = \frac{1}{2}J_\Delta\omega^2$$}
\Thr{Exemples de moments d'inertie}{Pour une tige de masse $m$ de longueur $L$ par rapport à un axe orthogonal : $J_\Delta = \dfrac{1}{3}mL^2$ si la tige touche l'axe en ses extrémités ; $J_\Delta = \dfrac{1}{12}mL^2$ si l'axe est au milieu de la tige.
\par Pour un cerceau de masse $M$ et de rayon $R$ par rapport à son axe de révolution : $J_\Delta=mR^2$.
\par Disque ou cylindre de masse $m$, de rayon $R$ par rapport à son axe de révolution : $J_\Delta = \dfrac{1}{2}mR^2$.
\par Boule pleine de masse $m$, de rayon $R$ par rapport à l'un de ses axes de révolution : $J_\Delta = \dfrac{2}{5}mR^2$.}
\Def{Forces à considérer}{On peut, dans un solide en rotation, avoir deux forces de résultante nulle (elles ont même norme) mais dont la somme des moments ne l'est pas. On l'appelle un couple de forces.
\par Il y a aussi, pour la rotation entre le solide et l'axe, un pivot qui peut être solide. Soit on néglige le frottement engendré par ce pivot et on dit qu'on a une liaison pivot parfaite, soit on la considère comme un couple de forces résistant.}
\Thr{Théorème du moment cinétique}{On a vu comment on devait écrire $\dfrac{dL_\Delta}{dt}$, ce qui quand on réinjecte dans le théorème du moment cinétique donne :
\par $$J_\Delta\dfrac{d\omega}{dt} = \sum\mathcal{M}_\Delta(_vec{F})$$}
\Def{Grandeurs associées à une force}{Dans ce système, on peut réécrire la puissance d'une force comme :
\par $$\mathcal{P} =\mathcal{M}_\Delta\omega$$
\par Et son travail comme :
\par $$W = \int_{\theta_1}^{\theta_2}\mathcal{M}_\Delta d\theta$$}
\Thr{Théorème de l'énergie cinétique}{On peut donc réécrire le théorème de l'énergie cinétique :
\par $$\frac{1}{2}J_\Delta(\omega_f^2-\omega_i^2) = \sum W^{i\to f}$$}


\newpage
\section{Thermo}
\subsection{Premier Principe}
\Def{Transformation}{Une transformation passe d'un état d'équilibre thermodynamique $A$ à un état d'équilibre thermodynamique $B$.
\par Les équilibres sont mécaniques ou thermiques :
\par Un équilibre mécanique est un état où la pression n'évolue plus.
\par Un équilibre thermique est un état où la température n'évolue plus.
\par Un équilibre thermodynamique est un état d'équilibre thermique et mécanique.}
\Def{Réversibilité}{Une transformation est réversible si elle passe par une succession continue d'états d'équilibre.
\par Une transformation est quasi-statique si, en tout instant, $P_{int} =P_{ext}$. Elle n'est pas forcément réversible, ça dépendra de l'évolution de la température.
\par Une transformation non-réversible est ditre irréversible.}
\Def{Types de transformations}{\begin{itemize}
\item Une transformation est isotherme si $T=cste$
\item Une transformation est isobare si $p=cste$
\item Une transformation est isochore si $V =cste$
\item Une transformation est monobare si $p_{ext} =cste$
\item Une transformation est monotherme si $T_{ext}=cste$
\item Une transformation est adiabatique si $Q=0$
\item Une transformation est isentropique si $\Delta S = 0$, elle doit être adiabatique et réversible.
\end{itemize}}
\Def{Travail}{On note $W$ le travail de toutes les forces non-conservatives sur le système.
\par On a $\delta W = -P_edV$ pour les forces de pression (donc la majorité des cas)
\par $W$ est le travail reçu par le système ; $W>0$ et le système reçoit de l'énergie mécanique, $W<0$ et il en fournit.}
\Def{Chaleur}{On note $Q$ l'ensemble des transferts énergétiques non-mécaniques au système. 
\par $Q$ est la chaleur reçue par le système, si elle est négative alors le système fournit de la chaleur, si elle est positive alors il en reçoit.}
\Def{}{On note $U$ l'énergie d'un système, distincte de l'énergie cinétique et potentielle. Elle est quand même en joules $J$.}
\Thr{Premier principe de la thermodynamique}{Pour un système immobile :
\par $$dU = \delta W + \delta Q$$
\par $$U = W+Q$$
\par On peut aussi écrire de manière plus générale
\par $$\Delta E = W+Q$$
\par Avec $\Delta E$ la différence d'énergie thermodynamique ($\Delta E = \Delta E_c+\Delta E_p+\Delta U$)}
\Def{Enthalpie}{On note $H$ l'enthalpie en joules $J$ d'un système définie par :
\par $$H=U+PV$$
\par $H$ ne dépend que de la température, tout comme l'énergie.}
\Thr{Lois de Joule}{On a l'existence d'une constante appelée la capacité thermique d'un système, telle que :
\par $$\Delta U = C_P\Delta T$$
\par On a que $C_P = \dfrac{\partial H}{\partial T}$
\par On définit la capacité thermique molaire :
\par $$C_{P_m} = \frac{C_p}{n}$$
\par On définit la capacité thermique massique :
\par $$c_P = \frac{C_P}{m}$$
\par On peut définir les mêmes valeurs pour l'enthalpie, en partant de $C_V$ la capacité calorifique, définie telle que :
\par $$\Delta H = C_V\Delta T$$
\par D'où, pour un gaz parfait :
\par $$C_V = C_p+nR$$
\par Et on a de même $C_{V_m}=\frac{C_V}{n}$ et $c_V=\frac{C_V}{m}$
\par Ce qui amène à définir le coefficient de Laplace : 
\par $$\gamma=\frac{C_P}{C_V}=\frac{C_{P_m}}{C_{V_m}} =\frac{c_P}{c_V}$$}
\Thr{Pour un gaz parfait}{Pour des gaz parfaits monoatomiques, on a :
\par \begin{align*} C_P = \frac{5}{2}nR & & C_{P_m} = \frac{5}{2}R & & c_P = \frac{5}{2}\frac{R}{M}\\
C_V = \frac{3}{2}nR & & C_{V_m} = \frac{3}{2}R & & c_V = \frac{3}{2}\frac{R}{M}
\end{align*}
\par Pour des gaz parfaits diatomiques, on a :
\par \begin{align*} C_P = \frac{7}{2}nR & & C_{P_m} = \frac{7}{2}R & & c_P = \frac{7}{2}\frac{R}{M}\\
C_V = \frac{5}{2}nR & & C_{V_m} = \frac{5}{2}R & & c_V = \frac{5}{2}\frac{R}{M}
\end{align*}}
\Thr{Pour une phase condensée}{Dans le cas d'une phase condensée, les capacités calorifiques sont égales aux capacités thermiques. On note :
\par $$C = C_V =C_P$$
\par $$C_m = C_{V_m} = C_{P_m}$$
\par $$c = c_V = c_P$$}

\subsection{Second principe}
\Thr{Second principe de la thermodynamique}{Il existe pour tout système thermodynamique une fonction d'état $S$ appelée l'entropie telle que pour toute transformation :
\par $$S_i\leq S_f$$
\par L'entropie est en joules par Kelvin $J.K^{-1}$
\par L'inégalité est stricte si la transformation est irréversible, et c'est une égalité si elle est réversible.
\par L'entropie se sépare entre :\begin{itemize}
\item L'entropie échangée $S_e$, qui correspond aux échanges thermiques. On a $\delta S_e=\frac{\delta Q}{T_{ext}}$.
\item L'entropie de création $S_c$ correspond à la variation d'entropie : si la transformation est réversible, elle est nulle. Sinon, elle est strictement positive.
\end{itemize}
\par On a donc :
\par $$\Delta S = S_e +S_c = \frac{Q}{T_{ext}}+ S_c\text{ ou }dS = \dfrac{\delta Q}{T_e}+\delta S_c$$}
\Thr{}{Si le système fait des échanges avec plusieurs thermostats, on écrira :
\par $$\delta S_e = \frac{\delta Q_1}{T_1}+\frac{\delta Q_2}{T_2}+...+\frac{\delta Q_p}{T_p}$$}
\Thr{Calcul d'entropie}{La formule pour calculer l'entropie d'une phase condensée est :
\par $$S(T_B) - S(T_A)=c\ln\left(\frac{T_B}{T_A}\right)$$
\par Pour un gaz parfait, on a deux cas :
\par \begin{align*}
\text{P fixe} && S(T_B, V_B)-S(T_A, V_A) = nC_{V_m}\ln\left(\frac{T_B}{T_A}\right)+nR\ln\left(\frac{V_B}{V_A}\right)\\
\text{V fixe} && S(T_B, p_B)-S(T_A, p_A) = nC_{P_m}\ln\left(\frac{T_B}{T_A}\right)-nR\ln\left(\frac{p_B}{p_A}\right)
\end{align*}}
\Thr{Loi de Laplace}{Pour une transformation isentropique d'un gaz parfait, on peut écrire :
\par $$pV^\gamma = cste$$
\par $$TV^{\gamma-1}=cste$$
\par $$T^\gamma p^{\gamma-1}=cste$$}

\subsection{Machines thermiques}
\Thr{Enthalpie de changement d'état}{Lors d'un changement d'état, on ne peut pas écrire les lois de Joule directement. À la place, on doit décomposer le changement d'état :
\par $A$ est l'état de départ de la transformation (état 1).
\par $B$ est l'état d'arrivée de la transformation (état 2).
\par $C$ est le point du changement d'état sur le diagramme.
Alors :
\par $$\Delta H = C_{V_2}(T_B-T_C) + \Delta h_{1->2}T_C+C_{V_1}(T_C-T_A)$$
\par $$\Delta H = C_{V_2}(T_B-T_c) + h_2(T_C)-h_1(T_C) +C_{V_1}(T_C-T_A)$$}
\Def{Machine thermique}{Une machine thermique est une machine qui contient un fluide (liquide ou gaz) qui subit des cycles de transformations.
\par On définit son rendement comme :
\par $$\eta = \left|\frac{\text{transfert utile}}{\text{transfert paye}}\right|$$
\par Pour tout moteur, $\eta$ est inférieur ou égal au rendement du cycle s'il était réversible.}



















\newpage
\section{Chimie}
\subsection{Cinétique}
\Def{Généralités}{Une espèce (physico-chimique) est une substance avec une formule chimique et un état (solide, liquide, gaz, soluté).
\par La composition d'un système est l'ensemble des quantités de matière des constituants du système.
\par Une espèce dissoute dans un solvant est un soluté, si le solvant est l'eau, l'espèce est aqueuse. Toutes les espèces dans l'eau sont dissoutes, sauf l'eau.
\par On associe à une espèce dissoute une concentration en $mol.L^{-1} = \frac{n}{V}$ où $n$ est la quantité de matière de l'espèce et $V$ le volume total. On définit la concentration en masse comme la concentration multipliée par la masse molaire de l'espèce.
\par Une espèce gazeuse se comporte comme un gaz parfait dans cette modélisation, on a donc : $PV = n_{totgaz}RT$
\par La pression partielle d'une espèce indicée i est donc $P_i V=n_iRT$. La fraction molaire d'un constituant dans un mélange est $\frac{n_i}{n_{totgaz}}$
\par Une transformation est totale (resp. limitée) si un réactif appelé limitant disparaît (resp. lorsque l'avancement total n'atteint pas sa valeur maximale).
\par L'état final d'une transfomation est un état d'équilibre chimique : $x_f = x_{eq}$ 
}
\Thr{Evolution chimique}{Le quotient de réaction est défini par $Q_r = \prod\limits_{i} a(A_i)^{v_i}$, où $a(A_i)$ réfère à l'activité d'une espèce :\begin{itemize}
\item L'activité d'un solide ou d'un liquide pur vaut toujours 1.
\item L'activité d'un soluté est $a(A_i) = \frac{[A_i]}{c^0}$ avec $[A_i]$ concentration molaire et $c^0 = 1 mol.L^{-1}$
\item L'activité d'un gaz parfait est $a(A_i) = \frac{P_i}{P^0}$ où $P_i$ pression partielle et $P^0 = 1 bar = 10^5 Pa$
\end{itemize}
Le quotient de réaction tend vers une constante d'équilibre $K^0(T)$ qui ne dépend que de la température. On a en déduit la loi d'action de masse : $K^0(T)=\prod\limits_{i}a(A_i)_{eq}^{v_i}$.
\par On a que si $Q_r<K^0(T)$ (resp. $Q_r=K^0(T)$, $Q_r>K^0(T)$), le système évolue dans le sens direct de l'équation de réaction (resp. est à l'équilibre, évolue dans le sens indirect).
}
\Def{Vitesses dans une réaction}{La vitesse de consommation d'un réactif (resp. de formation d'un produit) $A_i$ est définie par : $v_{d,A_i} = -\dfrac{d[A_i]}{dt}$ (resp. $v_{f,A_i} = \dfrac{d[A_i]}{dt}$)
\par La vitesse de volumique de réaction de la réaction $\sum\limits_{i} v_iA_i=0$ est définie par : $v=\frac{1}{V}\dfrac{dx}{dt}$ avec $x$ l'avancement. Alors pour tout constituant, $v= \frac{1}{v_i V}\dfrac{dn_{A_i}}{dt}$
\par Si la réaction admet un ordre, la vitesse de réaction s'écrit sous la forme : $v=k\prod\limits_{i}[A_i]^{\alpha_i}$ ; $\alpha_i$ est l'ordre partiel de la réaction par rapport à $A_i$, distinct de $v_i$ ; $k$ est la constante de vitesse de la réaction. $\sum\limits_{i} \alpha_i$ est appelé ordre total de la réaction.
\par La constante $k$ ne dépend que de la température. Loi empirique d'Arrhenius : $k = A\exp(-\dfrac{E_a}{RT})$ où $A$ est une constante et $E_a$ est l'énergie molaire d'activation de la réaction ($J.mol^{-1}$)
}
\Meth{Calculs d'ordre}{On a ces avancements pour l'espèce $A$ selon l'ordre de sa réaction :\begin{itemize}
\item En ordre 0 : $-\frac{1}{\vert v_A\vert}\dfrac{d[A]}{dt}=k\Rightarrow [A]=[A]_0-\vert v_a\vert kt$
\item En ordre 1 : $-\frac{1}{\vert v_A\vert}\dfrac{d[A]}{dt}=k[A]\Rightarrow \ln([A])=\ln([A]_0)-\vert v_a\vert kt$
\item En ordre 2 : $-\frac{1}{\vert v_A\vert}\dfrac{d[A]}{dt}=k[A]^2\Rightarrow \dfrac{1}{[A]}-\dfrac{1}{[A]_0}=\vert v_a\vert kt$
\item Avec les produits $\vert v_a\vert A + \vert v_b\vert B$, si on a des ordres partiels, alors $-\dfrac{1}{\vert v_a\vert}\dfrac{d[A]}{dt}=k[A]^\alpha[B]^\beta$ qui peut parfois se simplifier.
\end{itemize}
}
\Meth{Temps de demi-réaction}{Le temps de demi-réaction est le temps au bout duquel la moitié de l'avancement final est atteint. Avec une réaction totale d'équation $\vert v_A\vert A = produits$, on a :\begin{itemize}
\item En ordre 0 : $t_{1/2} = \frac{[A]_0}{2\vert v_A\vert k}$ 
\item En ordre 1 : $t_{1/2} = \frac{ln(2)}{\vert v_A\vert k}$
\item En ordre 2 : $t_{1/2} = \frac{1}{\vert v_A\vert k[A]_0}$
\end{itemize}
}
\Thr{Beer-Lambert}{La loi de Beer-Lambert est que $A = \varepsilon_{\lambda,T}lc$ avec $\varepsilon_{\lambda,T}$ le coefficient d'absorption molaire à la température T et à la longueur d'onde $\lambda$, $l$ la longeuur de la cuve du spectrophotomètre et $c$ la concentration molaire de l'espèce qui absorbe.
\par La conductivité d'une solution s'écrit $\sigma =\sum\limits_{i, ions}\lambda_i c_i$ où $\lambda_i$ est la conductivité molaire de l'ion i ($Sm^2mol^{-1}$) et $c_i$ la concentration molaire de l'ion.
\par On note $G=k_{cell}\sigma$ où $k_{cell}$ la constante de cellule de la sonde du conductimètre.
}

\subsection{Molécules et ions}
\Meth{Déterminer un schéma de Lewis}{La position dans le tableau périodique permet de déterminer le nombre d'électrons de valence : 1 pour la colonne 1, 2 pour la 2, 3 pour la 13, 4 pour la 14, 5 pour la 15, 6 pour la 16, 7 pour la 17 et 8 pour la 18 (sauf l'hélium, qui a 2)
\par une fois déterminé, on en déduit le nombre $P_v$ de paires de valence : $N_v = \sum\limits_{i}n_i$ et on en déduit $P_v=\frac{N_v}{2}$.
\par On décide d'un atome central, on répartit ensuite les doublets liants pour respecter la règle de l'octet (et plus rarement celle du duet).
\par Pour calculer la charge formelle d'un atome dans l'édifice, on détermine son nombre d'électrons de valence $n_v$, puis on détermine son nombre d'électrons de valence dans la molécule noté $n_{v,m}$ (on compte 2 pour tout doublet non-liant, 1 pour tout doublet liant, un par électron célibataire). On en déduit alors la charge formelle $c_f = n_v-n_{v,m}$ 
}

\Meth{Déterminer la configuration électronique}{\begin{enumerate}
\item Déterminer le nombre total d'électrons de l'atome
\item S'il y a assez d'électrons pour remplir une sous-couche, on la note de la forme $md^n$, où $n$ est le nombre d'électrons maximal de la couche, $m$ un nombre (qui indique la couche) et $d$ une lettre (qui indique la sous-cocuhe), $md$ représente une sous-couche.
\item S'il n'y a pas assez d'électrons pour remplir une sous-couche, on la note de la forme $md^n$, où $n$ est le nombre d'électrons qui restaient après avoir distribué les autres dans les couches précédentes.
\end{enumerate}
Les couches sont, dans l'ordre :
\par $1s$ qui peut contenir $2$ électrons
\par $2s$ qui peut contenir $2$ électrons
\par $2p$ qui peut contenir $6$ électrons
\par $3s$ qui peut contenir $2$ électrons
\par $3p$ qui peut contenir $6$ électrons
\par $3d$ qui peut contenir $10$ électrons
\par $4s$ qui peut contenir $2$ électrons
\par $4p$ qui peut contenir $6$ électrons
\par $4s$ qui peut contenir $10$ électrons
\par $4f$ qui peut contenir $14$ électrons
}


\subsection{Solides cristallins}
\Def{Grandeurs}{Coordinence : nombre de plus proches voisins d'un atome, à l'intérieur et à l'extérieur de la meille
\par Population : nombre d'entités présentes dans la maille, sans compter les bouts de l'entité dans une maille voisine. Noeuds par maille quoi.
\par Tangence : relation qui relie $a$ le rayon de la maille cubique et $r$ le rayon de l'atome (exemple : $a = 2r$ pour cubique simple)
\par Compacité $\mathcal{C}$ : rapport du volume occupé par les atomes de la maille sur le volume de la maille (compacité de 1 : tout l'espace serait occupé)
\par Rayon de Van der Waals : distances entre deux molécule plus proches voisines
\par Rayon métallique : demi-distance entre les noyaux de deux atomes plus proches voisins
\par Rayon covalent : demi-distance entre les noyaux de deux atomes liés par liaison covalente
\par Rayon ionique : distance entre anion et cation voisin (somme de leurs rayons ioniques)}

\Def{Types de cristaux}{Types de cristaux :\begin{itemize}
\item Les cristaux métalliques : ce sont des cations, dedans, les électrons de valence sont répartis dans tout le cristal. Les liaisons sont dites "métalliques" (donc fortes, il faut $100-600kJ.mol^{-1}$ pour en casser). Les liaisons ne sont pas directionnelles, vu que les électrons se baladent dans tout le cristal, sans se préoccuper des noyaux. Le métal est libertaire ???
\item Les cristaux covalents : toutes les liaisons sont covalentes, donc elles relient un électron d'un atome à un électron de l'autre (liaisons fortes, il faut $\sim100kJ.mol^{-1}$ pour en casser). Les angles entre liaisons sont régies par règles de Lewis, et donc les liaisons sont directionnelles.
\item Les cristaux moléculaire : les liaisons entre les molécules sont des liaisons hydrogènes  (donc plus faibles, il faut $10kJ.mol^{-1}$ pour en casser). Les liaisons sont directionnelles.
\item Les solides ioniques : ce qui compose les éléments de la maille, c'est pas des atomes mais des ions (genre le sel $NaCl$, ou $Na^+, Cl^-$). Dans ce cas, un des ions sera dans les lieux géométriques normaux de la maille, et l'autre ion sera dans les sites interstitiels. Encore une fois, liaisons fortes, il faut quelques centaines de $kJ.mol^{-1}$ pour casser des liaisons. La liaison n'est pas directionnelle.
\end{itemize}}


\Def{Types de mailles}{\begin{itemize}
\item Cubique simple : un atome sur tous les sommets d'un cube \begin{itemize}
    \item tangence : $a=2r$ ;
    \item population $1$ ;
    \item coordinence $6$ ;
    \item compacité $\frac{1 \times \frac{4}{3}\pi r^3}{a^3} =\frac{\pi}{6}\simeq 0,52$.
\end{itemize}
\item Hexagonale complète : hors programme
\item Cubique Face Centrée (CFC) : un atome sur tous les sommets du cube, et un atome sur toutes les faces du cube\begin{itemize}
    \item tangence $\frac{a\sqrt{2}}{2} = 2r$  (ou $a\sqrt{2} = 4r$);
    \item population $4$ ;
    \item coordinence $12$ (pour un sommet, les 3 ppv dans une maille sont sur les faces. Le sommet est partagé dans huit cubes, et il faut calculer les ppv sur chaque maille et enlever les redondants pour obtenir 12)
    \item Compacité : $\frac{4 \times \frac{4}{3}\pi r^3}{a^3} = \frac{\pi}{3\sqrt{2}}\simeq 0,74$ (qui est la compacité max possible, peu importe le sphere packing qu'on tenterait, même l'hexagonale complète fait pas mieux)
\end{itemize}
\end{itemize}}

\Thr{Sites intersticiels (CFC)}{Comme on peut le remarquer si on trace les cercles qui se touchent dans la maille CFC, il y a en fait des trous ; les sphères sur des sommets ne touchent que les sphères sur les faces, et pas les autres sphères sur les sommets.
\par Ces trous sont des sites interstitiels, intéressants parce qu'on peut insérer des atomes dedans... Ils sont en deux catégories.
\par Les sites intersticiels tétraédriques, au nombre de $8$ par maille. Ils sont situés entre un sommet et les trois atomes les plus proches (ceux sur les faces). Le rayon maximum dans lequel on peut insérer un atome dans un site tétraédrique est $0,225r$ (avec $r$ le rayon d'un atome sommet ou face)
\par Les sites octaédriques, au nombre de $4$ par maille. Ils sont situés entre deux centres et deux sommets, qui sont étendus aux mailles les plus proches (ce qui donne bien un octaèdre). Le rayon maximum d'insertion est $0,414r$ (avec $r$ le rayon d'un atome sommet ou face)
\par Les sites intersticiels permettent par exemple de faire des alliages, et faire un solide plus résistant. On parle d'insertion si l'atome est inséré dans un site interstitiel, de substitution si on enlève un atome de la maille pour le remplacer par un autre.
\par On pourrait faire la même avec la cubique simple.}

\Pre{Pour l'habitabilité du site tétraédrique :
\par On représente la vue dans le plan diagonal d'un petit cube d'arête $\frac{a}{2}$ (on considère le cube constitué par un sommet et les trois faces les plus proches)
\par En notant $r$ le rayon des sphères et $r_t$ le rayon du site tétraédrique, on obtient la relation suivante :
\par $$a\frac{\sqrt{3}}{4} = r+r_t$$
\par (Schéma du plan qui coupe deux atomes de la maille qui se touchent, rectangle de taille $2r = \frac{a}{2}\sqrt{2}$ et $\frac{a}{2}$)
\par Sachant que $4r = a\sqrt{2}$, on obtient finalement :
\par $$r_t = \left(\sqrt{\frac{3}{2}}-1\right)r \simeq 0,225r$$}
\Pre{Pour l'habitabilité du site octaédrique :
\par On représente la vue dans le plan médian d'un cube d'arête $a$ (on considère la maille classique et le site octaédrique au milieu de lui)
\par En notant $r$ le rayon des sphères et $r_o$ le rayon du site tétraédrique, on obtient la relation suivante :
\par $$2r+2r_o =a $$
\par (Schéma du plan médian qui coupe quatre atomes de la maille qui se touchent, carré de côté $a$, les centres des cercles au milieu des côtés)
\par Sachant que $4r = a\sqrt{2}$, on obtient finalement :
\par $$r_0 = \left(\sqrt{2}-1\right)r \simeq 0,414r$$}




\Meth{Population d'une maille}{Une entité peut appartenir à plusieurs mailles, pour déterminer combien d'entités d'un type appartiennent à une maille, on les compte en pondérant selon ces règles :\begin{itemize}
\item $\frac{1}{8}$ pour une entité sur un sommet de la maille (participe à huit autres mailles)
\item $\frac{1}{4}$ pour une entité sur une arête
\item $\frac{1}{2}$ pour une entité sur une face
\item $1$ pour une entité complètement à l'intérieur
\end{itemize}}

\Meth{Rayons des sphères à partir des paramètres de maille}{Il faut déterminer la direction selon laquelle le contact entre sphère se fait, et en déduire la relation entre le rayon et le paramètre de maille $a$
\par Pour la maille CFC : On a des sphères dans tous les coins de la maille, et une sphère au milieu de la face. Les sphères se touchent toutes.
\par Le contact se fait sur la diagonale de la face, or la diagonale d'un carré de côté $a$ est de longueur $a\sqrt{2}$, on en déduit $4r=a\sqrt{2}$}

\Meth{Compacité d'une structure}{Déterminer :\begin{itemize}
\item $N$ la population de la maille
\item $V$ le volume de la maille
\item $r$ le rayon des sphères
\end{itemize}
La compacité dans le modèle des sphères dures est $C = \frac{N(\frac{4}{3}\pi r^3)}{V}$}

\Meth{Masse volumique d'une structure}{Déterminer :\begin{itemize}
\item $N$ le nombre d'entités par maille
\item $V$ le volume de la maille
\end{itemize}
La masse volumique est $\rho = \frac{m_{maille}}{V_{maille}}=\frac{NM}{\mathcal{N}_AV}$}


\subsection{Oxydo-réduction}
\Def{}{Oxydant : peut capter électron
\par Réducteur : peut céder électron
\par Couple Ox/Red lié par la demi-équation électronique : $Ox + ne^- +xH^+= Red + yH_2O$
\par Nombre d'oxydation NO : déficit ou gain en électrons d'un élément (pour un couple, le réducteur a le NO le plus bas).}
\Meth{Déterminer le NO}{Si monoatomique, égal à la charge de l'édifice.
\par Sinon, on fait un schéma de Lewis (en notant bien où les charges finales sont localisées). Ensuite, on compte le nombre $n_e$ d'électrons autour de l'atome dont on veut le NO (en considérant que dans une liaison, l'atome le plus électronégatif attire les électrons). Le NO sera $Z-n_e$}
\Def{Pile}{Avec deux couples Ox/Red, on les met dans des bécher avec une électrode de métal dedans, on appelle ça des électrodes. On relie les béchers par un pont salin (ce qui permet la réaction d'oxydo-réduction, oxydation d'un élément et réduction de l'autre) et les lames de métal par un conducteur de courant.
\par L'électrode où se produit l'oxydation est l'anode (c'est là qu'un réactif reçoit des électrons, devient plus négatif et ion négatif = anion)
\par L'électrode où se produit la réduction est la cathode (c'est là qu'un réactif cède des électrons, devient plus positif et ion positif = cation)
\par La réduction cède des électrons : le courant va de la cathode vers l'anode.
\par On appelle $\delta q$ la charge élémentaire débitée par une pile.
\par On a alors $\delta q = idt = zFd\xi$
\par $F = 96485 C.mol^{-1}$ est la constante de Faraday
\par $z$ est le nombre d'électrons dans l'équation d'oxydo-réduction (ils s'annulent des deux côtés)}
\Thr{Nernst}{$E = E^0(Ox/Red) + \frac{RT}{nF}\ln \frac{a_{Ox}a_{H^+}^x}{a_{Red}a_{H_2O}^y}$
\par $E$ est le potentiel d'un couple Ox/Red particulier.
\par $E_0(Ox/Red)$ est une constante liée à un couple, le potentiel standard. Plus il est grand, plus on dit que son oxydant est fort, plus il est petit, plus on dit que le réducteur est fort. La réaction prépondérante est entre l'oxydant le plus ofrt et le réducteur le plus fort, et se déroule jusqu'à la consommation du réactif limitant ou jusqu'à l'état où les potentiels des deux couples sont égaux.
\par $n$ est le nombre d'électrons impliqués.
\par On a $\frac{RT}{F}\ln(10)\simeq 0,06 V$, ce qu'on utilise en pratique.
\par La constante d'équilibre d'une réaction d'oxydo-réduction est $K^0 = 10^{\frac{n(E^0(Ox1/Red1)-E^0(Ox2/Red2))}{0,06}}$, où Ox1 est réduit et Red2 est oxydé. $n$ est le nombre d'électrons impliqués.}


\subsection{Acides et bases}
Pour toute grandeur $X$ de cette section, on peut ajouter $p$ devant afin d'obtenir la grandeur $pX =-\log X$
\Def{}{Acide : peut céder $H^+$ (proton)
\par Base : peut capter $H^+$
\par Couple acide/base $HA/A^-$
\par Ampholyte ou amphotère : base d'un couple et acide d'un autre.
\par Constante d'acidité : $K_a = \frac{[A^-][H_3O^+]}{[HA]c_0}$ (constante de basicité : $K_b = K_e/K_a$)
\par Un acide/une base est fort.e si sa réaction avec l'eau est totale (resp $K_a$ haut, $K_b$ haut)}
\Def{Réaction}{Soient $HA_1/ A_1^-$ et $HA_2/A_2^-$ deux couples acide/base, le proton qui passe du premier couple à l'autre est $HA_1+A_2^-=HA_2+A_1^-$
\par La constante d'équilibre s'écrit $K = K_{a_1}/K_{a_2}$
}
\Thr{Autoprotolyse}{La réaction $2H_2O=HO^-+H_3O^+$ est l'autoprotolyse de l'eau
\par Sa constante d'équilibre est $K_e=\frac{[H_3O+][HO^-]}{(c_0)^2}$ (où $c_0 = 1 mol.L^{-1}$)
\par On a $pH = -\log\frac{[H_3O^+]}{c_0}$
\par Dans une réaction avec un seul couple $HA/A^-$, on a $pH =pK_a + \log\frac{[A^-]}{[HA]}$ 
}
\Meth{Déterminer la réaction prépondérante}{\begin{enumerate}
\item Déterminer les espèces dans le mélange après la réaction totale des acides et bases fortes avec l'eau
\item La réaction prépondérante est celle de l'acide le plus fort avec la base la plus faible (règle du Gamma)
\item écrire l'équation de la RP
\item Tableau d'avancement
\item Conclusion
\end{enumerate}}

\subsection{Précipités}
\Def{}{Un précipité est un solide composé de deux ions. On le note $C_xA_y$, où $C_x$ est le cation (positif) et $A_y$ l'anion (négatif).
\par On parle de couple donneur/accepteur d'anion pour $C_xA_y/C_x$.
\par Une réaction de dissolution est $C_xA_y = xC^{y+} + yA^{x-}$
\par Une réaction de précipitation est $xC^{y+}+yA^{x-} = C_xA_y$}
Comme pour les acides, un cation qui peut être dissous une nouvelle fois est appelé un amphotère.
\Def{}{La constante de solubilité $K_S$ est la constante d'équilibre thermodynamique de la réaction de dissolution, soit $K_s = \frac{[C^{y+}]^x[A^{x-}]^y}{c_0^{x+y}}$ à l'équilibre.
\par On peut aussi utiliser le quotient de réaction $Q_r$, en remplaçant $K^0$ par $K_S$
\par (Rappel :si $Q_r<K^0(T)$ (resp. $Q_r=K^0(T)$, $Q_r>K^0(T)$), le système évolue dans le sens direct de l'équation de réaction (resp. est à l'équilibre, évolue dans le sens indirect).) 
\par On note $pC = -\log\frac{[C^{y+}]}{c_0}$ et $pA = -\log\frac{[A^{x-}]}{c_0}$
\par $s$ la solubilité est la quantité maximale de matière qu'on peut dissoudre dans un litre de solution (on la calcule avec un tableau d'avancement et $K_S$)}
\Meth{Utiliser la condition de précipitation}{On calcule d'abord le quotient de réaction $Q_r$, puis on le compare à $K_S$. Si $Q_r>K_S$, la solution est saturée et le précipité est formé.}
\Meth{Diagramme d'existence}{On écrit l'équation de dissolution, la frontière du domaine d'existence du précipité est le point où le précipité apparaît. L'espèce soluble est alors de concentration initiale $x_0$}





\chapter{Physique - spé}
\section{Mécanique}
\subsection{Référentiels non-galiléens}
\Def{Référentiels}{Dans cette partie, on se munira de deux référentiels.
\par $\mathcal{R}$ le référentiel de référence
\par $\mathcal{R}'$ le référentiel en mouvement par rapport au premier.
\par Tout mouvement dans $\mathcal{R}'$ sera appelé mouvement relatif.
\par On dit que $\mathcal{R}'$ est en rotation uniforme autour d'un axe fixe de $\mathcal{R}$ si les axes de $\mathcal{R}'$ sont en rotation à vitesse angulaire constante $\omega$ par rapport à un axe immobile dans $\mathcal{R}$.}
\Thr{Composition des vitesses}{Si $\mathcal{R}'$ est en rotation autour de $\mathcal{R}$ :
\par $$\left.\dfrac{d\Vec{A}}{dt}\right|_\mathcal{R} = \left.\dfrac{d\Vec{A}}{dt}\right|_{\mathcal{R}'}+\Vec{\omega}(\mathcal{R}'/\mathcal{R})\wedge\Vec{A}$$
\par Si $\mathcal{R}'$ est en translation autour de $\mathcal{R}$ :
\par $$\left.\dfrac{d\Vec{OA}}{dt}\right|_\mathcal{R} = \left.\dfrac{d\Vec{O'A}}{dt}\right|_{\mathcal{R}'}+\dfrac{d\vec{OO'}}{dt}$$
\par Dans les deux cas, on notera $\Vec{v_a} = \Vec{v_r}+\Vec{v_e}$, où $v_a$ est la vitesse absolue de l'objet, $v_r$ sa vitesse relative dans l'autre référentiel et $v_e$ la vitesse d'entraînement (la vitesse d'un point fixe de l'autre référentiel)}
\Thr{Comporision des accélérations}{Si $\mathcal{R}'$ est en translation autour de $\mathcal{R}$ :
\par $$\left.\dfrac{d^2\Vec{OA}}{dt^2}\right|_\mathcal{R} = \left.\dfrac{d^2\Vec{O'A}}{dt^2}\right|_{\mathcal{R}'}+\dfrac{d^2\vec{OO'}}{dt^2}$$
\par On notera $\Vec{a_a} = \Vec{a_r}+\Vec{a_e}$, où $a_a$ est l'accélération absolue de l'objet, $a_r$ son accélération relative dans l'autre référentiel et $a_e$ l'accélération d'entraînement (accélération d'un point fixe de l'autre référentiel)
\par Si $\mathcal{R}'$ est en rotation autour de $\mathcal{R}$ :
\par $$\Vec{a(M)}_{\mathcal{R}} = \Vec{a(M)}_{\mathcal{R}'}+\Vec{\omega}\wedge(\Vec{\omega}\wedge\Vec{OM})+2\Vec{\omega}\wedge \Vec{v(M)}_{\mathcal{R}'}$$
\par Ce qu'on note aussi $\Vec{a_a}=\Vec{a_r}+\Vec{a_e}+\Vec{a_c}$, où $\Vec{a_c}$ est l'accélération de Coriolis, qui n'apparaît que dans les référentiels tournant.}
\Thr{Forces d'inertie}{Comme on l'a vu, il y a des accélérations d'entraînement dans les référentiels non-galiléens. Pour simplifier, on décide juste de les modéliser comme une force, c'est la force d'inertie d'entraînement $\Vec{f_{ie}}$
\par On a, pour une translation :
\par $$\Vec{f_{ie}} = -m\Vec{a(O')}_\mathcal{R}$$
\par Et pour une rotation :
\par $$\Vec{f_{ie}} = +m\omega^2\Vec{HM}$$
\par Où $H$ est le projeté orthogonal du point $M$ sur l'axe de rotation.}
\Thr{Force de Coriolis}{De même manière, on modélise l'accélération de Coriolis comme une force, la force d'inertie de Coriolis $\Vec{f_{ic}}$ :
\par $$\Vec{f_{ic}} = -2m\Vec{\omega}\wedge\Vec{v(M)}_{\mathcal{R}'}$$
\par Cette force est perpendiculaire à la trajectoire, donc sa puissance et son travail sont toujours nuls.}
\Thr{TEC non-galiléen}{Comme l'inertie de Coriolis a un travail nul, on peut écrire :
\par $$\Delta E_c = W^{ext}_{A\to B} + W^{ie}_{A\to B}$$}

\subsection{Frottement solide}
\Def{Force de réaction}{Tout point matériel $M$ en contact avec un support subit une réaction de ce support, qui se décompose en deux composantes :
\par $$\Vec{R} = \Vec{N}+\Vec{T}$$
\par $\Vec{N}$ est normale au support, dirigée vers l'extérieur (empêche la pénétration). Tant qu'il y a contact, elle existe.
\par $\Vec{T}$ est tangentielle au support, dirigée en contresens du mouvement.}
\Thr{Coefficient de frottement}{Il existe un coefficient sans dimension $f$, appelé coefficient de frottement, tel que :
\par Si $M$ est immobile : $\Vert\Vec{T}\Vert\leq f\Vert\Vec{N}\Vert$
\par Si $M$ est en mouvement : $\Vert\Vec{T}\Vert=f\Vert\Vec{N}\Vert$
\par En réalité, ces deux coefficients ne sont pas égaux, il y a un coefficient statique $f_s$ et un coefficient dynamique $f_d$ avec $f_s\leq f_d$, mais on ignore la différence le plus souvent.}
\Thr{Aspects énergétiques du frottement}{Le frottement est une force non-conservative résistance.
\par Si $M$ est en mouvement : $\mathcal{P}(\Vec{R}) = \mathcal{P}(\Vec{T}) = -\Vert\Vec{T}\Vert\Vert\Vec{v}\Vert$
\par Sinon : $\mathcal{P}(\Vec{R}) = 0$}


\section{Mécanique quantique}
\subsection{Einstein De Broglie vont dans un bar parce que De Broglie a vraiment un nom de merde}
\Thr{Planck-Einstein et De Broglie}{Les relations de Planck-Einstein pour une onde électromagnétique :
\par $$E = h\nu = \hbar\omega$$
\par $$\Vec{p} = \hbar\Vec{k} = \frac{h}{\lambda_0}\Vec{u}$$
\par Les relations de De Broglie pour la matière :
\par $$\nu_{DB} = \frac{E}{h}\text{ ou }\omega_{DB} = \frac{E}{\hbar}$$
\par $$\lambda_{DB} = \frac{h}{p}$$}
$p$ est l'impulsion, $mv$ pour de la matière.

\subsection{Les bases de la fonction d'onde}
\Def{Fonction d'onde}{La fonction d'onde $\Psi$ est une fonction telle que, avec $P$ la probabilité de présence de la particule :
\par $$\frac{dP}{d\tau} = \vert\Psi\vert^2$$
\par On la considérera dans la suite comme une fonction de $x$ et de $t$ pour simplifier les calculs. On peut alors écrire la condition de normalisation :
\par $$\int_{-\infty}^{+\infty}\vert\Psi(x,t)\vert^2dx =1$$}
\Thr{L'équation de Schrödinger}{Si la particule quantique (quanton) est soumis à un potentiel $V$, l'équation d'onde est solution de l'équation :
\par $$\frac{-\hbar^2}{2m}\dfrac{\partial^2\Psi}{\partial x^2}(x,t)+V(x)\Psi(x,t)=i\hbar\dfrac{\partial\Psi}{\partial t}$$}
\Thr{Solution stationnaire}{Si la fonction d'onde est stationnaire, c'est à dire que son énergie ne dépend pas du temps, on peut l'écrire sous la forme :
\par $$\Psi(x,t) = \varphi(x)f(t)$$
\par On peut réinjecter dans l'équation de Schrödinger pour obtenir les équations de Schrödinger indépendantes de l'espace...
\par $$i\hbar\dfrac{\partial f}{\partial t} = Ef(t)$$
\par ... et du temps.
\par $$\frac{-\hbar^2}{2m}\dfrac{\partial^2\varphi}{\partial x^2} + V(x)\varphi(x)=E\varphi(x)$$
\par Où $E$ est l'énergie de la particule, une constante définie comme :
\par $$E = \frac{\hbar^2}{2m}\frac{1}{\varphi(x)}\dfrac{\partial^2\varphi}{\partial x^2} +V(x) = i\hbar\frac{1}{f(t)}\dfrac{\partial f}{\partial t}$$}
On peut résoudre l'équation indépendante de l'espace facilement : $f(t) = f_0e^{-i\frac{E}{\hbar}t} = f_0e^{i\omega_{DB}t}$
\par Dans ce cas, $\vert\Psi(x,t)\vert=\Vert\varphi(x)\Vert$, c'est bien un état stationnaire vu que les propriétés ne dépendent pas du temps. Si la résolution était si facile, c'est bien qu'elle servait à rien...
\Thr{Indétermination d'Heisenberg}{Si $\Delta x$ est l'indétermination sur la position et $\Delta p_x$ l'indétermination sur l'impulsion dans la direction $x$, on a :
\par $$\Delta p_x\Delta x\geq\frac{\hbar}{2}$$
\par Ce qu'on notera souvent en ordre de grandeur :
\par $$\Delta p_x\Delta x\gtrsim \hbar$$}
\Def{Vecteur densité de probabilité}{Pour une fonction d'onde $\Psi$, on notera le vecteur densité de courant de probabilité :
\par $$\Vec{J(M)} = \vert\Psi(x,t)\vert^2\frac{\Vec{p}}{m} = \vert\Psi(x,t)\vert^2\frac{\hbar\Vec{k}}{m}$$}
\Thr{Utilité des ondes stationnaires}{Les solutions stationnaires de Schrödinger constituent une base des solutions de l'équation. Les solutions sont donc des sommes de solutions stationnaires.
\par Pour être exact, les solutions générales sont des transformées de Fourier des solutions stationnaires, donc des sommations continues. Mais on ne sait pas manipuler des transformées de Fourier en prépa. Et si on sait, on doit pas le dire.}

\subsection{Puits infini}
\Def{Présentation du problème}{On est dans une cavité unidimensionnelle selon x de longueur L, telle que pour tout point en-dehors du segment, le potentiel appliqué à la particule soit infini ; elle ne peut donc pas sortir de la cavité. Le potentiel dans la cavité est nul.
\par On décompose $\Psi$ en une fonction d'onde stationnaire $\varphi(x)f(t)$.
\par On a donc à résoudre, dans la cavité : $\frac{-\hbar^2}{2m}\dfrac{\partial^2\varphi}{\partial x^2} +0\varphi(x) = E\varphi(x)$
\par De plus, on a que $\varphi(0) = 0$ et $\varphi(L)=0$ par continuité de la fonction d'onde.}
\Thr{Solutions}{Les solutions sont donc, dans la cavité, de la forme :
\par $$\Psi(x,t)=\sqrt{\frac{2}{L}}\sin\left(\frac{n\pi}{L}x\right)e^{-i\frac{E}{\hbar}t}$$
\par On associe à cet état les énergies $E_n = \frac{n^2\pi^2\hbar^2}{2m L^2}$, qui se retrouvent avec $k_n = \sqrt{\frac{2mE_n}{\hbar^2}}= \frac{n\pi}{L}$}

\subsection{Marche de potentiel}
\Def{Présentation du problème}{Une particule "arrive" de moins l'infini selon Ox et en $0$, est soumise à un potentiel $V_0$.
\par On peut immédiatement écrire qu'avant la marche de potentiel, la solution stationnaire est de la forme :
\par $$\Psi(x,t) = Ae^{i(kx-\omega t)}+Be^{i(-kx-\omega t)}$$
\par On a la continuité de $\Psi$ et de sa dérivée en $0$.}
\Thr{Solution si l'énergie surpasse le potentiel}{On obtient pour $x\leq 0$ :
\par $$\varphi(x) =A(e^{ikx}+re^{-ikx})$$
\par Pour $x\geq 0$ :
\par $$\varphi(x) = Ate^{ik'x}$$
\par Où $k = \sqrt{\frac{2mE}{\hbar^2}}$ et $k' =\sqrt{\frac{2m(E-V_0)}{\hbar^2}}$
\par Et $r = \frac{k-k'}{k+k'}$ et $t=\frac{2k'}{k+k'}$
\par On définit alors $\Vec{J_i},\Vec{J_r}, \Vec{J_t}$ les vecteurs densité de courant de probabilité associés aux ondes incidentes, réfléchies et transmises.
\par On pose $R = \frac{\Vert\Vec{J_r}\Vert}{\Vert\Vec{J_i}\Vert}$ et $T=\frac{\Vert\Vec{J_t}\vert}{\Vert\Vec{J_i}\Vert}$ les coefficients de réflexion et de transmission.
\par On a donc $R+T=1$.}
\Thr{Solution avec énergie moins forte que le potentiel}{On aura une onde évanescente : elle se propagera sur une petite distance, puis diparaitra et sera entièrement réfléchie.
\par En $x\geq 0$ on aura quelque chose de la forme :
\par $$\varphi(x)=Ge^{-\mu x}$$
\par Avec $\mu=\sqrt{\frac{2m(V_0-E)}{\hbar^2}}$.
\par On peut poser les mêmes $r$ et $t$ que la dernière fois, mais ils valent désormais :
\par $$r=\frac{k-i\mu}{k+i\mu}\text{ et }t=\frac{2k}{k+i\mu}$$
\par On peut alors trouver $T=0$.
\par Et donc en $x\geq 0$, on a une fonction d'onde qui décroit en module à la manière d'une exponentielle. C'est l'onde évanescente.
\par On pose $\delta = \frac{1}{\mu}$ la "profondeur caractéristique de pénétration" de l'onde.}

\subsection{Histoire}
De tous les chapitres de physique, c'est le seul à demander des connaissances sur son histoire dans le programme... Il est donc important de se souvenir de quelques dates. Je propose celles-ci :
\par 1906 : l'effet photoélectrique est découvert par Einstein, ce qui commence la dualité onde-corpuscule.
\par 1911 : congrès de Solvay, sur la théorie du rayonnement et des quanta
\par 1923 : De Broglie affirme que la matière présente la même dualité onde-corpuscule que la lumière.
\par 1924 : Pauli propose le principe d'exclusion.
\par 1926 : Schrödinger propose "l'équation de la dynamique de la fonction d'onde".
\par 1927 : Heisenberg propose les relations d'indétermination posant une limite aux interprétations classiques.
\par 1927 : congrès de Solvay, sur les électrons et les photons.
\par 1950 : Kastler et Brossel inventent une technique appelée le pompage optique et depuis que j'ai lu ces mots je fais des cauchemars.
\par 1980-1982 : Alain Aspect met en place des expériences avec des photons et détrui les inégalités de Bell.



\newpage
\section{Electromagnétique}
\subsection{Champ électrostatique}
\Thr{Théorème de Gauss}{Le flux du champ électrostatique sortant d'une surface fermée est relié à la charge contenue à l'intérieur de cette surface par :
\par $$\Phi = \oiint_S\Vec{E}d\Vec{S} = \frac{Q_{int}}{\varepsilon_0}$$}
Méthode d'étude du champ électrostatique :\begin{enumerate}
\item Déterminer les invariances de la distribution de charge, on en déduit le système de coordonnées adapté et le nombre de variables dont il dépend.
\item Déterminer les symétries de la distribution de charges : on trouve tous les plans de symétrie $\Pi$ de la distribution passant par $M$, alors $\Vec{E(M)}\in \Pi$, ou on peut trouver les plans d'antisymétrie $\Pi^*$, dans ce cas $\Vec{E(M)}\perp\Pi^*$
\item Choisir une surface de Gauss passant par un point $M$ adapté aux symétries du système ($d\Vec{S}\perp$ ou $//$ à $\Vec{E(M)}$)
\item Calculer la charge intérieure dans la surface de Gauss
\item Exprimer le flux du champ au travers de la surface
\item Appliquer le théorème de Gauss, et déduire l'expression du champ.
\end{enumerate}
\Thr{Théorème de Gauss gravitationnel}{Le flux du champ de gravité sortant d'une surface fermée est relié à la masse intérieure à cette surface par :
\par $$\Phi_G = \oiint_S\Vec{G}.d\Vec{S} = -4\pi\mathcal{G}M_{int}$$}

\subsection{Potentiel électrostatique}
\Def{}{Le potentiel électrostatique est défini à partir du champ $\Vec{E}$ par :
\par $$\Vec{E} = -\Grad(V)$$
\par On dit que $\Vec{E}$ dérive du potentiel $V$. $V$ est en Volt}
\Thr{}{On a l'équation de Poisson :
\par $$\Delta V = -\frac{\rho}{\varepsilon_0}$$
\par Et dans une région vide de charge on a l'équation de Laplace :
\par $$\Delta V =0$$}

\subsection{Magnétostatique}
La magnétostatique résulte des effets créés par des particules chargées en mouvement.
\Def{Courant}{Avec $n$ la densité de charges de charge $q$, on note avec $\Vec{v}$ la vitesse des charges lr vecteur densité volumique de courant :
\par $$\Vec{j} = nq\Vec{v}$$
\par Alors l'intensité du courant électrique est :
\par $$i = \iint_S\Vec{j}d\Vec{S}$$}
\Thr{Théorème d'Ampère}{La circulation du champ magnétostatique le long d'un contour fermé $C$ est reliée à l'intensité traversant une surface s'appuyant sur ce contour (appelée intensité enlacée) par :
\par $$\oint_C\Vec{B}d\Vec{l} =\mu_0I_{enl}$$}
On change la méthode sur plusieurs points :\begin{itemize}
\item Si on trouve un plan de symétrie, alors $\Vec{B}$ est orthogonale à celui-ci.
\item Si on trouve un plan d'antisymétrie, alors $\Vec{B}$ y appartient.
\item On choisit un contour d'Ampère, qui est un contour qui entoure une surface. On ne calculera que le courant dans cette surface.
\end{itemize}

\subsection{Distribution dipolaires}
\Def{}{On appelle moment dipôlaire électrique d'une distribution de deux charges ($-q$ en $N$ et $+q$ en $P$) le vecteur :
\par $$\Vec{p} = q\Vec{NP}$$
\par La résultante des forces électriques agissant sur un dipôle électrique placé dans un champ extérieur uniforme est nulle.
\par Un dipôle électrique placé dans un champ $\Vec{E_0}$ uniforme subit un couple de forces de moment :
\par $$\Vec{\Gamma} = \Vec{P}\wedge\Vec{E_0}$$
\par L'énergie potentielle d'interaction entre un dipôle électrique et un champ $\Vec{E}$ extérieur s'exprime comme :
\par $$E_p =-\Vec{p}\cdot\Vec{E}$$}
\Def{}{On appelle moment dipolaire magnétique d'une spire de courant circulaire de surface $S$ parcourue par un courant d'intensité $I$ le vecteur :
\par $$\Vec{m} = I\Vec{S}$$
\par La résultante des forces est nulle.
\par Le moment résultant est :
\par $$\Vec{\Gamma} = \Vec{m}\wedge\Vec{B}$$
\par L'énergie potentielle d'interaction s'écrit :
\par $$E_p = -\Vec{m}\cdot\Vec{B}$$}
\Thr{}{Le potentiel créé à grande distance par une distribution dipolaire s'écrit sous la forme :
\par $$V(M) = \frac{\Vec{p}\Vec{r}}{4\pi\varepsilon_0r^3} =\frac{p\cos\theta}{4\pi\varepsilon_0r^2}$$}
Démonstration (à savoir) :
\par Symétries et invariances : à cause de la séparation de charge entre N chargé $-q$ et P chargé $+q$, on a invariance par rotation d'angle $\varphi$ autour de l'axe du dipôle. Donc $V(M)=V(r,\theta)$ en coordonnées sphériques.
\par On a comme symétries de $D$ : tout plan contenant N et P est plan de symétrie. Le plan $(M,\vec{e}_r, \vec{e}_\theta)$ est plan de symétrie donc $\vec{E}\in (O, \vec{e}_r,\vec{e}_\theta)$. Le plan médian qui passe par O est un plan d'antisymétrie, auquel le champ est toujours orthogonal.
\par Expression du potentiel : par superposition, on aura :
\par $V(r,\theta) = V_+(r,\theta)+V_-(r,\theta)=\frac{q}{4\pi\varepsilon_0PM} + \frac{-q}{4\pi\varepsilon_0NM}$
\par On a $PM=\sqrt{PM^2}=\sqrt{\vec{PM}\cdot\vec{PM}}=\sqrt{(\vec{PO}+\vec{OM})(\vec{PO}+\vec{OM})}$ \par $= \sqrt{r^2-ar\vec{e}_z\vec{e}_r+\left(\frac{a}{2}\right)^2} = r\sqrt{1-\frac{a}{r}\cos\theta + \left(\frac{a}{2r}\right)^2}$
\par Et on a $NM= r\sqrt{1+\frac{a}{r}\cos\theta + \left(\frac{a}{2r}\right)^2}$
\par D'où $V(r,\theta)=\dfrac{q}{4\pi\varepsilon_0 r\sqrt{1-\frac{a}{r}\cos\theta + \left(\frac{a}{2r}\right)^2}}+\dfrac{-q}{4\pi\varepsilon_0 r\sqrt{1+\frac{a}{r}\cos\theta + \left(\frac{a}{2r}\right)^2}}$
\par Donc $V(r,\theta)\simeq\frac{q}{4\pi\varepsilon_0r}\left[\left(1+\frac{a}{2r}\cos\theta\right)-\left(1-\frac{a}{2r}\cos\theta\right)\right]$
\par $V(r,\theta)=\frac{q}{4\pi\varepsilon_0 r}\left[2\frac{a}{2r}\cos\theta\right]= \frac{p}{4\pi\varepsilon_0r^2}$
\par En trichant un peu, on peut réécrire de manière intrinsèque : $V(r,\theta)=\frac{\vec{p}\cdot\vec{r}}{4\pi\varepsilon_0r^3}$

\subsection{Les équations de Maxwell}
\Thr{Conservation de la charge}{En un point $M$ de l'espace, portant une densité volumique de charge $\rho(M,t)$ quelconque et une densité de courant $\Vec{j}(M,t)$ quelconque, l'équation de conservation de la charge implique la relation :
\par $$\dfrac{\partial \rho}{\partial t}+\Div(\Vec{j}(M,t))=0$$}
\Thr{Maxwell-Gauss}{$$\Div\Vec{E} = \dfrac{\rho}{\varepsilon_0}$$}
\Thr{Maxwell-Faraday}{$$\Rot\Vec{E} = \dfrac{\partial\Vec{B}}{\partial t}$$}
\Thr{Maxwell-Thomson}{$$\Div\Vec{B} = 0$$}
\Thr{Maxwell-Ampère}{$$\Rot\Vec{B} = \mu_0\Vec{j} +\mu_0\varepsilon_0\dfrac{\partial\Vec{E}}{\partial t}$$}
\Def{ARQS}{L'approximation des régimes quasi-stationnaires arrive quand :
\par On peut négliger le temps de retard $\tau$ à la propagation du signal dans le circuit par rapport au temps caractéristique de variation des signaux $T_{signal}$.
\par Avec un circuit de longueur $L$, on a : $\tau= \frac{L}{c}$ (enfin, $\frac{2}{3}c$ plus rigoureusement.)
\par On veut donc pouvoir écrire :
\par $$\frac{L}{C}\ll T_{signal}=\frac{1}{f_{signal}}\Leftrightarrow f_{signal}\ll \frac{c}{L}$$
\par ou :
\par $$L\ll \lambda_{signal}=\frac{c}{f_{signal}}$$}

\subsection{Aspects énergétiques}
\Thr{L'équation locale de Poynting}{
$$\Div\left(\frac{\Vec{E}\wedge\Vec{B}}{\mu_0}\right) + \dfrac{\partial}{\partial t}\left[\frac{\varepsilon_0}{2}\Vec{E}^2+\frac{1}{2\mu_0}\Vec{B}^2\right]=-\Vec{j}\cdot\Vec{E}$$
\par On note $\Vec{\Pi} = \frac{\Vec{E}\wedge\Vec{B}}{\mu_0}$ le vecteur de Poynting, qui représente le rayonnement d'un champ électromagnétique au travers d'une surface : $\iint_S \Vec{\Pi}d\Vec{S} =\mathcal{P}_{rayonnee}$.
\par On note $u_{e} = \frac{\varepsilon_0}{2}E^2$, $u_{m} = \frac{1}{2\mu_0}B^2$ et $u_{em} =u_e+u_m$ l'énergie électromagnétique du champ.
\par $\Vec{j}\cdot\Vec{E}$ représente la puissance cédée par le champ électrique aux charges : $\Vec{j}\cdot\Vec{E} = \dfrac{d\mathcal{P}}{d\tau}$}
\Def{Loi d'Ohm}{Dans un milieu dit conducteur, on a la relation :
\par $$\Vec{j} = \gamma\Vec{E}$$
\par Avec $\gamma$ la conductivité du matériau.}

\subsection{Propagation dans le vide}
\Def{Vide}{Le vide est une zone de l'espace vide de charge et de courant. On peut donc réécrire les équations de Maxwell :
\par $$\Div\Vec{E} = 0$$
\par $$\Rot(\Vec{E}) = -\dfrac{\partial \Vec{B}}{\partial t}$$
\par $$\Div\Vec{B} = 0$$
\par $$\Rot(\Vec{B}) = -\mu_0\varepsilon_0\dfrac{\partial \Vec{E}}{\partial t}$$
\par On se servira ensuite de $\Rot(\Rot(\Vec{A})) = \Grad(\Div\Vec{A}) -\Delta\Vec{A}$ pour un champ $\Vec{A}$ quelconque et de ces équations pour obtenir l'équation de propagation dans le vide.}
\Thr{L'équation de propagation dans le vide}{$$\Delta\Vec{E} -\frac{1}{c^2}\dfrac{\partial^2\Vec{E}}{\partial t^2}=\Vec{0}$$
\par $$\Delta\Vec{B} -\frac{1}{c^2}\dfrac{\partial^2\Vec{B}}{\partial t^2}=\Vec{0}$$}
\Def{Onde}{Une onde est un champ (scalaire ou vectoriel) solution d'une équation de propagation. De manière plus qualitative, c'est un phénomène de propagation d'une perturbation, sans transport de matière mais avec transport d'énergie.
\par Une one est plane progressive monochromatique si on peut écrire :
\par $$a(x,t)=A_0\cos(\omega t - \Vec{k}\cdot\Vec{r}+\varphi_0)$$
\par Où $\Vec{k} = \frac{\omega}{c}\Vec{e_r}$ le vecteur d'onde, dirigé dans le sens de propagation de l'onde. On appelle $\omega = kc$ la relation de dispersion, qui est simple dans le vide mais très compliquée ailleurs.
\par On appelle phase d'une onde la fonction :
\par $$\varphi = \omega t - \Vec{k}\cdot\Vec{r}+\varphi_0$$
\par On notera aussi les OPPM de manière complexe comme : $\underline{\Vec{E}}(\Vec{r},t) = \underline{\Vec{E_0}}e^{i(\omega t -\Vec{k}\cdot\Vec{r}+\varphi_0)}$}
\Thr{}{Dans le vide, $\Vec{E}$ et $\Vec{B}$ sont des OPPM orthogonales : les deux champs forment un trièdre direct avec le vecteur de Poynting.
\par En notation complexe, les opérateurs deviennent des opérations vectorielles :
\par \begin{align*} \Div... &\Leftrightarrow & -i\Vec{k}\cdot...\\
\Rot...&\Leftrightarrow&-i\Vec{k}\wedge...\\
\Delta...&\Leftrightarrow&-k^2
\end{align*}}

\subsection{Propagation dans le plasma}
Tout le chapitre se fera avec des notations complexes.
\Def{Plasma}{Un plasma est un milieu ionisé constitué d'ions de masse $M_i$ et de charge $+n_ie$ et d'électrons de masse $m$ et de charge $-e$.
\par Dedans, on peut négliger le mouvement des ions comparé à celui des électrons créé par la force de Lorentz.
\par Dans un plasma, la loi d'Ohm s'écrit :
\par $$\Vec{j} = \frac{ine^2}{m\omega}\Vec{E}$$
\par Donc la conductivité $\gamma$ est imaginaire pure.}
\Thr{L'équation de propagation dans le plasma}{Dans un plasma froid et dilué :
\par $$\Delta\Vec{E} +\frac{1}{c^2}\dfrac{\partial^2\Vec{E}}{\partial t^2} = \mu_0\gamma\dfrac{\partial\Vec{E}}{\partial t}$$}
\Thr{Relation de dispersion}{En réinjectant la structure d'une OPPM dans l'équation de propagation dans le plasma, on obtient la relation de dispersion :
\par $$k^2 = \frac{\omega^2-\omega_p^2}{c^2}$$
\par Où $\omega_p =\sqrt{\frac{n_0e^2}{m\varepsilon_0}}$.
\par Si $k^2<0$, alors $k\in i\R$ en notant $k = ik'$, et donc l'onde est dite évanescente dans le plasma : son amplitude décroit en $e^{-\Vec{k'}\cdot\Vec{r}}$.
\par Si $k^2>0$, alors il y a propagation dans le plasma, avec $k$ positif.}
\Def{Vitesses}{On modélise une onde comme une transformée de Fourier de l'OPPM obtenue précédemment, donc une superposition continue de fréquences. On appelle ce groupe un paquet d'ondes.
\par La phase de chaque composante OPPM du paquet d'onde se propage à la vitesse dite vitesse de phase :
\par $$v_\varphi = \frac{\omega}{k} = \frac{w}{\Re(k)}$$
\par La vitesse de groupe correspond à la vitesse de propagation de l'enveloppe de l'onde :
\par $$v_g = \frac{d\omega}{dk} =\frac{d\omega}{d\Re(k)}$$
\par On constatera comme en optique géométrique que la vitesse de phase pourra dépasser $c$ mais pas la vitesse de groupe.}

\subsection{Ondes électromagnétiques dans un milieu ohmique}
Dans un conducteur métallique, un certain nombre d'électrons de valence des atomes de métal sont délocalisés dans tout le volume du métal et peuvent être mis en mouvement sous l'action d'un champ électrique, créant ainsi un courant de condution. On a déjà vu que la densité volumique de courant était proportionnelle aux champs électriques dans les matériaux ohmiques.
\par Dans ce cas, on aura deux conséquences :\begin{itemize}
\item Neutralité locale : $$\forall t,\rho(t) = 0$$
\item Courant de déplacement négligeable : $$\Vert\Vec{j_D}\Vert\ll\Vert\Vec{j}\Vert$$
\end{itemize}
\Thr{Les équations de propagation dans le conducteur}{On peut réécrire :
\par $$\Delta \Vec{E} = \mu_0\gamma\dfrac{\partial\Vec{E}}{\partial t}$$
\par $$\Delta\Vec{B} =\mu_0\gamma\dfrac{\partial\Vec{B}}{\partial t}$$}
\Thr{Relation de dispersion}{On obtient dans un conducteur ohmique :
\par $$k^2 = i\mu_0\gamma\omega$$
\par $k$ sera donc complexe, et son amplitude diminuera avec la propagation.
\par On observera que l'onde sera atténuée sur une distance caractéristique $\delta = \sqrt{\frac{2}{\mu_0\gamma\omega}}$.
\par $\delta$ est appelée l'épaisseur de peau. Ce phénomène d'atténuation du champ dans le métal est appelé effet de peau.}

\subsection{Réflexion sur un métal parfait}
\Def{Conducteur parfait}{Un conducteur parfait est un conducteur dont la conductivité tend vers $+\infty$.
\par comme la puissance volumique cédée aux charges doit rester finie, et que $\dfrac{d\mathcal{P}}{d\tau}=\Vec{j}\cdot\Vec{E} =\gamma\Vert\Vec{E}\Vert^2$, alors $\Vec{E}=\Vec{0}$ dans le conducteur parfait.
\par On a alors que $\dfrac{\partial\Vec{B}}{\partial t}=\Vec{0}$ par Faraday, donc que le champ magnétique peut être statique. 
\par De même que sur le conducteur ohmique, on a une densité volumique de charge nulle, mais il peut y avoir une charge surfacique. 
\par Il peut aussi exister des courants variables en surface du métal, avec des courants stationnaires ou nuls à l'intérieur du métal.}
\Thr{Réflexion}{Comme il ne peut pas y avoir de champ à l'intérieur, alors il est nécessairement réfléchi. Pour calculer le champ réfléchi dans le milieu 1, on a donc les relations de passages pour les composantes normales et parallèles aux surfaces métalliques de champs électriques et magnétiques :
\begin{itemize}
\item $$\Vec{E_{//2}} =\Vec{E_{//1}}$$
\item $$\Vec{E_{\perp2}}-\Vec{E_{\perp1}} = \frac{\sigma}{\varepsilon}\Vec{n_{12}}$$
\item $$\Vec{B_{\perp2}}=\Vec{B_{\perp1}}$$
\item $$\Vec{B_{//2}}-\Vec{B_{//1}}=\mu_0\Vec{j_s}\wedge\Vec{n_{12}}$$
\end{itemize}}
\Thr{Expression OPPM}{Si l'onde incidente est de la forme $\Vec{E_i} = E_0\vec{u_y}e^{i(kx-\omega t)}$
\par On a :
\begin{align*}
\Vec{E_r} &= -E_0e^{i(-kx-\omega t)}\\
\Vec{B_i} &= \frac{E_0}{c}e^{i(kx-\omega t)}\Vec{u_z}\\
\Vec{B_r} &= \frac{E_0}{c}e^{i(-kx-\omega t)}\Vec{u_z}
\end{align*}
\par La superposition des ondes électromagnétiques incidente et réléchie en incidence normale sur un conducteur parfait résulte en une onde stationnaire. Le plan de surface du conducteur est un noeud pour le champ électrique et un ventre pour le champ magnétique.}
\Thr{Pression de radiation}{Un conducteur parfait soumis à une onde électromagnétique incidente subit une pression dite pression de radiation :
\par $$p_{rad} = \varepsilon_0E_0^2$$}

\subsection{Rayonnement du dipôle oscillant}
\Def{Dipôle oscillant}{Un dipôle oscillant est dconstitué d'une charge $-q$ fixe en $O$ et d'une charge $+q$ qui oscille selon l'axe $Oz$ autour de $-q$ avec une amplitude $a$ et une pulsation $\omega$
\par On a alors : $\Vec{p}(t) = qa\cos(\omega t)\Vec{e_z}$
\par On a alors trois échelles de longueur caractéristique dans cette étude : la taille $a$ du dipôle, la distance $r$ à laquelle on se trouve et la longueur d'onde $\lambda$ de l'onde électromagnétique qu'elle rayonne. On fera ces approximations dessus :\begin{itemize}
\item Mouvement non-relativiste : $a\ll\lambda$
\item Champs lointains/zone de rayonnement : $\lambda\ll r$
\end{itemize}}
\Thr{Expressions}{Dans le cadre des approximations précédentes, le champ rayonné par le dipôle s'exprime en sphériques comme :
\par $$\Vec{E} = \frac{\mu_0\sin\theta}{4\pi r}\ddot{p}\left(t-\frac{r}{c}\right)\Vec{e_\theta}=-\frac{\mu_0\omega^2\sin\theta}{4\pi r}p_0\cos(\omega t-kr)\Vec{e_\theta}$$
\par $$\Vec{B} = \frac{\mu_0\sin\theta}{4\pi rc}\ddot{p}\left(t-\frac{r}{c}\right)\Vec{e_\varphi}=-\frac{\mu_0\omega^2\sin\theta}{4\pi rc}p_0\cos(\omega t-kr)\Vec{e_\varphi}$$
Ces expressions ne sont pas à apprendre ou à savoir démontrer.
\par L'onde rayonnée par le diôle se propage radialement, à pulsation $\omega$, à la vitesse $c$ dans le vide, selon le vecteur $\frac{\omega}{c}\Vec{e_r}$. Ce n'est ni une onde plane ni une onde sphérique.
\par Elle a localement la structure d'une OPPM, respectant : $\Vec{B} =\frac{\Vec{k}\wedge\Vec{E}}{\omega}$
\par Elle est anisotrope.
\par Son amplitude décroît en $\frac{1}{r}$ comme une onde sphérique.
\par Elle est polarisée rectilignement : $\Vec{E}=E(r,\theta)\Vec{e_\theta}$ et $\Vec{B} = B(r\theta)\Vec{e\varphi}$}
\Thr{Thr}{La puissance moyenne rayonnée par le dipôle oscillant au travers d'une sphère de rayon $r$ dans la zone de rayonnement s'exprime comme :
\par $$\mathcal{P} =\frac{p_0^2\omega^4}{3\times 4\pi\varepsilon_0c^3} = \frac{1}{3}\frac{1}{4\pi\varepsilon_0}\dfrac{\langle\ddot{p}\rangle}{c^3}$$
\par Cette dernière égalité est la formule de Larmor.}







\section{Ondes sur une corde}
Il y a trois domaines où on peut obtenir l'équation de D'Alembert :\begin{itemize}
\item Les ondes électromagnétiques, comme on l'a vu en EM7
\item Les ondes acoustiques mais c'est compliqué
\item Une onde sur une corde, qui n'est pas au programme de MP mais qui tombe très souvent
\end{itemize}
On étudie donc le système d'une corde :
\par À l'équilibre, elle est horizontale ($\forall x,y(x,0) =0$)
\par On l'étudie sur le tronçon $\{x,x+dx\}$ : on étudie donc des petits mouvements autour de la situation d'équilibre ; $\Vec{P}$ n'intervient donc pas. Dans ce tronçon, la pente de la corde est droite.
\par La portion étudiée est de longueur $dx$ et de masse linéique $\mu$
\par Bilan des forces :
\par Le poids : on le néglige.
\par La tension du fil : $\Vec{T}$ la tension exercée en un point $M$ par la droite du fil sur la gauche.
\par On pose : $\begin{cases} \text{En x+dx :}\Vec{T(x+dx,t)} \\\text{En x :} \Vec{T(x,t)}\end{cases}$
\par Avec $\Vert\Vec{T}\Vert(x,t) = T_0 = F$ ($F$ est la force appliquée à l'extrémité de la corde.)
\par On note $\alpha$ l'angle que fait la corde avec l'horizontale.
\par On applique la RFD :
\par $$\mu dx\Vec{a} =\Vec{T}(x+dx,t)-\Vec{T}(x,t)$$
\par On a $\Vec{a} =\dfrac{\partial^2 y}{\partial t^2}\Vec{u_y}$
\par On projette $\Vec{T}$ sur $\vec{u_y}$ :
\par $$\Vec{T}(x,t) = T_0\sin\alpha =F=\sin\alpha$$
\par On réinjecte, et on obtient, en prenant en compte que comme on est dans des petits mouvements :
\par\begin{align*}\mu dx\dfrac{\partial^2y}{\partial t^2} &= F(\sin\alpha(x+dx,t) -\sin\alpha(x,t))\\
&\simeq F(\alpha(x+dx,t)-\alpha(x,t))\\
&=Fdx\dfrac{\partial \alpha}{\partial x}
\end{align*}
\par Or on a que $\tan\alpha = \dfrac{dy}{dx}$
\par D'où, en réutilisant les petits angles :
\par $$\mu\dfrac{\partial^2y}{\partial t^2} = F\dfrac{\partial^2y}{\partial x^2}$$
\par $\frac{F}{\mu}$ est une constante, on peut définir la vitesse de la corde comme $c_{corde} =\sqrt{\frac{F}{\mu}}$
\par On obtient alors :
\par $$\dfrac{\partial^2y}{\partial t} = c_{corde}^2 \dfrac{\partial^2y}{\partial x^2}$$
\par Et donc si $F$ augmente, alors $c$ augmente, et si $\mu$ décroit, alors $c$ augmente. 
\par Si on écrit $y$ comme produit de fonctions à variables séparées et qu'on résout :
\par \begin{align*} y(x,t) &=f(x)g(t)\\
&=y_0\sin(k_nx)\sin(\omega_nt)
\end{align*}
Et on a $k_n = \frac{n\pi}{L}$ et $\omega_n = k_nc = \frac{n\pi c}{L}$ les facteurs qui décrivent les modes propres.


\newpage
\section{Thermodynamique}
\section{TH1}
\Thr{Identités thermodynamiques}{\begin{itemize}
\item $$dU = \delta W + \delta Q = -P_edV +\delta W' +\delta Q$$
\item $$dS = \delta S_e+\delta S_c = \frac{\delta Q}{T_e}+\delta S_c$$
\item $$dU = C_VdT = \frac{nR}{\gamma-1}dT$$
\item $$dH = C_PdT = \frac{nR\gamma}{\gamma-1}dT$$
\end{itemize}}
\Def{Débit massique}{Pour un écoulement d'un fluide, on note $D_m = \frac{dm_{entrant}}{dt} =\frac{dm_{sortant}}{dt}=\frac{dm}{dt}$ en régime stationnaire
\par On a alors $D_m = \mu S v$ avec $\mu$ la masse volumique du fluide, $S$ la section et $v$ la vitesse de l'écoulement.
\par On définit alors le vecteur densité volumique de courant de masse :
\par $$\Vec{j_m} = \mu(M,t)\Vec{v(M,t)}$$
\par On a alors $D_m = \iint_S\Vec{j_m}\Vec{dS}$}
\Thr{Conservation de la masse}{Pour tout écoulement :
\par $$\Div\Vec{j_m(M,t)}+\dfrac{\partial\mu}{\partial t}=0$$
\par Si l'écoulement est stationnaire, ces deux termes sont constants, et $D_m$ aussi.
\par Si l'écoulement est incompressible (masse volumique uniforme), on a $\Div\Vec{j_m}=0$, et $D_v$ le débit volumique constant aussi.}
\Thr{Premier principe en écoulement}{Pour un fluide incompressible en écoulement permanent, on écrit ce bilan d'énergie entre l'entrée et la sortie :
\par $$\Delta h+\Delta e_M=w_u+q(+w')$$
\par Dans cette formulation, on considère des grandeurs massiques, d'où la minuscule.
\par L'énergie mécanique varie peu, donc on n'a souvent pas de terme en $\Delta e_M$}
\Thr{Théorème de Bernouilli}{Pour un fluide incompressible en écoulement permanent dans le champ de pesanteur uniforme :
\par $$\Delta\left(\frac{P}{\mu}+\frac{1}{2}v^2+gz\right)=w_u+w_{visc}$$
\par Dans le cas où le travail massique est nul (pas d'échange de travail avec les parties mobiles de la machine) et où le fluide et parfait, on a la relation de Bernouilli
\par $$\Delta\left(\frac{P}{\mu}+\frac{1}{2}v^2+gz\right)=0$$}






\newpage
\section{Thermochimie}
\Def{Réacteurs}{Le réacteur est l'endroit où se déroulera la réaction. Comme des transformations thermodynamiques, il peut être isotherme, adiabatique ou isobare.
\par Dans la suite du chapitre, on se placera dans le cas d'un réacteur isobare et adiabatique ou isobare et isotherme.}
\Def{Enthalpie standard de réaction}{L'enthalpie du métalnge est donnée par :
\par $$H(T,P, n_1,n_2,..., n_p) = \sum\limits_{i=1}^pn_iH_{im}^0$$
\par Où les $n_i$ sont les concentrations des espèces et $H_{im}^0$ l'enthalpie molaire de l'espèce $i$ à l'état standard.
\par On en déduit l'existence de $\Delta_rH^0$ l'enthalpie standard de réaction telle que :
\par $$\Delta H = \Delta_rH^0\xi$$
\par Si $\Delta_rH^0<0$, la réaction est exothermique, elle produit de la chaleur. Si $\Delta_rH^0>0$, la réaction est endothermique, elle a besoin de chaleur pour se faire.}
\Def{Enthalpie standard de formation}{L'enthalpie standard de formation $\Delta_fH^0$ d'une espèce dans un état donné est l'enthalpie standard de réaction $\Delta_rH^0$ de la réaction standard ce formation à cette température.
\par Et donc si l'espèce est un corps simple dans son état standard de référence, $\Delta_fH^0 =0$}
\Thr{Loi de Hess}{Pour toute réaction écrite sous la forme $\sum\nu_iA_i = 0$ où les $\nu_i$ des produits sont positifs et ceux des réactifs sont négatifs, on a :
\par $$\Delta_rH^0(T) = \sum\limits_i\nu_i\Delta_fH_i^0(T)$$}
\Thr{Approximation d'Ellingham}{On considère qu'en l'absence de changement d'état, l'enthalpie standard est constante selon la température.
\par Au passage d'une phase à l'autre, l'enthalpie subit une discontinuité, et donc pour la réaction $\sum\nu_iA_i=0$, si l'espèce $A_j$ change d'état entre $T_0$ et $T$ on aura :
\par $$\Delta_rH^0(T)=\Delta_rH^0(T_0)+\nu_j\Delta_{r1\to 2}H_j^0$$}
\Def{Entropie standard de réaction}{L'entropie standard de réaction est donnée de la même manière que l'enthalpie standard de formation :
\par $$\Delta_rS^0 = \sum\limits_i\nu_iSm^0_i$$
\par Et on a :
\par $$\Delta_rS^0>0\Leftrightarrow \sum\limits_{\text{i gaz}}\nu_i>0$$}
\Def{Enthalpie libre}{On définit l'enthalpie libre d'une réaction comme :
\par $$G=H-TS$$
\par On définit le potentiel chimique d'un constituant $A_i$ dans un système composé comme :
\par $$\mu_i = \left.\dfrac{\partial G}{\partial n_i}\right\vert_{T,P, n_j\neq n_i}$$
\par On a alors, avec $a_i$ l'activité de $A_i$ :
\par $$\mu_i = \mu_i^0(T)+RT\ln(a_i)$$}
\Thr{Relation entre enthalpie enthalpie libre et entropie}{
\par $$\Delta_rG^0 = \Delta_rH^0 + T\Delta_rS^0$$
\par et :
\par $$\Delta_rG = \Delta_rG^0 +RT\ln(Q_r)$$}
\Def{Interprétation de l'enthalpie libre}{$$\Delta_rGd\xi = -T_e\delta S_c$$
\par Et donc :\begin{itemize}
\item $\Delta_rG<0\Rightarrow d\xi>0\Rightarrow$ réaction dans le sens direct
\item $\Delta_rG>0\Rightarrow d\xi<0\Rightarrow$ réaction dans le sens indirect
\item Si $\Delta_rGd\xi=0$, alors la réaction ne se produit plus.
\end{itemize}
\par On en déduit que :
\par $$\Delta_rG^0 = RT\ln(K^0(t))$$}
\Thr{Relation de Van't Hoff}{On a :
\par $$\dfrac{d(\ln(k^0))}{dT}=\frac{+\Delta_rH^0}{RT^2}$$
\par Ce qui se traduit par :
\par $$K(T_2) =K(T_1)\times e^{\frac{-\Delta_rH^0}{R}\left(\frac{1}{T_2}-\frac{1}{T1}\right)}$$}
\Thr{Loi de Van't Hoff}{Une élévation isobare de température entraîne un déplacement d'équilibre dans le sens endothermique.
\par Une diminution isobare de température entraîne un déplacement d'équilibre dans le sens exothermique.}
\Thr{Loi de Le Chatelier}{Une élévation isotherme de pression entraîne un déplacement d'équilibre dans le sens d'une diminution du nombre total de moles de gaz.
\par Une diminution isotherme de pression entraîne un déplacement d'équilibre dans le sens d'une augmentation du nombre total de mole de gaz.}
\Thr{Principe de modération}{Par leurs effets, les déplacements d'équilibre tendent à s'opposer aux causes qui les ont engendrés.}



\section{Electrochimie}
\subsection{Piles}
Dans la suite, on pose la demi-équation :
\par $$\alpha Ox+ne^-=\beta Red$$
\Def{Vitesse de réaction}{On définit la vitesse de réduction comme :
\par $$v_{red} = \left.\dfrac{d\xi}{dt}\right|_{Red} =\frac{-1}{n}\dfrac{d n(e-)}{dt}=\frac{-1}{n\mathcal{F}}\dfrac{dq}{dt} = \frac{-1}{N\mathcal{F}}i$$
\par L'intensité du courant traversant l'électrode est comptée positivement si l'électrode reçoit des électrons, négativement sinon.}
\Thr{Formule de Nernst}{Pour une oxydo-réduction d'enthalpie libre $\Delta_rG$ avec $n$ électrons échangés, on observe que :
\par $$\Delta E=E_a-E_c =-\frac{\Delta_rG}{n\mathcal{F}}$$
\par En découle :
\par $$E^0 =\frac{-\Delta_rG^0}{n\mathcal{F}}$$
\par Et aussi :
\par $$E = E^0 +\frac{RT}{n\mathcal{F}}\ln\left(\frac{a_{Ox}}{a_{Red}}\right)$$}
\Def{Capacité électrique}{On appelle capacité d'une pile la quantité maximale de charge électrique qu'elle peut fournir.
\par $$Q = n\mathcal{F}\xi_{max}$$}
\Thr{Tension réelle}{La tension réelle aux bornes d'une pile, qui prend en compte les surtensions des électrodes et la résistance au passage du courant de la pile elle même, s'écrit :
\par $$\Delta U = \Delta E_{Nernst}-(\vert\nu_{a0}\vert+\vert\nu_{c0}\vert)-(\vert\nu_a'\vert+\vert\nu_c\vert')-rI$$}

\subsection{L'électrolyse}
\Def{L'électrolyse}{Il y a des réactions d'oxydo-réductions qui ne sont pas thermodynamiquement favorisées ; la règle du gamma dit que la réaction n'est pas possible, le $K^0$ est inférieur à 1, $E_{N1}-E_{N2}<0$, des considérations du genre.
\par On peut quand même la provoquer, en imposant une différence de potentiel entre les électrodes.
\par Dans cette situation, la réduction se produit à la cathode et l'oxydation à l'anode, comme toujours.
\par Il y a besoin de surtensions anodiques et cathodiques pour forcer la réaction, et trouver non pas un potentiel mixte mais deux potentiels avec des courants opposés, une différence de potentiel mixte quoi.
\par On appelle tension seuil d'électrolyse la tension minimale à appliquer pour voir le passage d'un courant :
\par $$\mathcal{U}_{seuil} = \Delta E_{Nernst} + (\vert\nu_{a0}\vert+\vert\nu_{c0}\vert)$$
\par Dans les faits, la tension pour réaliser l'électrolyse est plus grande, et a cette formule :
\par $$\mathcal{U} = \Delta E_{Nernst}+(\vert\nu_{a0}\vert+\vert\nu_{c0}\vert)+(\vert\nu_{a}'\vert+\vert\nu_{c}'\vert)+rI$$
\par Où $I$ est le courant circulant dans la cellule.
\par On définit le rendement faradique le rapport :
\par $$rF = \frac{n_{cree}}{n_{th}}$$
\par Avec $n_{th} =\frac{i}{n\mathcal{F}}\Delta t$, et $n_{cree}$ se mesure.}

\subsection{La corrosion}
\Def{Corrosion}{On appelle corrrosion humide d'un métal $M$ une réaction d'oxydation de celui-ci de cette forme :
\par $$M+Ox =M^{n+}+Red$$
\par Les oxydants de corrosion seront $H^+, H_2O, O_2, Ho^-$ seulon la solution.
\par La corrosion transforme le métal solide en ion dissous, ce qui n'est pas très utile.}
\Def{Les domaines}{Sur un diagramme E-pH, il y a plusieurs zones en lien avec la corrosion :\begin{itemize}
\item La zone d'immunité est le domaine d'existence du métal lui-même, où il n'est pas corrodé
\item La zone de corrosion est le domaine de prédominance des ions issus de l'oxydation
\item La zone de passivation est la zone où il y a des hydroxydes/oxydes issus de l'oxydation du métal ; dans ce cas, ils se déposent sur le métal et font une couche protectrice qui empêche le métal d'être totalement oxydé.
\end{itemize}}
\Def{Pile de corrosion}{Si on met deux électrodes de métal dans la même solution, au lieu de corroder les deux, seul celui avec le plus petit $E^0$ le sera.
\par On peut donc protéger un métal de la corrosion en le mettant en contact avec un métal plus réducteur.}
\Thr{Aération différentielle}{Quand une surface de métal et d'oxydant n'est pas uniforme, on constate que c'est à l'endroit le moins aéré qu'a lieu l'oxydation. C'est une zone anodique, où le métal se fait oxyder.}
\Def{Types de protection}{Pour se protéger de la corrosion, on a plusieurs solutions :\begin{itemize}
\item Recouvrir le métal d'un matériau non-conducteur pour empêcher son oxydation ; mais par aération différentielle, si le vernis part à un endroit alors le métal sera très oxydé.
\item Recouvrir le métal d'un métal résistant mieux à la corrosion, qui a le même défaut que précédemment
\item Protection par passivation : On peut faire subir un traitement oxydant, de manière à ce qu'un oxyde se forme sur la surface du métal. Cette couche a beau protéger le métal, elle est très fragile.
\item Protection cathodique passive : le métal jouera non pas le rôle d'anode mais de cathode pour la réduction de $O_2$, en servant d'électrode pour l'oxydation d'un métal plus réducteur.
\item Protection électrochimique active : le métal jouera encore le rôle de cathode, mais pas juste parce qu'on aura rajouté un métal plus réducteur, c'est aussi en appliquant un courant au métal. 
\end{itemize}}





\chapter{Proba}
\section{Bases de la proba}
\Def{}{Une expérience aléatoire est une expérience renouvelable, et qui renouvelée dans des conditions identiques ne donne pas le même résultat à chaque renouvellement.}
\Def{}{On appelle univers l'ensemble des issues possibles d'une expérience aléatoire donnée. On le note en général $\Omega$}
\Def{}{Soit $\Omega$ l'univers d'une expérience aléatoire
\par On appelle tribu sur $\Omega$ une partie de $\mathcal{A}$ vérifiant les trois hypothèses suivantes :\begin{enumerate}
\item $\Omega\in \mathcal{A}$
\item $\forall A\in \mathcal{A},\overline{A}\in \mathcal{A}$ (stabilité par passage au complémentaire)
\item Pour toute suite $(A_n)_{n\in\N}$ d'éléments de $\mathcal{A}$, $\bigcup_{n=0}^{+\infty}A_n\in\mathcal{A}$ (stabilité par réunion dénombrable)
\end{enumerate}
Le couple $(\Omega, \mathcal{A})$ est dit espace probabilisable.
\par $\mathcal{A}$ est l'ensemble des événements (on rappelle qu'un événement est une partie de $\Omega$)}
\Def{}{Soit $(A_n)_{n\in\N}$ une suite d'événements.\begin{itemize}
\item La suite $(A_n)$ est dite croissante lorsque \par\begin{center}$\forall n\in\N, A_n\subset A_{n+1}$\end{center} \par ie : pour tout $n\in\N$, la réalisation de $A_n$ implique celle de $A_{n+1}$
\item La suite $(A_n)$ est dite décroissante lorsque \par\begin{center}$\forall n\in\N, A_{n+1}\subset A_{n}$\end{center} \par ie : pour tout $n\in\N$, la réalisation de $A_{n+1}$ implique celle de $A_{n}$
\item La suite $(A_n)$ est une suite d'événement deux à deux incompatibles (disjoints) lorsque : \par\begin{center}$\forall i,j\in\N, i\neq j\Rightarrow A_i\cap A_j=\emptyset$\end{center} \par ie : il est impossible que deux événements d'indices différents de la suite soient réalisés simultanément
\end{itemize}}
\Def{}{On appelle probabilité sur l'espace probabilisable $(\Omega,\mathcal{A})$ toute application $P$ définie sur $\mathcal{A}$ vérifiant :\begin{enumerate}
\item $\forall A\in\mathcal{A}, P(A)\in [0,1]$
\item $P(\Omega)=1$
\item Pour tout famille dénombrable $(A_i)_{i\in I}$ d'événements deux-à-deux incompatibles : \par\begin{center}$P\left(\bigcup_{i\in I}A_i\right)=\sum\limits_{i\in I}P(A_i)$ ($\sigma$-additivité)\end{center}
\end{enumerate}}
\Thr{}{Soit $(\Omega,\mathcal{A})$ un espace probabilisé, et soit $A$ et $B$ deux événements. On a :\begin{itemize}
\item $P(\overline{A})=1-P(A)$
\item $P(\emptyset)=0$ (cas particulier du résultat précédent)
\item $P(A\backslash B)=P(A)-P(A\cap B)$
\item $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ (formule de Poincaré)
\item P est croissante : Si $A\subset B$ alors $P(A)\leq P(B)$
\end{itemize}}
\Thr{}{Soit $(\Omega, \mathcal{A},P)$ un espace probabilisé.\begin{enumerate}
\item Pour toute suite croissante $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ : \par\begin{center}$P\left(\bigcup_{n=0}^{+\infty}A_n\right)=\lim\limits_{n\to+\infty}P(A_n)$ (Continuité croissante)\end{center}
\item Pour toute suite décroissante $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ : \par\begin{center}$P\left(\bigcap_{n=0}^{+\infty}A_n\right)=\lim\limits_{n\to+\infty}P(A_n)$(Continuité décroissante)\end{center}
\item Pour toute suite $(A_n)_{n\in\N}$ d'événements de $\mathcal{A}$ :\par\begin{center}$P\left(\bigcup_{n=0}^{+\infty}A_n\right)\leq\sum\limits_{n=0}^{+\infty}P(A_n)$\end{center}
\end{enumerate}}


\section{Dénombrement}
\Thr{}{On rappelle ces formules pour les coefficients binomiaux :\begin{itemize}
\item Définition :
\par $$\forall n\in\N, \forall p\in\llbracket 0,n\rrbracket, \binom{n}{p} = \dfrac{n!}{p!(n-p)!}$$
\item Formule des compléments :
\par $$\forall n\in\N, \forall p\in\llbracket 0, n\rrbracket, \binom{n}{n-p}=\binom{n}{p}$$
\item Petite formule :
\par $$\forall (n,p)\in(\N^*)^2, p\binom{n}{p} = n\binom{n-1}{p-1}$$
\item Formule de Pascal :
\par $$\forall (n,p)\in(\N^*)^2, \binom{n}{p} =\binom{n-1}{p}+\binom{n-1}{p-1}$$
\end{itemize}}
\Thr{}{On suppose qu'il existe $p\in\R_+^*$ tel que \par\begin{center}$\forall \omega\in \Omega, P(\{\omega\})=p$\end{center}
\par Alors :\begin{itemize}
\item $\Omega$ est un ensemble fini et \par\begin{center}$p=\frac{1}{\mathrm{Card}(\Omega)}$\end{center}
\item Pour tout événement $A$, \par\begin{center}$P(A)=\frac{\mathrm{Card}(A)}{\mathrm{Card}(\Omega)}$\end{center}
\end{itemize}}
Une partie de cardinal $k\in\N$ de $E$ est une $k$-combinaison. On peut voir une $k$-combinaison comme un prélèvement simultané de $k$ éléments de $E$ ; donc sans tenir compte ni d'ordre de tirage, ni de répétition.
\Thr{Nombre de parties}{Soit $E$ un ensemble à $n$ éléments\begin{enumerate}
\item Le nombre de $k$-combinaisons de $E$ est \par\begin{center} $\binom{n}{k}=\frac{1}{k!}n(n-1)...(n-k+1)=\left\{\begin{array}{rl} \frac{n!}{k!(n-k)!} & \text{si $k\leq n$} \\ 0 & \text{sinon}\end{array}\right.$\end{center}
\item Le nombre de parties de $E$ est \par\begin{center}$\sum\limits_{k=0}^n\binom{n}{k}=2^n$\end{center}
\end{enumerate}}
\subsubsection{Compter des listes}
Soit $k\in\N^*$. L'ensemble des $k$-listes d'éléments de $E$ est le produit cartésien $E^k$. On distingue deux cas particuliers de listes :\begin{itemize}
\item Une $k$-liste sans répétition (ou $k$-arrangement) est un élément $(x_1,...,x_k)\in E^k$ où $x_i\neq x_j$ si $i\neq j$. On rencontre des $k$-listes sans répétition quand par exeple on modélise des tirages successifs sans remise.
\item Une permutation de $E$ est une $n$-liste sans répétition de l'ensemble $E$ (de cardinal $n$)
\par On peut voir aussi une permutation de $E$ comme une bijection de $\llbracket 1,n\rrbracket$ dans $E$ ou une façon de réordonner les éléments de $E$.
\end{itemize}
\Thr{Nombre de listes}{Soit $E$ un ensemble à $n$ éléments.\begin{enumerate}
\item Soient $E_1,..., E_k$ $k$ ensembles de cardinaux respectifs $n_1,...,n_k\in\N^*$\par\begin{center}$\mathrm{Card}(E_1\times E_2\times...\times E_k)=n_1n_2...n_k$\end{center}
\item Le nombre de $k$-listes sans répétitions d'éléments de $E$ est $A_n^k = n(n-1)...(n-k+1)$
\item Le nombre de permutations d'éléments de $E$ est $n!$
\end{enumerate}}

\section{Conditionnel}
\Def{}{Soit $(\Omega, \mathcal{A},P)$ un espace probabilisé et $A$ un événement.
\par On peut définir la probabilité conditionnelle de $A$ sachant $B$ notée $P(A/B)$ ou $P_B(A)$ :\begin{itemize}
\item Si $B$ un événement de probabilité non-nulle : \par\begin{center}$P(A/B)=P_B(A)=\frac{P(A\cap B)}{P(B)}$\end{center}
\item Si $P(B)=0$ \par\begin{center} $P_B(A)=0$\end{center}
\end{itemize}}
\Def{}{Un système complet d'événements est une famille $(A_i)_{i\in I}$ au plus dénombrable d'événements tels que :\begin{itemize}
\item les événements sont deux à deux incompatibles ($\forall i,j\in I, i\neq j\Rightarrow A_i\cap A_j=\emptyset$)
\item leur union est l'univers tout entier ($\bigcup_{i\in I}A_i=\Omega$)
\end{itemize}}
\Def{}{Un système quasi-complet d'événements est uen famille $(A_i)_{i\in I}$ au plus dénombrable d'événements tels que :\begin{itemize}
\item les événements sont incompatibles deux à deux ($\forall i,j\in I, i\neq j\Rightarrow A_i\cap A_j=\emptyset$)
\item leur union est presque sûre : $P\left(\bigcup_{i\in I}A_i\right)=1$
\end{itemize}}
\Thr{Formule des probabilités composées}{Soit $(A_n)_{n\in\N}$ une famille d'événements telle que pour tout entier $n$, $P\left(\bigcap_{i=1}^{n-1}A_i\right)\neq 0$. Alors, pour tout entier $n$ :
\par $$P\left(\bigcap_{i=1}^nA_i\right)=P(A_1)P_{A_1}(A_2)...P_{A_1\cap A_2\cap...\cap A_{n-1}}(A_n)$$
\par $$P\left(\bigcap_{n\in\N}A_n\right) = \lim\limits_{n\to+\infty}P(A_1)P_{A_1}(A_2)...P_{A_1\cap...\cap A_{n-1}}(A_n)$$}
\Thr{}{Soit $(\Omega, A, P)$ un espace probabilisé. Pour tout système complet d'événements $(A_i)_{i\in I}$, on a :
\par $$\sum\limits_{i\in I}P(A_i)=1$$}
\Thr{Formule des probabilités totales}{Soit $(\Omega, \mathcal{A}, P)$ un espace probabilisé. Soit $(A_i)_{i\in I}$ un système complet ou quasi-complet d'événements.
\par Pour tout événement $B$ :
\par $$P(B)=\sum\limits_{i\in I}P(A_i\cap B)=\sum\limits_{i\in I}P(A_i)P_{A_i}(B)$$}
\Thr{Formule d'inversion de Bayes}{Soit $(A_i)_{i\in I}$ un système complet ou quasi-complet d'événements d'un espace probabilisé $(\Omega, \mathcal{A},P)$. Soit $B$ un événement de probabilité non-nulle, soit $i_0\in I$.
\par Alors :
\par $$P_B(A_{i_0})=\frac{P(B\cap A_{i_0})}{P(B)}=\frac{P(A_{i_0})P_{A_{i_0}}(B)}{\sum\limits_{i\in I}P(A_i)P_{A_i}(B)}$$}

\section{Variables aléatoires discrètes}
\subsection{Définitions}
\Def{}{Soit $E$ un ensemble et $(\Omega, \mathcal{A})$ un espace probabilisable.
\par Une application $X:\to E$ est une variable aléatoire discrète si :\begin{itemize}
\item L'ensemble $X(\Omega)$ des valeurs prises par $X$ est au plus dénombrable.
\item Pour tout $x\in X(\Omega)$, l'ensemble $X^{-1}(\{x\})$, noté $(X=x)$ ou $[X=x]$, est un événement (un élément de $\mathcal{A}$)
\end{itemize}
Lorsque $E=\R$, la variable $X$ est dite réelle.}
\Def{}{Soit $X$ une variable aléatoire discrète sur un espace probabilisé, à valeurs dans un ensemble $E$
\par La loi $P_X$ de $X$ est la donnée de :\begin{itemize}
\item l'ensemble des valeurs prises par $X$ appelé univers image : $X(\Omega)$
\item les probabilités élémentaires : $p_x = P(X=x)$ pour tout $x\in X(\Omega)$
\end{itemize}}

\Thr{}{Si $X$ est une variable aléatoire discrète sur l'espace probabilisable $(\Omega, \mathcal{A})$, alors la suite $((X=x))_{x\in X(\Omega)}$ est un système complet d'événements de $\Omega$}
\Thr{}{Si $X$ une variable aléatoire discrète à valeurs dans un ensemble $E$, \begin{itemize}
\item la famille $(P(X=x))_{x\in E}$ est sommable de somme $1$ :
\par $$\sum\limits_{x\in E}P(X=x)=1$$
\item par $\sigma$-additivité, pour toute partie $U$ de $X(\Omega)$,
\par $$P(X\in U)=P\left(\bigcup_{x\in U}(X=x)\right)=\sum\limits_{x\in U}P(X=x)$$
\item $(X(\Omega), \mathcal{P}(X(\Omega)), P_X)$ est un espace probabilisé.
\end{itemize}
Réciproquement, si $(p_x)_{x\in E}$ est une famille sommable de réels positifs de somme $1$ alors il existe une variable aléatoir discrète $X$ telle que pour tout $x\in E$, $P(X=x)=p_x$}
\Def{}{$X$ et $Y$ sont deux variables aléatoires discrètes indépendantes lorsque pour toutes parties $A$ et $B$ de $X(\Omega)$ et $Y(\Omega)$ respectivement :
\par $$P((X\in A)\cap (Y\in B))=P(X\in A)P(Y\in B)$$
\par On note $X\ind Y$}
\Thr{}{Deux variables aléatoires discrètes $X$ et $Y$ sont dites indépendantes si, et seulement si,
\par $$\forall (x,y)\in X(\Omega)\times Y(\Omega), P((X=x)\cap (Y=y))=P(X=x)P(Y=y)$$
\par ie : $\forall (x,y)\in X(\Omega)\times Y(\Omega)$, les événements $(X=x)$ et $(Y=y)$ sont indépendants.}
\Thr{}{Les variables aléatoires discrètes $X_1,X_2,...,X_n$ sont (mutuellement) indépendantes si, et seulement si, pour tout $n$-uplet $(x_1,x_2,...,x_n)$ de $X_1(\Omega)\times X_2(\Omega)\times...\times X_n(\Omega)$ :
\par $$P\left(\bigcap_{i=1}^n(X_i=x_i)\right)=\prod\limits_{i=1}^nP(X_i=x_i)$$}
\Thr{Lemme des coalitions}{Soient $X_1,X_2,...,X_n$ $n$ variables aléatoires discrètes définies sur $\Omega$ à valeurs dans $E$.
\par Soient $f$ et $g$ deux applications respectivement de $E^m$ dans $F$ et de $E^{n-m}$ dans $F$.
\par Si $X_1,X_2,..., X_n$ sont (mutuellement) indépendantes alors $f(X_1,..., X_m)$ et $g(X_{m+1}, X_n)$ le sont aussi.
\par Ce théorème s'étend à plus de deux coalitions.}


\subsection{Lois usuelles}
\Def{}{Une variable aléatoire discrète $X$ suit une loi de Bernoulli de paramètre $p$ si
\par\begin{center}$X(\Omega)=\{0,1\}$ et $P(X=1)=p$\end{center}
\par On note $X\sim\mathcal{B}(p)$}
\Thr{}{Si $X\sim\mathcal{B}(p)$ alors
\par $$E(X)=p\text{ et }V(X)=pq$$
En particulier, le paramètre d'une loi de Bernouilli est son espérance.}

\Def{}{Une variable aléatoire discrète $X$ suite une loi uniforme sur l'ensemble fini non-vide $K\subset \R$ si
\par $$X(\Omega)=K\text{ et }\forall k\in K, P(X=k) = \frac{1}{\mathrm{Card}(K)}$$
On note $X\sim \mathcal{U}(K)$}
\Thr{}{Si $X\sim\mathcal{U}(\llbracket 1,n\rrbracket)$ alors
\par\begin{center}$E(X)=\frac{n+1}{2}$ et $V(X) = \frac{n^2-1}{12}$\end{center}}

\Def{}{Une variable aléatoire discrète $X$ suit une loi binomiale des paramètres $n$ et $p$ si $X(\Omega)=\llbracket 0,n\rrbracket$ et
\par $$\forall k\in \llbracket 0,n\rrbracket, P(X=k)=\binom{n}{k}p^kq^{n-k}$$
\par $X\sim \mathcal{B}(n,p)$}
\Thr{Modèle loi binomiale}{Si $X$ est le nombre de succès lors de la répétition de $n$ épreuves de Bernouilli indépendantes de même paramètre $p$ alors $X$ suit la loi $\mathcal{B}(n,p)$
\par Formellement :
\par On pose pour $k\in\llbracket 1,n\rrbracket$ $X_k$ la variable aléatoire discrète qui vaut $1$ si la $k$-ième épreuve est un euscès et $0$ sinon. Si :\begin{itemize}
\item pour tout $k\in\llbracket 1,n\rrbracket, X_k\sim\mathcal{B}(p)$
\item $X_1,X_2,..., X_n$ sont mutuellement indépendantes
\item $X=X_1+X_2+...+X_n$
\end{itemize}
Alors
\par $$X\sim \mathcal{B}(n,p)$$
\par En outre
\par $$E(X)=np\text{ et }V(x)=npq$$}

\Def{}{On suppose que $p\in]0,1[$
\par Une variable aléatoire discrète $X$ suit une loi géométrique de paramètre $p$ si $X(\Omega)=\N^*$ et
\par\begin{center}$\forall n\in\N^*, P(X=n)=pq^{n-1}$\end{center}
\par On note $X\sim\mathcal{G}(p)$}
\Thr{}{Si $X$ suite une loi géométrique de paramètre $p$ alors pour tout $k\in\N^*$
\par\begin{center}$P(X>k)=(1-p)^k$\end{center}}
\Thr{Espérance et variance d'une loi géométrique}{Si la variable aléatoire discrète $X$ suite une loi $\mathcal{G}(p)$ alors $X$ possède une espérance et une variance qui valent
\par\begin{center}$E(X)=\frac{1}{p}$ et $V(X)=\frac{q}{p^2}$\end{center}}
\Pre{Espérance : $p\sum\limits_{i=0}^{+\infty} npq^{n-1} = \left(\frac{p}{1-q}\right)'=\frac{1}{p}$
\par Variance : $ V(X) = E(X^2 - X)+E(X) (E(X))^2 = \frac{2(1-p)}{p^2}+\frac{1}{p} -\frac{1}{p^2} = \frac{1-p}{p^2} = \frac{q}{p^2}$}
\Thr{Modèle loi géométrique}{Si $X$ est le rand u premier succès dans une suite illimitée d'épreuves de Bernouilli indépendantes de même paramètre $p$ alors $X$ suit la loi $\mathcal{G}(p)$
\par Formellement :
\par On pose pour tout $k\in\N^*$ $X_k$ la variable aléatoire discrète qui vaut $1$ si la $k$-ième épreuve est un succés et $0$ sinon.
\par Si :\begin{itemize}
\item pour tout $k\in\N^*n, X_k\sim\mathcal{B}(p)$
\item la suite $(X_n)$ est une suite de variables mutuellement indépendantes
\item $X = \min\{n\in\N^*, X_n=1\}$
\end{itemize}
Alors $X\sim\mathcal{G}(p)$}

\Def{Loi de Poisson}{Une variable aléatoire discrète $X$ suit une loi de Poisson de paramètre $\lambda\in\R_+^*$ si $X(\Omega)=\N$ et
\par $$\forall n\in\N, P(X=n)=e^{-\lambda}\frac{\lambda^n}{n!}$$
\par On note $X\sim \mathcal{P}(\lambda)$}
\Thr{Espérance et variance d'une loi de Poisson}{Si la variable aléatoire discrète $X$ suit une loi $\mathcal{P}(\lambda)$ alors $X$ possède une espérance et une variance et
\par $$E(X)=V(X)=\lambda$$}


\subsection{Espérance}
\Def{}{Soit $X$ une variable aléatoire discrète à valeurs dans $[0, +\infty]$.
\par $$E(X)=\sum\limits_{x\in X(\Omega)}xP(X=x)$$
\par Avec la convention $xP(X=x)=0$ lorsque $X=+\infty$ et $P(X=+\infty)=0$}
\Thr{}{Soit $X$ une variable aléatoire discrète à valeurs dans $\N\cup\{+\infty\}$. On a :
\par $$E(X)=\sum\limits_{n\in\N}P(X\geq n)$$}
\Thr{Transfert}{Soit $X$ une variable aléatoire discrète à valeurs réelles ou complexes et $f:X(\Omega)\to\C$
\par La variable aléatoire réelle $f(X)$ est d'espérance finie si, et seulement si, la famille $(f(x)P(X=x))_{x\in X(\Omega)}$ est sommable.
\par Dans ce cas :
\par\begin{center}$E(f(X))=\sum\limits_{x\in X(\Omega)}f(x)P(X=x)$\end{center}}
\Thr{Propriétés de l'espérance}{Soient $X,Y$ deux variables aléatoires discrètes à valeurs réelles ou complexes et d'espérance finie.\begin{itemize}
\item $\forall \lambda, \mu\in C, \lambda X+\mu Y$ est d'espérance finie et
\par\begin{center}$E(\lambda X+\mu Y)=\lambda E(X)+\mu E(Y)$ (linéarité)\end{center}
\par On peut généraliser cette propriété avec $n$ variables aléatoires réelles discrètes d'espérance finie, qu'elles soient indépendantes ou non.
\par Dans la suite, $X$ et $Y$ sont à valeurs réelles.
\item Si $X\geq 0$ alors $E(X)\geq 0$ (Positivité)
\item Si $X\geq 0$ et $E(X)=0$ alors $(X=0)$ est presque sûr (stricte positivité)
\item Si $X\leq Y$ alors $E(X)\leq E(Y)$ (croissance)
\end{itemize}}
\Thr{Espérance d'un produit de variables indépendantes}{Soit $X,Y$ deux variables aléatoires discrètes à valeurs réelles ou complexes.
\par Si on a :\begin{itemize}
\item $X$ et $Y$ d'espérance finie
\par ($X$ et $Y$ admettent une espérance serait plus adapté)
\item $X$ et $Y$ indépendantes
\end{itemize}
Alors $XY$ est d'espérance finie et
\par $$E(XY)=E(X)E(Y)$$}
\Thr{Espérance d'un produit de n variables indépendantes}{Soit $X_1,...,X_n$ des variables aléatoires discrètes à valeurs réelles ou complexes.
\par Si on a :\begin{itemize}
\item $X_1,..., X_n$ d'espérance finie
\item $X_1,..., X_n$ (mutuellement) indépendantes
\end{itemize}
Alors la variable $X_1...X_n$ est d'espérance finie et
\par $$E\left(\prod\limits_{i=1}^nX_i\right)=\prod\limits_{i=1}^nE(X_i)$$}


\subsection{Variance}
\Def{}{$X$ une variable aléatoire complexe, on dit que $X$ admet un moment d'ordre $k$ pour $k\in\N^*$ si $E(X^k)$ existe}
\Thr{}{Si $X$ admet un moment d'ordre $k+1$ alors $X$ admet un moment d'ordre $k$.}
Ce théorème permet de justifier l'existence d'une espérance dans le cas où une variance existe, et donc de justifier la prochaine définition.
\Def{Variance et ecart-type}{Soit $X$ une variable aléatoire discrète à valeurs réelles.
\par Si $X^2$ est d'espérance finie, on définit la variance de $X$ par
\par $$V(X)=E((X-E(X))^2)$$
\par et son écart-type par
\par $$\sigma(X)=\sqrt{V(X)}$$}
\Thr{Propriétés de la variance}{Soit $X$ une variable aléatoire discrète à valeurs réeles.
\par On suppose que $X^2$ est d'espérance finie.\begin{enumerate}
\item Köning-Huygens : formule pratique pour la variance :\par\begin{center}$V(X)=E(X^2)-E(X)^2$\end{center}
\item Soient $a,b\in\R$ \par\begin{center}$V(aX+B)=a^2V(X)$\end{center} \par En particulier, la variance est invariante par translation
\item $V(X)$ est nulle si, et seulement si, $P(X=E(X))=1$ \par ie : $X$ est presque sûrement constante.
\end{enumerate}}
\Def{}{Soit $X$ une variable aléatoire discrète à valeurs réelles telle que $X^2$ est d'espérance finie et telle que $\sigma(X)>0$
\par La variable $\frac{X}{\sigma(X)}$ a un écart-type égal à $1$. Elle est appelée réduite de $X$.
\par La variable $\frac{X-E(X)}{\sigma(X)}$ a une espérance nulle et un écart-type égal à $1$. Elle est appelée variable centrée réduite associée à $X$.}
\Def{}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors on peut définir la covariance de $X$ et $Y$ par :
\par $$\mathrm{cov}(X,Y)=E((X-E(X))(Y-E(Y)))$$}
\Thr{Propriétésde la covariance}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors :\begin{itemize}
\item $$\mathrm{cov}(X,X)=V(X)$$
\item Köning-Huygens : formule pratique pour la covariance :
\par $$\mathrm{cov}(X,Y)=E(XY)-E(X)E(Y)$$
\item Si $X$ et $Y$ sont indépendantes alors :
\par $$\mathrm{cov}(X,Y)=0$$
\item La covariance est une forme linéaire positive, symétrique et bilinéaire (c'est "presque" un produit scalaire)
\end{itemize}}
\Thr{Variance d'une somme}{Si les variables aléatoires $X^2$ et $Y^2$ sont d'espérance finie alors $(X+Y)^2$ aussi avec
\par $$V(X+Y)=V(X)+2\mathrm{cov}(X,Y)+V(Y)$$
\par Si de plus les variables $X$ et $Y$ sont indépendantes :
\par $$V(X+Y)=V(X)+V(Y)$$}
\Thr{Variance d'une somme}{Si $X_1,...,X_n$ sont $n$ variables telles que $X_1^2,..., X_n^2$ sont d'espérance finie alors $(X_1+...+X_n)^2$ l'est aussi avec :
\par $$V\left(\sum\limits_{k=1}^nX_k\right)=\sum\limits_{k=1}^nV(X_k)+2\sum\limits_{1\leq i<j\leq n}\mathrm{cov}(X_i, X_j)$$
\par Si de plus les variables $X_1,...,X_n$ sont indépendantes \textbf{deux à deux} alors :
\par $$V\left(\sum\limits_{k=1}^nX_k\right)=\sum\limits_{k=1}^nV(X_k)$$}


\subsection{Fonctions génératrices}
\Def{}{On note $R_X$ le rayon de convergence de la série entière $\sum P(X=n)t^n$
\par La fonction génératrice d'une variable aléatoire $X$ à valeurs dans $\N$ est définie pour tout $t\in]-R_X,R_X[$ par
\par $$G_X(t)=E(t^X)=\sum\limits_{n=0}^{+\infty}P(X=n)t^n$$}
\Thr{}{Soit $X$ une variable aléatoire à valeurs dans $\N$.\begin{enumerate}
\item La loi de $X$ est entièrement caractérisée par la connaissance de sa fonction génératrice $G_X$
\par ie : $G_X:t\mapsto \sum\limits_{n=0}^{+\infty}a_nt^n$ est la fonction génératrice de $X$ si, et seulement si, $\forall n\in \N, P(X=n)=a_n$
\item $G_X(1)=1$
\item $X$ admet une espérance si, et seulement si, $G_X$ est dérivable en $1$, avec dans ce cas
\par $$E(x) =G_X'(1)$$
\item $X$ admet une variance si, et suelement si, $G_X$ est deux fois dérivable en $1$, avec dans ce cas :\par\begin{center}$E(X(X-1))=G_X''(1)$\end{center}
\par et donc
\par $$V(X)=G_X''(1)+G_X'(1)-(G_X'(1))^2$$
\end{enumerate}}
\Thr{}{Si $X$ et $Y$ sont deux variables aléatoires indépendantes à valeurs dans $\N$ alors :
\par $$\forall t\in[-1,1], G_{X+Y}=G_XG_Y$$
\par Soit $n\in\N, n\geq 2$. Si $X_1,...,X_n$ sont des variables aléatoire réelles mutuellement indépendantes alors :
\par $$G_{X_1+...+X_n}=G_{X_1}...G_{X_n}$$}


\subsection{Lois conjointes et conditionnelles}
\Def{}{Soient $X$ et $Y$ deux variables aléatoires discrètes réelles sur un même espace probabilisé $(\Omega, \mathcal{A},P)$.\begin{enumerate}
\item On appelle couple des variables $X$ et $Y$, et on note $Z=(X,Y)$ l'application \par\begin{center}$Z:\left\{\begin{array}{rcl}\Omega & \to & X(\Omega)\times Y(\Omega) \\ \omega & \mapsto & (X(\omega), Y(\omega)) \end{array}\right.$\end{center}
\item La loi du couple $Z=(X,Y)$ est appelée loi conjointe, elle est définir par :\begin{itemize}
    \item $Z(\Omega)=X(\Omega)\times Y(\Omega)$
    \item Les probabilités élémentaires $P(X=x, Y=y)=P((X=x)\cap(Y=y))$ pour tous $(x,y)\in X(\Omega)\times Y(\Omega)$
\end{itemize} On a bien sûr \par\begin{center}$\forall (x,y)\in X(\Omega)\times Y(\Omega), P(X=x,Y=y)\geq 0$ et $\sum\limits_{(x,y)\in X(\Omega)\times Y(\omega)}P(X=x,Y=y)=1$\end{center}
\item Les lois de $X$ et $Y$ sont appelées lois marginales du couple $Z=(X,Y)$
\par Si la loi conjointe du couple $Z=(X,Y)$ est connue, alors les lois marginales de $X$ et $Y$ le sont aussi :\begin{itemize}
    \item On détermine la loi de $X$ en appliquant la formule des probabilités totales avec le système complet d'événements $([Y=y])_{y\in Y(\Omega)}$ :
    \par\begin{center}$\forall x\in X(\Omega), P(X=x)=\sum\limits_{y\in Y(\Omega)}P(X=x, Y=y)$\end{center}
    \item On détermine la loi de $Y$ en appliquant la formule des probabilités totales avec le système complet d'événements $([X=x])_{x\in X(\Omega)}$ :
    \par\begin{center}$\forall y\in Y(\Omega), P(Y=y)=\sum\limits_{x\in X(\Omega)}P(X=x, Y=y)$\end{center}
\end{itemize} La réciproque est évidemment fausse, la connaissance des lois marginales de $X$ et $Y$ ne permet pas de déterminer la loi conjointe du couple $Z=(X,Y)$
\end{enumerate}}
\Def{}{Soit $X$ une variable aléatoire réelle discrète sur $(\Omega,\mathcal{A}, P)$ et $A$ un événement de probabilité non-nulle. La loi conditionnelle de $X$ sachant $A$ est la donnée de :\begin{itemize}
\item $X(\Omega)$
\item $\forall x\in X(\Omega), P_A(X=x)$
\end{itemize}
Elle est notée $X_{/A}$}
\Def{}{Soit $Z=(X,Y)$ un couple de variables aléatoires réelles discrètes sur $(\Omega, \mathcal{A}, P)$
\par La loi de $Z$ est donnée par :\begin{enumerate}
\item Les lois de $X$ conditionnées par $Y$ sont les lois de $X$ conditionnées par les événements $[Y=y]$ pour tout $y\in Y(\Omega)$
\par Plus précisement, pour $y\in Y(\Omega)$ fixé tel que $P(Y=y)\neq 0$, la loi de $X$ sachant $[Y=y]$ est définie par :\begin{itemize}
    \item La donnée de $X(\Omega)$
    \item Les nombres $P_{Y=y}(X=x)=\frac{P(X=x,Y=y)}{P(Y=y)}$ pour tout $x\in X(\Omega)$
\end{itemize}
\item Les lois de $Y$ conditionnées par $X$ sont les lois de $Y$ conditionnées par les événements $[X=x]$ pour tout $x\in X(\Omega)$
\par Plus précisement, pour $x\in X(\Omega)$ fixé tel que $P(X=x)\neq 0$, la loi de $Y$ sachant $[X=x]$ est définie par :\begin{itemize}
    \item La donnée de $Y(\Omega)$
    \item Les nombres $P_{X=x}(Y=y)=\frac{P(X=x,Y=y)}{P(X=x)}$ pour tout $x\in X(\Omega)$
\end{itemize}
\end{enumerate}}


\section{Résultats asymptotiques}
Les preuves sont à savoir pour ces théorèmes.
\Thr{Inégalité de Markov}{Soit $X$ une variable aléatoire réelle discrète positive d'espérance finie, on a:
\par\begin{center}$\forall a>0, P(X\geq a)\leq \frac{E(X)}{a}$\end{center}}
\Pre{Avec $a\in\R_+$ fixé :
\par $E(X) = \sum\limits_{x\in X(\Omega)}xP(X=x) \geq \sum\limits_{x\in X(\Omega),x\geq a}xP(X=x)$ (comme $X$ est positive)
\par $\geq a\sum\limits_{x\in X(\Omega), x\geq a} P(X=x)\geq aP(X\geq A)$
\par D'où l'inégalité $P(X\geq a)\leq \frac{E(X)}{a}$}

\Thr{Inégalité de Bieinaymé-Tchebychev}{Soit $X$ une variable aléatoire rélle discrète telle que $X^2$ est d'espérance finie, on a :
\par\begin{center}$\forall \varepsilon>0, P(\vert X-E(X)\vert\geq \varepsilon)\leq \frac{V(X)}{\varepsilon^2}$\end{center}
\par En passant à l'événement contraire :
\par\begin{center}$\forall\varepsilon>0, P(\vert X-E(X)\vert<\varepsilon)\geq 1 - \frac{V(X)}{\varepsilon^2}$\end{center}}
\Pre{Pour $\varepsilon>0$:
\par $E((X-E(X))^2) = \sum\limits_{x\in X(\Omega)}(x-E(X))^2P(X=x)$
\par $\geq \varepsilon^2\sum\limits_{x\in X(\Omega), \vert X-E(X)\vert\geq\varepsilon} P(X=x)$
\par $\geq\varepsilon^2P(\vert X - E(X)\vert\geq\varepsilon)$
\par D'où l'inégaité.}

\Thr{Loi faible des grands nombres}{Soit $(X_n)_{n\in\N^*}$ une suite de variables réelles indépendantes de même loi, de variance finie.
\par En notant $S_n=\sum\limits_{k=1}^nX_k, m=E(X_1)$ et $\sigma =\sigma(X_1)$, on a :\begin{enumerate}
\item \begin{center}$\forall \varepsilon>0, P\left(\left\vert\frac{S_n}{n}-m\right\vert\geq\varepsilon\right)\leq\frac{\sigma^2}{n\varepsilon^2}$\end{center} \par Cette inégalité doit êtr edémontrée à chaque utilisation d'après le programme
\item \begin{center}$\forall \varepsilon>0, P\left(\left\vert\frac{S_n}{n}-m\right\vert\geq\varepsilon\right)\to_{n\to+\infty} 0$\end{center}
\end{enumerate}}
\Pre{On a ici que $(\frac{S_n}{n}) = \frac{1}{n}\sum\limits_{i=1}^nE(X_i) = m$
\par Par indépendance, $V(S_n) = \frac{1}{n^2}\sum\limits_{i=1}^nV(X_i) = \frac{\sigma^2}{n}$}
\end{document}